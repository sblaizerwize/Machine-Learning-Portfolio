{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50947,"databundleVersionId":5741538,"sourceType":"competition"},{"sourceId":6129135,"sourceType":"datasetVersion","datasetId":3481501}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sblaizer/in-depth-analysis-of-kaggle-and-arxiv-datasets?scriptVersionId=159089025\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-27T07:59:19.41305Z","iopub.execute_input":"2023-10-27T07:59:19.414065Z","iopub.status.idle":"2023-10-27T07:59:19.456557Z","shell.execute_reply.started":"2023-10-27T07:59:19.414033Z","shell.execute_reply":"2023-10-27T07:59:19.455406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Objective\nThis notebook is a follow-up to the [EDA Kaggle and Arxiv datasets](https://www.kaggle.com/code/sophieb/eda-kaggle-and-arxiv-datasets). The aim is to dive deeper into the following tasks:\n\n* Integrate all text data-related competitions (9) from the past two years into the metadata analysis of the Kaggle write-ups. In the first EDA, we only referred to five competitions.\n* Analyze the Arxiv dataset in greater detail to compare the insights gained in academia with those learned from text data write-ups reported in [A Journey Through Text Data Competitions](https://www.kaggle.com/code/sophieb/a-journey-through-text-data-competitions). \n* Take a step further by using the PKE model to extract keywords from both Kaggle write-ups and Arxiv datasets, considering n-gram candidates, stopwords, and integrating a function to compute idf weights.\n* Present results using resources other than horizontal bar plots, such as stylecloud and n-gram plots.","metadata":{}},{"cell_type":"code","source":"# Installing Modules\n!pip install git+https://github.com/boudinfl/pke.git\n!pip install stylecloud wordcloud","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:02:14.04878Z","iopub.execute_input":"2023-10-27T08:02:14.049113Z","iopub.status.idle":"2023-10-27T08:02:47.667572Z","shell.execute_reply.started":"2023-10-27T08:02:14.049089Z","shell.execute_reply":"2023-10-27T08:02:47.666482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Library Definition\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nimport re\n\nimport string\nfrom string import punctuation\nimport pke\nfrom pke import compute_document_frequency\n\nimport stylecloud\nfrom PIL import Image\nfrom IPython.display import Image\n\nimport gc","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:03:21.243079Z","iopub.execute_input":"2023-10-27T08:03:21.243473Z","iopub.status.idle":"2023-10-27T08:03:41.322878Z","shell.execute_reply.started":"2023-10-27T08:03:21.243442Z","shell.execute_reply":"2023-10-27T08:03:41.321744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 1. Analyzing Kaggle Writeups Dataset","metadata":{}},{"cell_type":"code","source":"# Reading the Kaggle writeups dataset\nwriteup_df = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/kaggle_writeups_20230510.csv\", parse_dates=[0,3]) # Consider the first four columns as date-format ones\nwriteup_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:03:51.252904Z","iopub.execute_input":"2023-10-27T08:03:51.254251Z","iopub.status.idle":"2023-10-27T08:03:51.654259Z","shell.execute_reply.started":"2023-10-27T08:03:51.254215Z","shell.execute_reply":"2023-10-27T08:03:51.653123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"writeup_df.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:04:09.225375Z","iopub.execute_input":"2023-10-27T08:04:09.225739Z","iopub.status.idle":"2023-10-27T08:04:09.253437Z","shell.execute_reply.started":"2023-10-27T08:04:09.22571Z","shell.execute_reply":"2023-10-27T08:04:09.252277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of writeups in the dataset: \" + str(len(writeup_df)))","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:04:15.833399Z","iopub.execute_input":"2023-10-27T08:04:15.833756Z","iopub.status.idle":"2023-10-27T08:04:15.839265Z","shell.execute_reply.started":"2023-10-27T08:04:15.833733Z","shell.execute_reply":"2023-10-27T08:04:15.838141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Kaggle writeups dataset contains a total of 3,127 writeups and uses 22 MB of memory**","metadata":{}},{"cell_type":"markdown","source":"## How many unique competitions has the Kaggle writeups dataset?","metadata":{}},{"cell_type":"code","source":"num_competitions = writeup_df[\"Title of Competition\"].nunique()\nprint(f\"The dataset has {num_competitions} competitions.\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:05:06.020852Z","iopub.execute_input":"2023-10-27T08:05:06.021234Z","iopub.status.idle":"2023-10-27T08:05:06.027882Z","shell.execute_reply.started":"2023-10-27T08:05:06.021198Z","shell.execute_reply":"2023-10-27T08:05:06.026084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: The Kaggle writeups dataset includes 310 competitions with a total of 3,127 writeups**","metadata":{}},{"cell_type":"markdown","source":"## What are the earliest and latest competitions?","metadata":{}},{"cell_type":"code","source":"early_comp = writeup_df[\"Competition Launch Date\"].min().strftime('%Y-%m-%d')\nlate_comp = writeup_df[\"Competition Launch Date\"].max().strftime('%Y-%m-%d')\n\nprint(f\"The earliest competition is {early_comp} \\nThe latest competition is {late_comp}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:05:31.473991Z","iopub.execute_input":"2023-10-27T08:05:31.474343Z","iopub.status.idle":"2023-10-27T08:05:31.481656Z","shell.execute_reply.started":"2023-10-27T08:05:31.474316Z","shell.execute_reply":"2023-10-27T08:05:31.480224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: The dates of the Kaggle competitions range from <ins>2010-08-03</ins> to <ins>2023-02-23</ins>**","metadata":{}},{"cell_type":"markdown","source":"## How many competitions are from the past two years?\nLet's consider competitions from January 2021 onwards.","metadata":{}},{"cell_type":"code","source":"writeup_past2years_df = writeup_df[(writeup_df[\"Competition Launch Date\"].dt.year >= 2021) & (writeup_df[\"Competition Launch Date\"].dt.month >= 1)]\nnumcomp_past2years = writeup_past2years_df['Title of Competition'].nunique()\n\nprint(f\"Number of competitions from the past two years is {numcomp_past2years}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:07:24.152791Z","iopub.execute_input":"2023-10-27T08:07:24.153148Z","iopub.status.idle":"2023-10-27T08:07:24.164146Z","shell.execute_reply.started":"2023-10-27T08:07:24.153119Z","shell.execute_reply":"2023-10-27T08:07:24.162308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(writeup_past2years_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:07:28.716307Z","iopub.execute_input":"2023-10-27T08:07:28.716636Z","iopub.status.idle":"2023-10-27T08:07:28.723651Z","shell.execute_reply.started":"2023-10-27T08:07:28.71661Z","shell.execute_reply":"2023-10-27T08:07:28.722476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"writeup_past2years_df[\"Title of Competition\"].unique()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:07:31.214834Z","iopub.execute_input":"2023-10-27T08:07:31.215192Z","iopub.status.idle":"2023-10-27T08:07:31.224004Z","shell.execute_reply.started":"2023-10-27T08:07:31.215167Z","shell.execute_reply":"2023-10-27T08:07:31.222728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: There are 71 competitions held within the past two years (from January 2021 to February 2023) having 1,073 writeups.**","metadata":{}},{"cell_type":"markdown","source":"## What are the competitions from the past two years with most writeups?","metadata":{}},{"cell_type":"code","source":"writeup_past2years_df[\"Title of Competition\"].value_counts().head()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:07:51.922497Z","iopub.execute_input":"2023-10-27T08:07:51.922871Z","iopub.status.idle":"2023-10-27T08:07:51.93158Z","shell.execute_reply.started":"2023-10-27T08:07:51.922841Z","shell.execute_reply":"2023-10-27T08:07:51.929991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What are the competitions from the past two years with less writeups?\n","metadata":{}},{"cell_type":"code","source":"writeup_past2years_df[\"Title of Competition\"].value_counts().tail()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:07:56.402915Z","iopub.execute_input":"2023-10-27T08:07:56.403347Z","iopub.status.idle":"2023-10-27T08:07:56.411253Z","shell.execute_reply.started":"2023-10-27T08:07:56.403318Z","shell.execute_reply":"2023-10-27T08:07:56.410158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answers: The Feedback Prize - English Language Learning and Jigsaw Rate Severity of Toxic Comments competitions take the lead (both are text-related competitions) whereas the Herbarium 2021 and Herbarium 2022 competitions have only one writeup.**","metadata":{}},{"cell_type":"markdown","source":"## What is the number of writeups corresponding to text data competitions held in the past two years? \nFor this analysis, we're going to use the following external dataset [Top 3 Kaggle Text Data Competitions (2021-2023)](https://www.kaggle.com/datasets/sblaizer/top-3-kaggle-text-data-competitions-2021-2023) that has identified nine competitions related to text data. ","metadata":{}},{"cell_type":"code","source":"textdata_df = pd.read_csv(\"/kaggle/input/top-3-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport - Text Data Write-ups 27.csv\")\ntextdata_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:08:26.88368Z","iopub.execute_input":"2023-10-27T08:08:26.884057Z","iopub.status.idle":"2023-10-27T08:08:26.915531Z","shell.execute_reply.started":"2023-10-27T08:08:26.884029Z","shell.execute_reply":"2023-10-27T08:08:26.91409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"textdata_df[\"Competition\"].unique()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:08:31.252144Z","iopub.execute_input":"2023-10-27T08:08:31.252495Z","iopub.status.idle":"2023-10-27T08:08:31.261131Z","shell.execute_reply.started":"2023-10-27T08:08:31.252467Z","shell.execute_reply":"2023-10-27T08:08:31.259985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turning unique competitions into a list \nlist_textdata_comp = list(textdata_df[\"Competition\"].unique())","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:08:38.78365Z","iopub.execute_input":"2023-10-27T08:08:38.784063Z","iopub.status.idle":"2023-10-27T08:08:38.789858Z","shell.execute_reply.started":"2023-10-27T08:08:38.784032Z","shell.execute_reply":"2023-10-27T08:08:38.788515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correcting middle dash typos of the list\nlist_textdata_comp[0] = 'Feedback Prize - Predicting Effective Arguments'\nlist_textdata_comp[3] = 'Feedback Prize - Evaluating Student Writing'\nlist_textdata_comp[5] = 'chaii - Hindi and Tamil Question Answering'\nlist_textdata_comp[7] = 'Coleridge Initiative - Show US the Data'\nlist_textdata_comp[8] = 'NBME - Score Clinical Patient Notes'\n\nlist_textdata_comp","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:08:40.74086Z","iopub.execute_input":"2023-10-27T08:08:40.741251Z","iopub.status.idle":"2023-10-27T08:08:40.748623Z","shell.execute_reply.started":"2023-10-27T08:08:40.741216Z","shell.execute_reply":"2023-10-27T08:08:40.747871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filtering out text data related competitions from the writeups of the past two years\ntext_past2years_df = writeup_past2years_df[writeup_past2years_df[\"Title of Competition\"].isin(list_textdata_comp)].copy()\ntext_past2years_df[\"Title of Competition\"].unique()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:08:44.695642Z","iopub.execute_input":"2023-10-27T08:08:44.696035Z","iopub.status.idle":"2023-10-27T08:08:44.705016Z","shell.execute_reply.started":"2023-10-27T08:08:44.696008Z","shell.execute_reply":"2023-10-27T08:08:44.703372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total writeups from the past two years: {len(writeup_past2years_df)}\")\nprint(f\"Total writeups related to text data competitions from the past two years: {len(text_past2years_df)}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:09:02.339435Z","iopub.execute_input":"2023-10-27T08:09:02.339797Z","iopub.status.idle":"2023-10-27T08:09:02.344635Z","shell.execute_reply.started":"2023-10-27T08:09:02.339773Z","shell.execute_reply":"2023-10-27T08:09:02.343948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_past2years_df = text_past2years_df.reset_index(drop=True)\ntext_past2years_df.sort_values(by='Competition Launch Date', ascending=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:09:08.867668Z","iopub.execute_input":"2023-10-27T08:09:08.868061Z","iopub.status.idle":"2023-10-27T08:09:08.889727Z","shell.execute_reply.started":"2023-10-27T08:09:08.868028Z","shell.execute_reply":"2023-10-27T08:09:08.888813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Response: There are 9 competitions related to text data spanning from March 2021 to May 2022 having a total of 208 writeups.**","metadata":{}},{"cell_type":"markdown","source":"## What is the number of writeups per text data competition?","metadata":{}},{"cell_type":"code","source":"text_past2years_df[\"Title of Competition\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:09:26.03404Z","iopub.execute_input":"2023-10-27T08:09:26.034412Z","iopub.status.idle":"2023-10-27T08:09:26.045074Z","shell.execute_reply.started":"2023-10-27T08:09:26.034384Z","shell.execute_reply":"2023-10-27T08:09:26.043599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Response: Jigsaw Rate Severity of Toxic Comments takes the lead with 33 writeups**","metadata":{}},{"cell_type":"markdown","source":"# 2. Extracting keywords from text data writeups\nNow that we have identified 9 competitions related to text data and their 208 writeups, let's analyze the content of the writeups using the [PKE](https://boudinfl.github.io/pke/build/html/index.html) (Python Keyword Extraction) module. Before stepping into this task, it's paramount to implement a cleaning text data stage. ","metadata":{}},{"cell_type":"markdown","source":"## Cleaning text data","metadata":{}},{"cell_type":"markdown","source":"Let's have a look at the format of a single writeup:","metadata":{}},{"cell_type":"code","source":"text_past2years_df[\"Writeup\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:10:53.031121Z","iopub.execute_input":"2023-10-27T08:10:53.031461Z","iopub.status.idle":"2023-10-27T08:10:53.037797Z","shell.execute_reply.started":"2023-10-27T08:10:53.031433Z","shell.execute_reply":"2023-10-27T08:10:53.036825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a function that performs several text data cleaning steps \ndef clean_text(df, col_to_clean):\n\n    # Remove HTML tags\n    df['cleaned_text'] = df[col_to_clean].apply(lambda x: re.sub('<[^<]+?>', ' ', x))\n \n    # Remove brackets and apostrophes from Python lists\n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r\"[\\[\\]'\\\"]\",\" \", x))\n    \n    # Remove change of line characters \n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"\\n\", \" \", regex=True)\n   \n    # Remove special characters\n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"-\", \"\", regex=False)\n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"[^a-zA-Z0-9 ]\", \"\", regex=True)\n     \n    # Lowercase text\n    df['cleaned_text'] = df['cleaned_text'].str.lower()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:11:02.677519Z","iopub.execute_input":"2023-10-27T08:11:02.677941Z","iopub.status.idle":"2023-10-27T08:11:02.686085Z","shell.execute_reply.started":"2023-10-27T08:11:02.677881Z","shell.execute_reply":"2023-10-27T08:11:02.684787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After applying the cleaning function, this is the outcome we obtained:","metadata":{}},{"cell_type":"code","source":"# Applying `clean_text()` function on writeups\ntext_past2years_df = clean_text(text_past2years_df, 'Writeup')\ntext_past2years_df['cleaned_text'][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:11:07.383146Z","iopub.execute_input":"2023-10-27T08:11:07.383494Z","iopub.status.idle":"2023-10-27T08:11:07.428066Z","shell.execute_reply.started":"2023-10-27T08:11:07.383465Z","shell.execute_reply":"2023-10-27T08:11:07.426851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Computing the frequency of keywords in writeups","metadata":{}},{"cell_type":"code","source":"# Creating a list containing all writeups\nlst_writeups = text_past2years_df['cleaned_text'].to_list()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:12:12.064492Z","iopub.execute_input":"2023-10-27T08:12:12.064822Z","iopub.status.idle":"2023-10-27T08:12:12.069891Z","shell.execute_reply.started":"2023-10-27T08:12:12.064797Z","shell.execute_reply":"2023-10-27T08:12:12.068795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function calculates the frequency of keywords in the collection of writeups. If using a CPU setting, this task will take around 7 min to complete. \n","metadata":{}},{"cell_type":"code","source":"#Reference1: https://github.com/boudinfl/pke/blob/master/examples/compute-df-counts.ipynb\n#Reference2: https://boudinfl.github.io/pke/build/html/unsupervised.html\ncompute_document_frequency(\n    documents=lst_writeups,     # List of writeups\n    output_file='inspec.df.gz',\n    language='en',              # language of the input files\n    normalization='stemming',   # use porter stemmer\n    stoplist=list(punctuation), # stoplist (punctuation marks)\n    n=3                         # compute n-grams up to 3-grams\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:14:09.534062Z","iopub.execute_input":"2023-10-27T08:14:09.534419Z","iopub.status.idle":"2023-10-27T08:20:49.042497Z","shell.execute_reply.started":"2023-10-27T08:14:09.53439Z","shell.execute_reply":"2023-10-27T08:20:49.041072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at the frequency of 20 keywords from the writeups collection ","metadata":{}},{"cell_type":"code","source":"from pke import load_document_frequency_file\ndict_freq = load_document_frequency_file(input_file='inspec.df.gz')\n\ncount = 0  # Initialize a counter\nfor key, value in dict_freq.items():\n    if count < 20:  # Limit to the first 5 key-value pairs\n        print(f'{key}: {value}')\n        count += 1\n    else:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:21:11.37871Z","iopub.execute_input":"2023-10-27T08:21:11.379086Z","iopub.status.idle":"2023-10-27T08:21:11.497822Z","shell.execute_reply.started":"2023-10-27T08:21:11.379058Z","shell.execute_reply":"2023-10-27T08:21:11.496963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Erasing non-utilized variables to freeing up memory\nimport gc\n\ndel writeup_df\ndel writeup_past2years_df \n\n# Freeing up memory \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:21:30.067241Z","iopub.execute_input":"2023-10-27T08:21:30.067578Z","iopub.status.idle":"2023-10-27T08:21:30.827189Z","shell.execute_reply.started":"2023-10-27T08:21:30.067552Z","shell.execute_reply":"2023-10-27T08:21:30.825381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting keywords from writeups","metadata":{}},{"cell_type":"markdown","source":"The keyword extraction stage is based on the [TfIdf](https://boudinfl.github.io/pke/build/html/unsupervised.html) (Term Frequency-Inverse Document Frequency) method from PKE. *TfIdf is a popular and effective technique for identifying keyphrases in a collection of text documents.* We have created the `extract_keywords()` function to extract the top 5 keywords from each writeup. This function will process the 208 writeups and render a total of 1,040 keywords (208*5). \n\n","metadata":{}},{"cell_type":"code","source":"def extract_keywords(text):\n    stoplist = list(string.punctuation)\n    stoplist += pke.lang.stopwords.get('en')\n    extractor = pke.unsupervised.TfIdf()\n    extractor.load_document(input=text,\n                           language='en',\n                           stoplist=stoplist,\n                           normalization=None)\n \n    extractor.candidate_selection(n=3) #Select 1 to 3 grams\n    df = load_document_frequency_file(input_file='inspec.df.gz')\n    extractor.candidate_weighting(df=df) #Candidate weighting using document frequencies\n    keyphrases = extractor.get_n_best(n=10)\n    \n    # Extract top 5 keywords\n    keywords = [keyword[0] for keyword in keyphrases[:5]]\n    \n    return keywords","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:24:52.276648Z","iopub.execute_input":"2023-10-27T08:24:52.277044Z","iopub.status.idle":"2023-10-27T08:24:52.284629Z","shell.execute_reply.started":"2023-10-27T08:24:52.277015Z","shell.execute_reply":"2023-10-27T08:24:52.28351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating a memory usage estimation of the collection of writeups to be processed.\ntext_past2years_df['cleaned_text'].info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:25:55.009161Z","iopub.execute_input":"2023-10-27T08:25:55.009574Z","iopub.status.idle":"2023-10-27T08:25:55.019598Z","shell.execute_reply.started":"2023-10-27T08:25:55.009544Z","shell.execute_reply":"2023-10-27T08:25:55.01722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a bar to track the keyword extraction progress\nfrom tqdm import tqdm\nwith tqdm(total=len(text_past2years_df), desc=\"Processing\") as pbar:\n    def apply_with_progress(text):\n        result = extract_keywords(text)\n        pbar.update(1)  # Update the progress bar\n        return result\n\n    # Apply the function to the Series with progress tracking\n    abstract_keywords = text_past2years_df['cleaned_text'].apply(apply_with_progress)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:26:21.039562Z","iopub.execute_input":"2023-10-27T08:26:21.039882Z","iopub.status.idle":"2023-10-27T08:33:23.332026Z","shell.execute_reply.started":"2023-10-27T08:26:21.039859Z","shell.execute_reply":"2023-10-27T08:33:23.330804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying the top 5 keywords of the first 10 writeups\nabstract_keywords[:10]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:34:17.176439Z","iopub.execute_input":"2023-10-27T08:34:17.176799Z","iopub.status.idle":"2023-10-27T08:34:17.186786Z","shell.execute_reply.started":"2023-10-27T08:34:17.176774Z","shell.execute_reply":"2023-10-27T08:34:17.185444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result is a list of lists that contains the top 5 keywords of each writeup. Let's store these results in a new `keywords_lst` column. ","metadata":{}},{"cell_type":"code","source":"text_past2years_df['keywords_lst'] = abstract_keywords\ntext_past2years_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:35:49.612866Z","iopub.execute_input":"2023-10-27T08:35:49.613313Z","iopub.status.idle":"2023-10-27T08:35:49.629009Z","shell.execute_reply.started":"2023-10-27T08:35:49.613271Z","shell.execute_reply":"2023-10-27T08:35:49.627587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Freeing up memory \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:35:58.475221Z","iopub.execute_input":"2023-10-27T08:35:58.475597Z","iopub.status.idle":"2023-10-27T08:35:58.959904Z","shell.execute_reply.started":"2023-10-27T08:35:58.475568Z","shell.execute_reply":"2023-10-27T08:35:58.959175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting extracted keywords from writeups\nLet's count the most mentioned keywords in the collection of writeups","metadata":{}},{"cell_type":"code","source":"text_past2years_df_exploded = text_past2years_df.explode('keywords_lst')\ntext_past2years_df_exploded = text_past2years_df_exploded.reset_index(drop=True)\ntext_past2years_df_exploded['keywords_lst'].value_counts().head(30)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:36:07.109992Z","iopub.execute_input":"2023-10-27T08:36:07.110364Z","iopub.status.idle":"2023-10-27T08:36:07.132357Z","shell.execute_reply.started":"2023-10-27T08:36:07.110333Z","shell.execute_reply":"2023-10-27T08:36:07.130679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the previous keyword list into a dataframe\nkeywords_count_serie = text_past2years_df_exploded['keywords_lst'].value_counts()\nkeywords_count_df = pd.DataFrame({'Keywords': keywords_count_serie.index,'Count': keywords_count_serie.values})\nkeywords_count_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:37:35.768024Z","iopub.execute_input":"2023-10-27T08:37:35.768495Z","iopub.status.idle":"2023-10-27T08:37:35.781199Z","shell.execute_reply.started":"2023-10-27T08:37:35.768463Z","shell.execute_reply":"2023-10-27T08:37:35.779822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting the top 50 words\nkeywords_count_50_df = keywords_count_df[:50]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:37:46.802417Z","iopub.execute_input":"2023-10-27T08:37:46.802776Z","iopub.status.idle":"2023-10-27T08:37:46.808236Z","shell.execute_reply.started":"2023-10-27T08:37:46.802747Z","shell.execute_reply":"2023-10-27T08:37:46.807367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.bar(keywords_count_50_df, x='Count', y='Keywords', title='Top 50 keywords found in all Writeups by the TfIdf method', orientation='h', width=750, height=900, color='Keywords')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:37:50.604278Z","iopub.execute_input":"2023-10-27T08:37:50.604644Z","iopub.status.idle":"2023-10-27T08:37:53.328782Z","shell.execute_reply.started":"2023-10-27T08:37:50.60462Z","shell.execute_reply":"2023-10-27T08:37:53.327308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filtering out duplicated keywords\nkeywords_tree = text_past2years_df_exploded['keywords_lst'].to_list()\nset_keywords_tree = set(keywords_tree)\nlst_keywords_tree = list(set_keywords_tree)\nprint(f\"Total keywords: {len(keywords_tree)} \\nUnique keywords: {len(lst_keywords_tree)}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:38:13.604519Z","iopub.execute_input":"2023-10-27T08:38:13.605287Z","iopub.status.idle":"2023-10-27T08:38:13.612299Z","shell.execute_reply.started":"2023-10-27T08:38:13.605253Z","shell.execute_reply":"2023-10-27T08:38:13.610909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a word cloud image using stylecloud\nstylecloud.gen_stylecloud(\n    text=' '.join(lst_keywords_tree), \n    icon_name='fas fa-tree',                     # 'fas fa-cloud'; 'fas fa-eye'; ''\n    palette='cmocean.sequential.Matter_10',\n    background_color='black',\n    gradient='horizontal',\n    size=1024\n)\nImage(filename=\"./stylecloud.png\", width=1024, height=768)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:39:32.159741Z","iopub.execute_input":"2023-10-27T08:39:32.160147Z","iopub.status.idle":"2023-10-27T08:39:38.309315Z","shell.execute_reply.started":"2023-10-27T08:39:32.160117Z","shell.execute_reply":"2023-10-27T08:39:38.307978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 3. Examining popular architectures, domains, and techniques used in Kaggle writeups based on word occurrences\n\nIn the previous section, we extracted the top 5 keywords of every writeup and computed an empirical analysis of their occurrences in all writeups. We identified common words, including 'models,' 'competition,' 'training,' 'ensemble,' and 'different.' However, these words do not appear to offer valuable insights about the write-ups. In this section, we will formulate specific questions and provide keywords that are more likely to yield better results in understanding the techniques, text data domains, and architectures used in Kaggle's text data competitions.","metadata":{}},{"cell_type":"markdown","source":"## What are the main architectures used in the solutions of text data competitions?\nWe considered the following 16 architectures as keywords for this question. ","metadata":{}},{"cell_type":"code","source":"text_architectures_keywords = [\n    \"fasttext\", \"roberta\", \"bert\", \"gpt\", \"rnn\", \"cnn\", \"gru\", \"t5\", \"electra\", \"xlnet\",\n    \"encoder\", \"decoder\", \"lstm\", \"transformer\", \"deberta\", \"codebert\"\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:41:44.779051Z","iopub.execute_input":"2023-10-27T08:41:44.77938Z","iopub.status.idle":"2023-10-27T08:41:44.785161Z","shell.execute_reply.started":"2023-10-27T08:41:44.779357Z","shell.execute_reply":"2023-10-27T08:41:44.783363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function that matchs a list of specific words with a given column of a dataframe\ndef count_ocurrences_in_dataframe(df, column_name, strings_list):\n    # Convert the string_list input to a set for faster membership checking\n    strings_set = set(strings_list)\n    \n    # Filter out the dataframe to only include rows where 'column_name' contains any of the strings in 'strings_list' \n    # This is used to create a regular expression pattern where the '|' pipe acts as an \"OR\" operator.\n    filtered_df = df[df[column_name].str.contains('|'.join(strings_set))]\n    \n    # Create a dictionary to store the counting results\n    results_dict = {'String': [], 'Occurrences':[]}\n    \n    # Iterate over the strings list\n    for string in strings_list:\n        # Add the string and its corresponding count to the dictionary\n        results_dict['String'].append(string)\n      \n        # Count the actual ocurrences in the filtered dataframe\n        actual_occurrences = filtered_df[column_name].str.count(string).sum()\n        results_dict['Occurrences'].append(actual_occurrences)\n    \n    # Convert the dictionary to a dataframe\n    counts_df = pd.DataFrame(results_dict)\n    \n    return counts_df\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:42:20.020186Z","iopub.execute_input":"2023-10-27T08:42:20.020555Z","iopub.status.idle":"2023-10-27T08:42:20.028446Z","shell.execute_reply.started":"2023-10-27T08:42:20.020526Z","shell.execute_reply":"2023-10-27T08:42:20.026696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = count_ocurrences_in_dataframe(text_past2years_df, 'cleaned_text', text_architectures_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:42:23.849873Z","iopub.execute_input":"2023-10-27T08:42:23.850225Z","iopub.status.idle":"2023-10-27T08:42:23.884859Z","shell.execute_reply.started":"2023-10-27T08:42:23.850202Z","shell.execute_reply":"2023-10-27T08:42:23.883991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax= plt.subplots(1, 1, figsize=(8,4))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Architectures')\nax.set_xlabel('Occurrences')\nax.set_title('Architectures used in Kaggle text data competitions', fontsize=12)\n#ax.set_limits([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:42:35.073248Z","iopub.execute_input":"2023-10-27T08:42:35.075143Z","iopub.status.idle":"2023-10-27T08:42:35.457081Z","shell.execute_reply.started":"2023-10-27T08:42:35.075097Z","shell.execute_reply":"2023-10-27T08:42:35.45583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: The top 3 architectures used in Kaggle text data competitions are BERT, DEBERTA, and ROBERTA.**","metadata":{}},{"cell_type":"markdown","source":"## Which of the following techniques is mostly used in the solutions of text data competitions?","metadata":{}},{"cell_type":"code","source":"techniques_keywords = [\n    \"pseudo labeling\",\n    \"masked language modeling\",\n    \"adversarial weight perturbation\",\n    \"model ensembling\",\n    \"model efficiency\",\n    \"data augmentation\"\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:43:53.116161Z","iopub.execute_input":"2023-10-27T08:43:53.116591Z","iopub.status.idle":"2023-10-27T08:43:53.123374Z","shell.execute_reply.started":"2023-10-27T08:43:53.116561Z","shell.execute_reply":"2023-10-27T08:43:53.121768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = count_ocurrences_in_dataframe(text_past2years_df, 'cleaned_text', techniques_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:43:55.869999Z","iopub.execute_input":"2023-10-27T08:43:55.870388Z","iopub.status.idle":"2023-10-27T08:43:55.899217Z","shell.execute_reply.started":"2023-10-27T08:43:55.870361Z","shell.execute_reply":"2023-10-27T08:43:55.897498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax= plt.subplots(1, 1, figsize=(8,4))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Techniques')\nax.set_xlabel('Occurrences')\nax.set_title('Trending NLP techniques found in Kaggle writeups')\n#ax.set_limits([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:43:59.320239Z","iopub.execute_input":"2023-10-27T08:43:59.320694Z","iopub.status.idle":"2023-10-27T08:43:59.544661Z","shell.execute_reply.started":"2023-10-27T08:43:59.320663Z","shell.execute_reply":"2023-10-27T08:43:59.542896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: Pseudo labeling is the most referred technique along with data augmentation. Interestingly, it seems that kagglers didn't worry at all about optimizing their model's efficiency.**","metadata":{}},{"cell_type":"markdown","source":"## Which of the following domains is mostly referred in the solutions of text data competitions?","metadata":{}},{"cell_type":"code","source":"domain_keywords = [\n     \"text mining\",\n     \"text analytics\",\n     \"text preprocessing\",\n     \"text classification\",\n     \"text clustering\",\n     \"named entity recognition\",\n     \"topic modeling\",\n     \"information retrieval\",\n     \"text summarization\",\n     \"text generation\",\n     \"text similarity\",\n     \"word embeddings\",\n     \"document classification\",\n     \"text feature extraction\",\n     \"text segmentation\",\n     \"text normalization\",\n     \"text corpora\",\n     \"textual data analysis\",\n     \"question answering\",\n     \"sentiment analysis\",\n     \"language modeling\"    \n]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:47:37.709257Z","iopub.execute_input":"2023-10-27T08:47:37.70966Z","iopub.status.idle":"2023-10-27T08:47:37.716633Z","shell.execute_reply.started":"2023-10-27T08:47:37.70963Z","shell.execute_reply":"2023-10-27T08:47:37.715074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = count_ocurrences_in_dataframe(text_past2years_df, 'cleaned_text', domain_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:47:40.27214Z","iopub.execute_input":"2023-10-27T08:47:40.272502Z","iopub.status.idle":"2023-10-27T08:47:40.322783Z","shell.execute_reply.started":"2023-10-27T08:47:40.27248Z","shell.execute_reply":"2023-10-27T08:47:40.321441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax= plt.subplots(1, 1, figsize=(8,6))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Domains')\nax.set_xlabel('Occurrences')\nax.set_title('NLP domains found in Kaggle writeups')\nax.set_xlim([0, 12])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:47:43.624366Z","iopub.execute_input":"2023-10-27T08:47:43.624748Z","iopub.status.idle":"2023-10-27T08:47:43.973811Z","shell.execute_reply.started":"2023-10-27T08:47:43.62472Z","shell.execute_reply":"2023-10-27T08:47:43.972726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: Question and answering and text classification domains are the most referred in the collection of writeups.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# Freeing up memory \ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:52:12.58752Z","iopub.execute_input":"2023-10-27T08:52:12.587993Z","iopub.status.idle":"2023-10-27T08:52:13.006191Z","shell.execute_reply.started":"2023-10-27T08:52:12.587958Z","shell.execute_reply":"2023-10-27T08:52:13.004504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 4. Analyzing the Arxiv Dataset \n\nLikewise the Kaggle writeup dataset, we're going to analyze the Arxiv dataset to gain more insights about the strategies followed in Academia related to text data.","metadata":{}},{"cell_type":"code","source":"# Loading the Arxiv Dataset\ndf_arxiv = pd.read_json(\n    '/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json',\n    lines = True, \n    convert_dates = True, \n    chunksize = 100000\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:54:51.318436Z","iopub.execute_input":"2023-10-27T08:54:51.318794Z","iopub.status.idle":"2023-10-27T08:54:51.327803Z","shell.execute_reply.started":"2023-10-27T08:54:51.318755Z","shell.execute_reply":"2023-10-27T08:54:51.326477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading a single chunk from the Arxiv dataset \nfor chunk in df_arxiv:\n    break\nlen(chunk)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:55:15.930977Z","iopub.execute_input":"2023-10-27T08:55:15.931385Z","iopub.status.idle":"2023-10-27T08:55:22.067761Z","shell.execute_reply.started":"2023-10-27T08:55:15.931356Z","shell.execute_reply":"2023-10-27T08:55:22.066451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:55:24.458457Z","iopub.execute_input":"2023-10-27T08:55:24.458835Z","iopub.status.idle":"2023-10-27T08:55:24.484411Z","shell.execute_reply.started":"2023-10-27T08:55:24.458806Z","shell.execute_reply":"2023-10-27T08:55:24.48329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:55:36.633169Z","iopub.execute_input":"2023-10-27T08:55:36.633624Z","iopub.status.idle":"2023-10-27T08:55:37.123343Z","shell.execute_reply.started":"2023-10-27T08:55:36.633592Z","shell.execute_reply":"2023-10-27T08:55:37.121804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading all chunks and concatenating them into a single dataframe\narxiv_df = pd.DataFrame()\nfor chunk in df_arxiv:\n    arxiv_df = pd.concat([arxiv_df, chunk], ignore_index=True)\narxiv_df.head(5) ","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:55:47.306972Z","iopub.execute_input":"2023-10-27T08:55:47.307332Z","iopub.status.idle":"2023-10-27T08:58:21.525739Z","shell.execute_reply.started":"2023-10-27T08:55:47.307309Z","shell.execute_reply":"2023-10-27T08:58:21.524578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arxiv_df.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:58:54.04273Z","iopub.execute_input":"2023-10-27T08:58:54.04318Z","iopub.status.idle":"2023-10-27T08:59:00.154062Z","shell.execute_reply.started":"2023-10-27T08:58:54.04315Z","shell.execute_reply":"2023-10-27T08:59:00.152485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We found around 2.15 M papers in the Arxiv dataset. This dataframe has a memory usage of 4.0 GB.**","metadata":{}},{"cell_type":"markdown","source":"## How many distinct categories of papers are present in the ArXiv Dataset? ","metadata":{}},{"cell_type":"code","source":"print(f\"The Arxiv Dataset has {arxiv_df['categories'].nunique()} unique categories\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:59:30.901355Z","iopub.execute_input":"2023-10-27T08:59:30.901688Z","iopub.status.idle":"2023-10-27T08:59:31.614368Z","shell.execute_reply.started":"2023-10-27T08:59:30.901664Z","shell.execute_reply":"2023-10-27T08:59:31.612793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: The Arxiv dataset has around 76 k categories**","metadata":{}},{"cell_type":"markdown","source":"## What are the earliest and latest papers in the Arxiv Dataset?","metadata":{}},{"cell_type":"code","source":"# Turning the 'update_date' column into a datetime format column \narxiv_df['update_date'] = pd.to_datetime(arxiv_df['update_date'])\narxiv_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T08:59:48.002958Z","iopub.execute_input":"2023-10-27T08:59:48.0034Z","iopub.status.idle":"2023-10-27T08:59:49.155352Z","shell.execute_reply.started":"2023-10-27T08:59:48.003367Z","shell.execute_reply":"2023-10-27T08:59:49.15389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arxiv_date_min = arxiv_df['update_date'].min().strftime('%Y-%m-%d')\narxiv_date_max = arxiv_df['update_date'].max().strftime('%Y-%m-%d')\nprint(f\"The Arxiv Dataset includes papers from {arxiv_date_min} to {arxiv_date_max}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:00:01.341548Z","iopub.execute_input":"2023-10-27T09:00:01.341911Z","iopub.status.idle":"2023-10-27T09:00:01.357096Z","shell.execute_reply.started":"2023-10-27T09:00:01.341885Z","shell.execute_reply":"2023-10-27T09:00:01.355463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer:  The Arxiv Dataset contains papers from <u>2007-05-23</u> to <u>2023-05-05</u>**","metadata":{}},{"cell_type":"markdown","source":"## How many papers has the Arxiv Dataset from the past two years? ","metadata":{}},{"cell_type":"code","source":"arxiv_2years_df = arxiv_df[(arxiv_df['update_date'].dt.year >= 2021) & (arxiv_df['update_date'].dt.month >= 1)].copy()\narxiv_2years_df = arxiv_2years_df.reset_index(drop=True)\narxiv_2years_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:00:19.445904Z","iopub.execute_input":"2023-10-27T09:00:19.446286Z","iopub.status.idle":"2023-10-27T09:00:20.549873Z","shell.execute_reply.started":"2023-10-27T09:00:19.44626Z","shell.execute_reply":"2023-10-27T09:00:20.54828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The Arxiv Dataset contains {len(arxiv_2years_df)} papers from January 2021 to 2023\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:00:27.003618Z","iopub.execute_input":"2023-10-27T09:00:27.003977Z","iopub.status.idle":"2023-10-27T09:00:27.010273Z","shell.execute_reply.started":"2023-10-27T09:00:27.003954Z","shell.execute_reply":"2023-10-27T09:00:27.00862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: The Arxiv Dataset contains around 527 k papers from January 2021 to 2023**","metadata":{}},{"cell_type":"markdown","source":"## What are the papers with most categories from the past two years?","metadata":{}},{"cell_type":"code","source":"arxiv_2years_df['categories'].value_counts().head(20)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:00:40.6935Z","iopub.execute_input":"2023-10-27T09:00:40.693948Z","iopub.status.idle":"2023-10-27T09:00:40.910126Z","shell.execute_reply.started":"2023-10-27T09:00:40.693892Z","shell.execute_reply":"2023-10-27T09:00:40.909012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: Computer Vision takes the lead on number of papers followed by quantum physics.**","metadata":{}},{"cell_type":"markdown","source":"## What are the papers with less categories from the past two years?","metadata":{}},{"cell_type":"code","source":"arxiv_2years_df['categories'].value_counts().tail(20)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:01:09.379395Z","iopub.execute_input":"2023-10-27T09:01:09.37977Z","iopub.status.idle":"2023-10-27T09:01:09.558055Z","shell.execute_reply.started":"2023-10-27T09:01:09.379744Z","shell.execute_reply":"2023-10-27T09:01:09.55608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: We noticed that categories that are concatenated in a single row are hard to identify as unique categories**","metadata":{}},{"cell_type":"markdown","source":"## What is the number of NLP-related papers from the past two years?\n\nTo identify the most popular categories in the NLP domain in the Arxiv Dataset, we searched the term **Natural Language Processing** in the [arxiv](https://arxiv.org/) dataset from 2021-01 to 2023. Then we ordered the results by **Annnoucement date (oldest first)** and then by **Annnoucement date (newest first)** and identified 19 categories that were mostly referred by researchers.","metadata":{}},{"cell_type":"code","source":"# Filtering out only text data related papers from the Arxiv dataset\nnlp_categories_arxiv = [\n    \"cs.SE\",      # Software Engineering\n    \"cs.CY\",      # Computers and Society\n    \"cs.IR\",      # Information Retrieval\n    \"cs.CL\",      # Computation and Language\n    \"cs.LG\",      # Machine Learning\n    \"cs.NE\",      # Neural and Evolutionary Computing\n    \"cs.AI\",      # Artificial Intelligence\n    \"cs.DL\",      # Digital Libraries\n    \"cs.HC\",      # Human Computer Interaction\n    \"cs.SI\",      # Social and Information Networks\n    \"stat.ML\",    # Machine Learning\n    \"cs.SD\",      # Sound\n    \"cs.CR\",      # Cryptography and Security\n    \"q-fin.ST\",   # Statistical Finance\n    \"quant-ph\",   # Quantum Physics\n    \"q-bio.OT\",   # Other Quantitative Biology\n    \"physics.comp-ph\", # Computational Physics\n    \"physics.data-an\", # Data Analysis, Statistics, and Probability\n    \"cs.AR\"            # Hardware Architecture\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:04:05.979411Z","iopub.execute_input":"2023-10-27T09:04:05.979781Z","iopub.status.idle":"2023-10-27T09:04:05.98622Z","shell.execute_reply.started":"2023-10-27T09:04:05.979754Z","shell.execute_reply":"2023-10-27T09:04:05.985007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_arxiv_2years_df = arxiv_2years_df[arxiv_2years_df.categories.isin(nlp_categories_arxiv)]\nnlp_arxiv_2years_df = nlp_arxiv_2years_df.reset_index(drop=True)\nprint(f\"Number of papers from the past two years: {len(arxiv_2years_df)} \\nNumber of NLP-related papers from the past two years: {len(nlp_arxiv_2years_df)}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:04:11.295063Z","iopub.execute_input":"2023-10-27T09:04:11.2955Z","iopub.status.idle":"2023-10-27T09:04:11.432093Z","shell.execute_reply.started":"2023-10-27T09:04:11.29547Z","shell.execute_reply":"2023-10-27T09:04:11.431448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_arxiv_2years_df.sort_values('update_date', ascending=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:04:21.894604Z","iopub.execute_input":"2023-10-27T09:04:21.894939Z","iopub.status.idle":"2023-10-27T09:04:21.963646Z","shell.execute_reply.started":"2023-10-27T09:04:21.894898Z","shell.execute_reply":"2023-10-27T09:04:21.961262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: There is a total of 527 k papers from the past two years (from January 2021 to May 2023) and around 46 k of them corresponding to the NLP domain.**","metadata":{}},{"cell_type":"markdown","source":"## What is the count of NLP-related papers per category from the past two years?","metadata":{}},{"cell_type":"code","source":"nlp_keywords_serie = nlp_arxiv_2years_df['categories'].value_counts()\nnlp_keywords_serie","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:10:18.209509Z","iopub.execute_input":"2023-10-27T09:10:18.209898Z","iopub.status.idle":"2023-10-27T09:10:18.232775Z","shell.execute_reply.started":"2023-10-27T09:10:18.209869Z","shell.execute_reply":"2023-10-27T09:10:18.231765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nkeywords_count_df = pd.DataFrame({'Keywords': nlp_keywords_serie.index,'Count': nlp_keywords_serie.values})\nfig = px.bar(keywords_count_df, x='Count', y='Keywords', title='NLP-related papers per category found in the Arxiv dataset', orientation='h', width=750, height=900, color='Keywords')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:12:33.393193Z","iopub.execute_input":"2023-10-27T09:12:33.393516Z","iopub.status.idle":"2023-10-27T09:12:33.522571Z","shell.execute_reply.started":"2023-10-27T09:12:33.393492Z","shell.execute_reply":"2023-10-27T09:12:33.521086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: Interestingly, Quantum Physics takes the lead in NLP-related papers followed by Computation and Language, and Machine Learning papers.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 5. Extracting keywords from Arxiv papers\nIn this section, we analyze the content of abstracts of papers using the PKE (Python Keyword Extraction) module to identify main keywords.  ","metadata":{}},{"cell_type":"markdown","source":"## Cleaning the Arxiv Dataset \nBefore stepping into this task, it's paramount to implement a cleaning text data stage. ","metadata":{}},{"cell_type":"code","source":"# Eliminating duplicates \nnlp_arxiv_2years_cleaned_df = nlp_arxiv_2years_df.drop_duplicates(subset=['title'])\nlen(nlp_arxiv_2years_df), len(nlp_arxiv_2years_cleaned_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:14:50.791029Z","iopub.execute_input":"2023-10-27T09:14:50.791404Z","iopub.status.idle":"2023-10-27T09:14:50.833438Z","shell.execute_reply.started":"2023-10-27T09:14:50.791379Z","shell.execute_reply":"2023-10-27T09:14:50.832551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing a sample abstract \nnlp_arxiv_2years_cleaned_df['abstract'][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:15:07.790728Z","iopub.execute_input":"2023-10-27T09:15:07.79113Z","iopub.status.idle":"2023-10-27T09:15:07.799896Z","shell.execute_reply.started":"2023-10-27T09:15:07.791102Z","shell.execute_reply":"2023-10-27T09:15:07.798874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a function that performs several text data cleaning steps \ndef clean_text(df, col_to_clean):\n\n    # Remove HTML tags\n    df['cleaned_text'] = df[col_to_clean].apply(lambda x: re.sub('<[^<]+?>', ' ', x))\n \n    # Remove brackets and apostrophes from Python lists\n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r\"[\\[\\]'\\\"]\",\" \", x))\n    \n    # Remove change of line characters \n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"\\n\", \" \", regex=True)\n   \n    # Remove special characters\n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"-\", \"\", regex=False)\n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"[^a-zA-Z0-9 ]\", \"\", regex=True)\n     \n    # Lowercase text\n    df['cleaned_text'] = df['cleaned_text'].str.lower()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:15:11.750615Z","iopub.execute_input":"2023-10-27T09:15:11.751087Z","iopub.status.idle":"2023-10-27T09:15:11.758707Z","shell.execute_reply.started":"2023-10-27T09:15:11.751056Z","shell.execute_reply":"2023-10-27T09:15:11.757576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the `clean_text()` function on abstracts\nnlp_arxiv_2years_copy_df = nlp_arxiv_2years_cleaned_df.copy()\nnlp_arxiv_2years_copy_df = clean_text(nlp_arxiv_2years_copy_df, 'abstract')\n\n# Printing a cleaned abstract\nnlp_arxiv_2years_copy_df['cleaned_text'][0]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:15:13.771193Z","iopub.execute_input":"2023-10-27T09:15:13.771565Z","iopub.status.idle":"2023-10-27T09:15:14.889407Z","shell.execute_reply.started":"2023-10-27T09:15:13.771535Z","shell.execute_reply":"2023-10-27T09:15:14.887995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_arxiv_2years_copy_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:15:18.978509Z","iopub.execute_input":"2023-10-27T09:15:18.978885Z","iopub.status.idle":"2023-10-27T09:15:19.000269Z","shell.execute_reply.started":"2023-10-27T09:15:18.978857Z","shell.execute_reply":"2023-10-27T09:15:18.998904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Computing keyword extraction on the Arxiv Dataset\nThe keyword extraction stage is based on the [TfIdf](https://boudinfl.github.io/pke/build/html/unsupervised.html) (Term Frequency-Inverse Document Frequency) method from PKE. *TfIdf is a popular and effective technique for identifying keyphrases in a collection of text documents.* We have created the `extract_keywords()` function to extract the top 5 keywords from each abstract. ","metadata":{}},{"cell_type":"code","source":"# Cleaning up memory \nimport gc\n\ndel df_arxiv\ndel chunk\ndel arxiv_df\ndel arxiv_2years_df\ndel nlp_arxiv_2years_df\ndel nlp_arxiv_2years_cleaned_df\n\ngc.collect() ","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:15:27.263778Z","iopub.execute_input":"2023-10-27T09:15:27.264736Z","iopub.status.idle":"2023-10-27T09:15:35.2796Z","shell.execute_reply.started":"2023-10-27T09:15:27.264699Z","shell.execute_reply":"2023-10-27T09:15:35.277875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/boudinfl/pke.git","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:15:48.371274Z","iopub.execute_input":"2023-10-27T09:15:48.371656Z","iopub.status.idle":"2023-10-27T09:16:01.101607Z","shell.execute_reply.started":"2023-10-27T09:15:48.371627Z","shell.execute_reply":"2023-10-27T09:16:01.100098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_keywords(text):\n    stoplist = list(string.punctuation)\n    stoplist += pke.lang.stopwords.get('en')\n    extractor = pke.unsupervised.TfIdf()\n    extractor.load_document(input=text,\n                           language='en',\n                           stoplist=stoplist,\n                           normalization=None)\n \n    extractor.candidate_selection() #Select 1 to 3 grams\n    extractor.candidate_weighting() #Candidate weighting using document frequencies\n    keyphrases = extractor.get_n_best(n=10)\n    \n    # Extract top 5 keywords\n    keywords = [keyword[0] for keyword in keyphrases[:5]]\n   \n    return keywords","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:20:34.123956Z","iopub.execute_input":"2023-10-27T09:20:34.124394Z","iopub.status.idle":"2023-10-27T09:20:34.132574Z","shell.execute_reply.started":"2023-10-27T09:20:34.124364Z","shell.execute_reply":"2023-10-27T09:20:34.130694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_arxiv_2years_copy_df['cleaned_text'].info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:20:38.562948Z","iopub.execute_input":"2023-10-27T09:20:38.563377Z","iopub.status.idle":"2023-10-27T09:20:38.600854Z","shell.execute_reply.started":"2023-10-27T09:20:38.563353Z","shell.execute_reply":"2023-10-27T09:20:38.599361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Important: The size of text-related papers (52.5 MB) is 250 times larger than that of the processed Kaggle writeups collection (208 KB). This implies that the keyword extraction process for the ArXiv dataset will experience a significant RAM memory overload if using the standard CPU settings. For this reason, we have selected only 5,000 abstracts to prove that the keyword extraction process works by identifying 25,000 keywords (5*5,000).**","metadata":{}},{"cell_type":"code","source":"# Selecting only 5,000 abstracts to be processed\nclean_abstract = nlp_arxiv_2years_copy_df['cleaned_text'][:5000]\nclean_abstract.info(memory_usage='deep')","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:27:09.291242Z","iopub.execute_input":"2023-10-27T09:27:09.291622Z","iopub.status.idle":"2023-10-27T09:27:09.304046Z","shell.execute_reply.started":"2023-10-27T09:27:09.291593Z","shell.execute_reply":"2023-10-27T09:27:09.302343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing a progress bar to track the keyword extraction process","metadata":{}},{"cell_type":"code","source":"!pip install tqdm","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:27:22.29202Z","iopub.execute_input":"2023-10-27T09:27:22.292421Z","iopub.status.idle":"2023-10-27T09:27:32.008581Z","shell.execute_reply.started":"2023-10-27T09:27:22.292393Z","shell.execute_reply":"2023-10-27T09:27:32.007109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:27:35.491278Z","iopub.execute_input":"2023-10-27T09:27:35.491698Z","iopub.status.idle":"2023-10-27T09:27:35.497885Z","shell.execute_reply.started":"2023-10-27T09:27:35.491657Z","shell.execute_reply":"2023-10-27T09:27:35.496552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a tqdm progress bar to track the keyword extraction process. It takes about 6 h\ncleanup_interval = 20\n\nwith tqdm(total=len(clean_abstract), desc=\"Processing\") as pbar:\n    def apply_with_progress(text):\n        result = extract_keywords(text)\n        pbar.update(1)  # Update the progress bar\n        # Check if it's time to clean up memory\n        if pbar.n % cleanup_interval == 0:\n            gc.collect()\n        return result\n\n    # Apply the function to the Series with progress tracking\n    abstract_keywords = clean_abstract.apply(apply_with_progress)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:37:56.700315Z","iopub.execute_input":"2023-10-27T09:37:56.700735Z","iopub.status.idle":"2023-10-27T15:08:48.885044Z","shell.execute_reply.started":"2023-10-27T09:37:56.700705Z","shell.execute_reply":"2023-10-27T15:08:48.882343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abstract_keywords[100:]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T15:32:09.757148Z","iopub.execute_input":"2023-10-27T15:32:09.757568Z","iopub.status.idle":"2023-10-27T15:32:09.770716Z","shell.execute_reply.started":"2023-10-27T15:32:09.757541Z","shell.execute_reply":"2023-10-27T15:32:09.769055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting identified keywords from 5,000 papers","metadata":{}},{"cell_type":"code","source":"# Finding the most popular keywords from 5,000 papers\nnlp_keywords_serie = abstract_keywords.explode()\nnlp_keywords_count = nlp_keywords_serie.value_counts()\nnlp_keywords_count_df = pd.DataFrame({'Keywords': nlp_keywords_count.index,'Count': nlp_keywords_count.values})\nnlp_keywords_count_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T15:27:17.199884Z","iopub.execute_input":"2023-10-27T15:27:17.20046Z","iopub.status.idle":"2023-10-27T15:27:17.243064Z","shell.execute_reply.started":"2023-10-27T15:27:17.20042Z","shell.execute_reply":"2023-10-27T15:27:17.242007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting the top 50 words\nnlp_keywords_count50_df = nlp_keywords_count_df[:50]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T15:27:58.237465Z","iopub.execute_input":"2023-10-27T15:27:58.237914Z","iopub.status.idle":"2023-10-27T15:27:58.244166Z","shell.execute_reply.started":"2023-10-27T15:27:58.237879Z","shell.execute_reply":"2023-10-27T15:27:58.242533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.bar(nlp_keywords_count50_df, x='Count', y='Keywords', title='Top 50 keywords found in 5,000 abstracts by the TfIdf method', orientation='h', width=750, height=900, color='Keywords')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T15:28:38.232912Z","iopub.execute_input":"2023-10-27T15:28:38.23339Z","iopub.status.idle":"2023-10-27T15:28:38.49537Z","shell.execute_reply.started":"2023-10-27T15:28:38.233355Z","shell.execute_reply":"2023-10-27T15:28:38.493798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filtering out duplicated keywords\nkeywords_tree = nlp_keywords_serie.to_list()\nset_keywords_tree = set(keywords_tree)\nlst_keywords_tree = list(set_keywords_tree)\nprint(f\"Total keywords: {len(keywords_tree)} \\nUnique keywords: {len(lst_keywords_tree)}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-27T15:30:44.566383Z","iopub.execute_input":"2023-10-27T15:30:44.566847Z","iopub.status.idle":"2023-10-27T15:30:44.58294Z","shell.execute_reply.started":"2023-10-27T15:30:44.566813Z","shell.execute_reply":"2023-10-27T15:30:44.581875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a word cloud image using stylecloud\nstylecloud.gen_stylecloud(\n    text=' '.join(lst_keywords_tree), \n    icon_name='fas fa-tree',                     # 'fas fa-cloud'; 'fas fa-eye'; ''\n    palette='cmocean.sequential.Matter_10',\n    background_color='black',\n    gradient='horizontal',\n    size=1024\n)\nImage(filename=\"./stylecloud.png\", width=1024, height=768)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T15:31:39.182957Z","iopub.execute_input":"2023-10-27T15:31:39.183393Z","iopub.status.idle":"2023-10-27T15:31:52.898677Z","shell.execute_reply.started":"2023-10-27T15:31:39.18336Z","shell.execute_reply":"2023-10-27T15:31:52.896783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 6. Examining popular architectures, domains, and techniques in the Arxiv dataset based on word occurrences\n\nLikewise the analysis on the Kaggle writeup dataset, in this section we make specific questions and provide keywords to narrow down our analysis. We will focus specifically on the techniques, text data domains, and architectures mostly employed in the papers of the Arxiv dataset. ","metadata":{}},{"cell_type":"markdown","source":"## What are the main architectures used in Academia?\nWe considered the following 16 architectures as keywords for this question. ","metadata":{}},{"cell_type":"code","source":"text_architectures_keywords = [\n    \"fasttext\", \"roberta\", \"bert\", \"gpt\", \"rnn\", \"cnn\", \"gru\", \"t5\", \"electra\", \"xlnet\",\n    \"encoder\", \"decoder\", \"lstm\", \"transformer\", \"deberta\", \"codebert\"\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:30:03.309494Z","iopub.execute_input":"2023-10-27T09:30:03.309907Z","iopub.status.idle":"2023-10-27T09:30:03.31591Z","shell.execute_reply.started":"2023-10-27T09:30:03.309877Z","shell.execute_reply":"2023-10-27T09:30:03.314661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function that matchs a list of specific words with a column of a dataframe\ndef count_ocurrences_in_dataframe(df, column_name, strings_list):\n    # Convert the string_list input to a set format for faster membership checking\n    strings_set = set(strings_list)\n    \n    # Filter out the dataframe to only include rows where 'column_name' contains any of the strings in 'strings_list' \n    # This is used to create a regular expression pattern where the '|' pipe acts as an \"OR\" operator.\n    filtered_df = df[df[column_name].str.contains('|'.join(strings_set))]\n    \n    # Create a dictionary to store the counting results\n    results_dict = {'String': [], 'Occurrences':[]}\n    \n    # Iterate over the strings list\n    for string in strings_list:\n        # Add the string and its corresponding count to the dictionary\n        results_dict['String'].append(string)\n      \n        # Count the actual ocurrences in the filtered dataframe\n        actual_occurrences = filtered_df[column_name].str.count(string).sum()\n        results_dict['Occurrences'].append(actual_occurrences)\n    \n    # Convert the dictionary to a dataframe\n    counts_df = pd.DataFrame(results_dict)\n    \n    return counts_df\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:30:14.472124Z","iopub.execute_input":"2023-10-27T09:30:14.472497Z","iopub.status.idle":"2023-10-27T09:30:14.481645Z","shell.execute_reply.started":"2023-10-27T09:30:14.472473Z","shell.execute_reply":"2023-10-27T09:30:14.480154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = count_ocurrences_in_dataframe(nlp_arxiv_2years_copy_df, 'cleaned_text', text_architectures_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:30:18.447911Z","iopub.execute_input":"2023-10-27T09:30:18.449303Z","iopub.status.idle":"2023-10-27T09:30:20.059647Z","shell.execute_reply.started":"2023-10-27T09:30:18.449249Z","shell.execute_reply":"2023-10-27T09:30:20.058495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax= plt.subplots(1, 1, figsize=(8,4))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Architectures')\nax.set_xlabel('Occurrences')\nax.set_title('Architectures used in the Arxiv Dataset', fontsize=12)\n#ax.set_limits([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:30:25.549811Z","iopub.execute_input":"2023-10-27T09:30:25.550281Z","iopub.status.idle":"2023-10-27T09:30:25.913542Z","shell.execute_reply.started":"2023-10-27T09:30:25.55025Z","shell.execute_reply":"2023-10-27T09:30:25.911729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: It seems that BERT and encoder-based architectures take the lead in Academia along with trasformers.**","metadata":{}},{"cell_type":"markdown","source":"## Which of the following techniques is mostly used in Academia?","metadata":{}},{"cell_type":"code","source":"techniques_keywords = [\n    \"pseudo labeling\",\n    \"masked language modeling\",\n    \"adversarial weight perturbation\",\n    \"model ensembling\",\n    \"model efficiency\",\n    \"data augmentation\"\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:33:32.463439Z","iopub.execute_input":"2023-10-27T09:33:32.463864Z","iopub.status.idle":"2023-10-27T09:33:32.469693Z","shell.execute_reply.started":"2023-10-27T09:33:32.463833Z","shell.execute_reply":"2023-10-27T09:33:32.468235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = count_ocurrences_in_dataframe(nlp_arxiv_2years_copy_df, 'cleaned_text', techniques_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:33:38.900179Z","iopub.execute_input":"2023-10-27T09:33:38.900596Z","iopub.status.idle":"2023-10-27T09:33:39.46148Z","shell.execute_reply.started":"2023-10-27T09:33:38.900564Z","shell.execute_reply":"2023-10-27T09:33:39.46061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax= plt.subplots(1, 1, figsize=(8,4))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Techniques')\nax.set_xlabel('Occurrences')\nax.set_title('Trending NLP techniques found in Academia')\n#ax.set_limits([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:33:42.691594Z","iopub.execute_input":"2023-10-27T09:33:42.691976Z","iopub.status.idle":"2023-10-27T09:33:42.893326Z","shell.execute_reply.started":"2023-10-27T09:33:42.691948Z","shell.execute_reply":"2023-10-27T09:33:42.891694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: Unlike the text-related Kaggle writeups, researchers are more interested in data augmentation techniques rather than pseudo labeling.**","metadata":{}},{"cell_type":"markdown","source":"## Which of the following domains is mostly referred in Academia?","metadata":{}},{"cell_type":"code","source":"text_data_keywords = [\n     \"text mining\",\n     \"text analytics\",\n     \"text preprocessing\",\n     \"text classification\",\n     \"text clustering\",\n     \"named entity recognition\",\n     \"topic modeling\",\n     \"information retrieval\",\n     \"text summarization\",\n     \"text generation\",\n     \"text similarity\",\n     \"word embeddings\",\n     \"document classification\",\n     \"text feature extraction\",\n     \"text segmentation\",\n     \"text normalization\",\n     \"text corpora\",\n     \"textual data analysis\",\n     \"question answering\",\n     \"sentiment analysis\",\n     \"language modeling\"    \n]","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:35:03.865764Z","iopub.execute_input":"2023-10-27T09:35:03.866177Z","iopub.status.idle":"2023-10-27T09:35:03.871915Z","shell.execute_reply.started":"2023-10-27T09:35:03.866148Z","shell.execute_reply":"2023-10-27T09:35:03.870981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = count_ocurrences_in_dataframe(nlp_arxiv_2years_copy_df, 'cleaned_text', text_data_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:35:09.315713Z","iopub.execute_input":"2023-10-27T09:35:09.316087Z","iopub.status.idle":"2023-10-27T09:35:11.276996Z","shell.execute_reply.started":"2023-10-27T09:35:09.316059Z","shell.execute_reply":"2023-10-27T09:35:11.275002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax= plt.subplots(1, 1, figsize=(8,6))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Domains')\nax.set_xlabel('Occurrences')\nax.set_title('NLP domains found in Academia')\nax.set_xlim([0, 800])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T09:35:15.943884Z","iopub.execute_input":"2023-10-27T09:35:15.944318Z","iopub.status.idle":"2023-10-27T09:35:16.305012Z","shell.execute_reply.started":"2023-10-27T09:35:15.944286Z","shell.execute_reply":"2023-10-27T09:35:16.303319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Answer: Regarding the NLP domains, we found agreement of interest in both the Kaggle community and Academia focusing their efforts on question and answering and text classification fields.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nBy considering 9 text-data-related competitions instead of 5, we identified 208 writeups (70 times more data to be analyzed than in our previous EDA). This helped us gain a better understanding of Kaggle text data competitions. We've also expanded our consideration to 19 categories that could contain NLP papers, as opposed to the previous 12, for the ArXiv dataset. Here's a general summary of our findings over the last two years:\n\n* BERT and encoder-based architectures are the most popular in both the Kaggle community and academic contexts.\n* Pseudo labeling was the most frequently referenced technique in text data writeups, while data augmentation was more prevalent in text-related papers. It appears that researchers prioritize model efficiency, whereas Kagglers might overlook it in their solutions.\n* Both the Kaggle community and academia are increasingly focusing their efforts on Question and Answer (Q&A) and text classification domains.\n---\n\n# Appendix\nFinally, here are some useful tips for processing large datasets:\n\n1. Keep an eye on the RAM memory usage at every stage of your dataset analysis. You can:\n\n    * Assess the memory size of dataframes using the `df.info(memory_usage='deep')` command\n    * Consider removing dataframes that you no longer need with `del df`\n    * Free up memory whenever possible using the `gc.collect()` command.\n    * Use the following commands to assess the memory usage of your variables:\n    ```\n        from __future__ import print_function  # for Python2\n        import sys\n\n        local_vars = list(locals().items())\n        for var, obj in local_vars:\n        print(var, sys.getsizeof(obj))    \n    ```\n2. Implement a progress bar when executing large processes","metadata":{}}]}