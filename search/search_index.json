{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Portfolio","text":"<p>Hi, my name is Salomon Marquez. I\u2019m a versatile professional with a career that has evolved across multiple domains. From my early role as a Research Associate in academia to my position as a Senior Technical Writer in the software industry, I\u2019ve embraced diverse challenges and cultivated a broad skill set to adapt to new environments.</p> <p>With over 10 years of experience in academic writing and 5+ years in technical writing, I\u2019ve developed the ability to translate complex concepts into clear, accessible information for diverse audiences. As a Senior Technical Writer, I\u2019ve taken on roles as a mentor, technical lead, consultant, and open-source contributor, focusing on fostering collaboration with stakeholders and cross-functional teams.</p> <p>Currently, I\u2019m digging into Machine Learning, with a focus on Bioinformatics. This field connects my technical background with my passion for leveraging data-driven insights to address challenges in health and biology.</p>"},{"location":"#education","title":"Education","text":"<ul> <li> <p>Online Bioinformatics and Biostatistics Master's Degree (2025 \u2013 ) University of Barcelona (UB) and Universitat Oberta of Catalunya (UOC)</p> </li> <li> <p>Ph.D. in Electronics Engineering (2011-2016) Catalan Institute of Nanoscience and Nanotechnology, Autonomous University of Barcelona, Spain </p> </li> <li> <p>M.S. in Electrical Engineering (2008-2010) Center for Research and Advanced Studies of the National Polytechnic Institute (CINVESTAV-IPN), Mexico </p> </li> <li> <p>Student Exchange (Spring 2007) West Virginia University, Morgantown, WV Courses: Digital Image Processing, Antennas, Information Theory, Neuromorphic Analog VLSI </p> </li> <li> <p>B.S. in Communications and Electronics Engineering (2002-2008) University of Guanajuato, Mexico </p> </li> </ul>"},{"location":"#certifications","title":"Certifications","text":"<ul> <li>Machine Learning Specialization by Coursera (Stanford University &amp; DeepLearning.AI). Credential ID: <code>IDURPJ8T83YU</code>, Dec 2024.</li> <li>Gen\u00f3mica en la Pr\u00e1ctica Cl\u00ednica y en la Investigaci\u00f3n Biom\u00e9dica by CIMA University of Navarra. Certificate's link, Nov 2024.</li> <li>Introduction to Genomic Technologies by Coursera (Johns Hopkins University). Credential ID: <code>DWOKJIWQU6QB</code>, Aug 2024.</li> <li>DNA Decoded by Coursera (McMaster University). Credential ID: <code>XSDBE3B6DYRV</code>, Jul 2024.</li> <li>Feature Engineering by Kaggle. Certificate's link, Jul 2024.</li> <li>An\u00e1lisis de datos con R: Introducci\u00f3n y estad\u00edstica b\u00e1sica by Navarrabiomed and University of Navarra. Certificate's link, May 2024.</li> <li>Introducci\u00f3n al an\u00e1lisis de datos \u00f3micos transcript\u00f3mica by Navarrabiomed and University of Navarra. Certificate's link, May 2024.</li> <li>Improving Deep Neural Networks by Coursera (Stanford University &amp; DeepLearning.AI). Credential ID: <code>STXS68SE9XFG</code>, Apr 2024.</li> <li>AWS Cloud Practitioner by Amazon Web Services (AWS). Credential ID: <code>fa219cd5-5450-4123-a264-9ffa254c7e5a</code>, Apr 2024.</li> <li>Neural Networks and Deep Learning by Coursera (Stanford University &amp; DeepLearning.AI). Credential ID: <code>V5MFW67VBL8E</code>, Aug 2023.</li> <li>GCP Associate Cloud Engineer by Google Cloud. Credential ID: <code>85247f1da3e14987b6aa1b020f86d8cf</code>, Oct 2022.</li> <li>Data Engineering Nanodegree by Udacity. Credential ID: <code>GJQ6XA9K</code>, Jul 2021.</li> </ul>"},{"location":"#open-source-contributions","title":"Open Source Contributions","text":"<ul> <li>TimeCopilot: the GenAI Forecasting Agent Supporting the documentation of the project.</li> <li>Deploy Airbyte on Azure | Airbyte Created a guide to deploying Airbyte on a Microsoft Azure VM.</li> <li>WizelineLabs | Wizeline Created a repository containing documentation and licenses that served as a template to generate new open source projects at Wizeline. </li> <li>GitWize | Wizeline GitWize is an AWS cloud-based application that allows you to extract valuable insights and metrics from a single GitHub repository using git commit logs. I led the documentation strategy of this project. <ul> <li>GitWize Frontend</li> <li>GitWize Backend </li> </ul> </li> </ul>"},{"location":"#mentorship","title":"Mentorship","text":"<ul> <li>Bachelor of Biomedical Engineering of University of Guanajuato, Mexico (2020). Thesis: \u201cDispositivo microflu\u00eddico para enfoque hidrodin\u00e1mico de part\u00edculas\u201d by Rocio Lizbeth Olmos Ram\u00edrez</li> <li>Bachelor of Biomedical Engineering of University of Guanajuato, Mexico (2019). Thesis: \u201cCaracterizaci\u00f3n y Estudio de la Biocompatibilidad de Dispositivos Microflu\u00eddicos Fabricados por Estereolitograf\u00eda 3D\u201d by Osmar Moreno Rivas</li> </ul>"},{"location":"#relevant-publications","title":"Relevant Publications","text":"<ul> <li> <p>Liliana Badillo and Salomon Marquez, \u201cA Journey Through Kaggle Text Data Competitions From 2021 to 2023\u201d written for the 2023 Kaggle AI Report Competition. (2023)</p> </li> <li> <p>Monserrat del C Alonso-Murias, David Monz\u00f3n-Hern\u00e1ndez, Alfredo Maria Gravagnuolo, Salom\u00f3n Marquez, Paola Giardina and Eden Morales-Narv\u00e1ez,  \u201cGraphene oxide biohybrid layer enhances sensitivity and anticorrosive properties in refractive index sensor\u201d J. Phys. Photonics 3 034009 (2021)</p> </li> <li> <p>Salomon Marquez, Jie Liu, Eden Morales-Narvaez, \u201cPaper-based analytical devices in environmental applications and their integration with portable technologies\u201d Current Opinion in Environmental Science &amp; Health, vol. 10, pp. 1-8. (2019)</p> </li> <li> <p>V. Solis-Tinoco, S. Marquez, T. Quesada-Lopez, F. Villarroya, A. Homs-Corbera, and L. M. Lechuga, \u201cBuilding of a flexible microfluidic plasmo-nanomechanical biosensor for live cell analysis\u201d Sensors and Actuators B: Chemical, vol. 291, pp. 48\u201357. (2019)</p> </li> <li> <p>S. Marquez and E. Morales-Narv\u00e1ez, \u201cNanoplasmonics in Paper-Based Analytical Devices\u201d Front. Bioeng. Biotechnol., vol. 7. (2019)</p> </li> <li> <p>S. Marquez, M. Alvarez, J.A. Plaza, L.G. Villanueva, C. Dominguez, y L.M. Lechuga. \u201cAsymmetrically coupled resonators for mass sensing\u201d Appl. Phys. Lett. Volumen 11, no. 11, pages: 113101. (2017)</p> </li> <li> <p>S. Marquez, M. Alvarez, D. Fari\u00f1a, A Homs-Corbera, C. Dominguez and L. M. Lechuga. \"Array of microfluidic beam resonators for density and viscosity analysis of liquids\" IEEE Journal of Microelectromechanical Systems. DOI: 10.1109/JMEMS.2017.2709944. (2017) </p> </li> <li> <p>V. Solis-Tinoco, S. Marquez, B. Sepulveda y L.M. Lechuga. \u201cFabrication of well-ordered silicon nanopillars embedded in a microchannel via metal-assisted chemical etching: a route towards and opto-mechanical biosensor\u201d in RSC Advances. Volume 6, Pages: 85666-85674. (2016) </p> </li> <li> <p>D. Fari\u00f1a, M. \u00c1lvarez, S. M\u00e1rquez, C. Dominguez, and L. M. Lechuga \u201cOut-of-plane single-mode photonic microcantilevers for integrated nanomechanical sensing platform\u201d in Sensors and Actuators B: Chemical. Volume: 232, Pages: 60-67. (2016)</p> </li> <li> <p>D. Fari\u00f1a, M. \u00c1lvarez, S. M\u00e1rquez, C. Dominguez, and L. M. Lechuga.  \u201cSensitivity analysis for improving nanomechanical photonic transducers biosensors\u201d in Journal of Physics D: Applied Physics. Volume: 48(33), Pages: 335401. (2015)</p> </li> </ul>"},{"location":"#proceedings","title":"Proceedings","text":"<ul> <li> <p>O. Moreno-Rivas, D. Hern\u00e1ndez-Vel\u00e1zquez, V. Piazza, and S. Marquez, \u201cRapid prototyping of microfluidic devices by SL 3D printing and their biocompatibility study for cell culturing\u201d Materials Today: Proceedings, vol. 13, pp. 436\u2013445. (2019)</p> </li> <li> <p>S. Marquez, M. Alvarez, D. Fari\u00f1a, C. Dominguez and L. M. Lechuga. \"Simulation and characterization of hollow microbridge resonators for label-free biosensing\" Proc. SPIE 9518, Bio-MEMS and Medical Microdevices II, 95180U; doi:10.1117/12.2178981. (2015)</p> </li> <li> <p>S. Marquez, M. Alvarez, D. Farina, C. Dominguez, and L. M. Lechuga, \u201cTowards a biosensing multiple platform based on an array of hollow microbridge resonators\u201d in IEEE SENSORS, ISBN: 978-1-4799-0162-3, pp. 329\u2013331. (2014)</p> </li> <li> <p>S. Marquez, L. Leija and A. Vera. \u201cInfluence of Temperature Variations on the Average Grayscale of B-mode Images of HIFU-Induced Lesions\u201d in 2010 Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), ISBN: 978-1-4244-4123-5. (2010)</p> </li> <li> <p>S. Marquez, L. Leija and A. Vera. \u201cCharacterization of ultrasound images of HIFU-induced lesions by extraction of its morphological properties\u201d, 2010 7th International Conference on Electrical Engineering Computing Science and Automatic Control (CCE), ISBN: 978-1-4244-7314-4. (2010)</p> </li> </ul>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/","title":"Clinical Factors Associated with Survival in Gastrointestinal Cancer","text":"<p>by Sefora Conti and Salomon Marquez</p> <p>15/06/2025 </p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#1-objective","title":"1 Objective","text":"<p>Gastrointestinal cancer encompasses a group of malignant and heterogeneous neoplasms that affect different organs of the digestive system, including the biliary tract, the stomach, the pancreas, the colon, and the rectum. These diseases represent a significant proportion of the global cancer burden and often have unfavorable prognoses (1, 2). Identifying clinico-pathological factors that influence patients\u2019 overall survival is essential for improving risk stratification, therapeutic decision-making, and the design of personalized follow-up strategies.</p> <p>In recent years, several approaches have been developed to improve this prediction, including in vitro models (such as patient-derived organoids)(3\u20135), in vivo models (for example, xenografts in animal models)(6), and in silico strategies(7\u20139). The latter rely on different types of data, such as histopathological images(10), magnetic resonance imaging(11), or clinical information, to generate tools that help personalize cancer treatment.</p> <p>In the present work, we analyze a publicly available clinical dataset from the TCGA (The Cancer Genome Atlas Program) repository using R, with the aim of identifying potential clinical factors associated with the survival of patients with gastrointestinal tumors. This analysis will explore possible correlations between pathological and demographic variables and overall survival, with the aim of providing evidence that improves patient stratification.</p> <p>The data used are open access, anonymized, and their use for academic purposes is permitted under the policies of the GDC Data Portal.</p> <p>Visit the  repository of the project to check out: -  TCGA data -  R code -  Shiny application  </p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#2-data-preparation","title":"2 Data Preparation","text":""},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#21-initial-dataset-exploration","title":"2.1 Initial Dataset Exploration","text":"<p>The following table shows a general description of the TCGA dataset. The following sections show how this information was obtained.</p> Key Points Description File Type and Data The dataset was imported from a semicolon-delimited .csv file, containing clinical and pathological information of cancer patients. It includes variables such as age, cancer type, presence of venous or perineural invasion, microsatellite instability, and follow-up variables such as days to last contact or death. Due to its combined origin (different clinical sources within TCGA), the dataset requires considerable preprocessing to make it suitable for analysis. Available Variables Categorical variables (e.g., cancer type, sex) and continuous variables (age, weight, height, etc.) Dataset Dimensions The dataset includes 154 clinical variables from 1,308 patients, of which we have selected 34 variables considered most relevant for the study objective. Detection of Missing Values There are 135,291 missing values in the original dataset and 21,007 in the new dataset (with the 34 selected variables). Inconsistencies Typical inconsistencies in clinical data were detected, such as mismatches between vital status and follow-up or death dates. To calculate overall survival, it was necessary to create a new survival time variable (<code>os_time</code>) combining <code>days_to_death</code> and <code>days_to_last_followup</code>, as well as a binary event variable (<code>os_event</code>). <pre><code># Cargar el conjunto de datos (csv)\ndf_TCGA &lt;- read.csv(\"Combined_TCGA_Clinical_300525.csv\",header=TRUE, sep = \";\")\n# Visualizar registros\nhead(df_TCGA[,1:5], n=3) #mostramos los primeros 3 registros\n</code></pre> <pre><code>##   bcr_patient_barcode additional_studies tumor_tissue_site\n## 1        TCGA-3L-AA1B               &lt;NA&gt;             Colon\n## 2        TCGA-4N-A93T               &lt;NA&gt;             Colon\n## 3        TCGA-4T-AA8H               &lt;NA&gt;             Colon\n##               histological_type other_dx\n## 1          Colon Adenocarcinoma       No\n## 2          Colon Adenocarcinoma       No\n## 3 Colon Mucinous Adenocarcinoma       No\n</code></pre> <pre><code>tail(df_TCGA[,1:5], n=3) #mostramos los ultimos 3 registros\n</code></pre> <pre><code>##      bcr_patient_barcode additional_studies tumor_tissue_site\n## 1306        TCGA-Z5-AAPL               &lt;NA&gt;          Pancreas\n## 1307                &lt;NA&gt;               &lt;NA&gt;              &lt;NA&gt;\n## 1308                                                         \n##                        histological_type other_dx\n## 1306 Pancreas-Adenocarcinoma Ductal Type       No\n## 1307                                &lt;NA&gt;     &lt;NA&gt;\n## 1308\n</code></pre> <pre><code># Obtener el n\u00famero de variables y sus nombres\ndim(df_TCGA) #dimensiones de df\n</code></pre> <pre><code>## [1] 1308  154\n</code></pre> <pre><code>names(df_TCGA) #nombres de las variables\n</code></pre> <pre><code>##   [1] \"bcr_patient_barcode\"                                       \n##   [2] \"additional_studies\"                                        \n##   [3] \"tumor_tissue_site\"                                         \n##   [4] \"histological_type\"                                         \n##   [5] \"other_dx\"                                                  \n##   [6] \"gender\"                                                    \n##   [7] \"vital_status\"                                              \n##   [8] \"days_to_birth\"                                             \n##   [9] \"days_to_last_known_alive\"                                  \n##  [10] \"days_to_death\"                                             \n##  [11] \"days_to_last_followup\"                                     \n##  [12] \"race_list\"                                                 \n##  [13] \"tissue_source_site\"                                        \n##  [14] \"patient_id\"                                                \n##  [15] \"bcr_patient_uuid\"                                          \n##  [16] \"history_of_neoadjuvant_treatment\"                          \n##  [17] \"informed_consent_verified\"                                 \n##  [18] \"icd_o_3_site\"                                              \n##  [19] \"icd_o_3_histology\"                                         \n##  [20] \"icd_10\"                                                    \n##  [21] \"tissue_prospective_collection_indicator\"                   \n##  [22] \"tissue_retrospective_collection_indicator\"                 \n##  [23] \"days_to_initial_pathologic_diagnosis\"                      \n##  [24] \"age_at_initial_pathologic_diagnosis\"                       \n##  [25] \"year_of_initial_pathologic_diagnosis\"                      \n##  [26] \"person_neoplasm_cancer_status\"                             \n##  [27] \"ethnicity\"                                                 \n##  [28] \"weight\"                                                    \n##  [29] \"height\"                                                    \n##  [30] \"day_of_form_completion\"                                    \n##  [31] \"month_of_form_completion\"                                  \n##  [32] \"year_of_form_completion\"                                   \n##  [33] \"residual_tumor\"                                            \n##  [34] \"anatomic_neoplasm_subdivision\"                             \n##  [35] \"primary_lymph_node_presentation_assessment\"                \n##  [36] \"lymph_node_examined_count\"                                 \n##  [37] \"number_of_lymphnodes_positive_by_he\"                       \n##  [38] \"number_of_lymphnodes_positive_by_ihc\"                      \n##  [39] \"preoperative_pretreatment_cea_level\"                       \n##  [40] \"non_nodal_tumor_deposits\"                                  \n##  [41] \"circumferential_resection_margin\"                          \n##  [42] \"venous_invasion\"                                           \n##  [43] \"lymphatic_invasion\"                                        \n##  [44] \"perineural_invasion_present\"                               \n##  [45] \"microsatellite_instability\"                                \n##  [46] \"number_of_loci_tested\"                                     \n##  [47] \"number_of_abnormal_loci\"                                   \n##  [48] \"kras_gene_analysis_performed\"                              \n##  [49] \"kras_mutation_found\"                                       \n##  [50] \"kras_mutation_codon\"                                       \n##  [51] \"braf_gene_analysis_performed\"                              \n##  [52] \"braf_gene_analysis_result\"                                 \n##  [53] \"synchronous_colon_cancer_present\"                          \n##  [54] \"history_of_colon_polyps\"                                   \n##  [55] \"colon_polyps_present\"                                      \n##  [56] \"loss_expression_of_mismatch_repair_proteins_by_ihc\"        \n##  [57] \"loss_expression_of_mismatch_repair_proteins_by_ihc_results\"\n##  [58] \"number_of_first_degree_relatives_with_cancer_diagnosis\"    \n##  [59] \"radiation_therapy\"                                         \n##  [60] \"postoperative_rx_tx\"                                       \n##  [61] \"primary_therapy_outcome_success\"                           \n##  [62] \"has_new_tumor_events_information\"                          \n##  [63] \"has_drugs_information\"                                     \n##  [64] \"has_radiations_information\"                                \n##  [65] \"has_follow_ups_information\"                                \n##  [66] \"project\"                                                   \n##  [67] \"stage_event_system_version\"                                \n##  [68] \"stage_event_clinical_stage\"                                \n##  [69] \"stage_event_pathologic_stage\"                              \n##  [70] \"stage_event_tnm_categories\"                                \n##  [71] \"stage_event_psa\"                                           \n##  [72] \"stage_event_gleason_grading\"                               \n##  [73] \"stage_event_ann_arbor\"                                     \n##  [74] \"stage_event_serum_markers\"                                 \n##  [75] \"stage_event_igcccg_stage\"                                  \n##  [76] \"stage_event_masaoka_stage\"                                 \n##  [77] \"cancer_type\"                                               \n##  [78] \"patient_death_reason\"                                      \n##  [79] \"anatomic_neoplasm_subdivision_other\"                       \n##  [80] \"neoplasm_histologic_grade\"                                 \n##  [81] \"country_of_procurement\"                                    \n##  [82] \"city_of_procurement\"                                       \n##  [83] \"reflux_history\"                                            \n##  [84] \"antireflux_treatment\"                                      \n##  [85] \"antireflux_treatment_types\"                                \n##  [86] \"barretts_esophagus\"                                        \n##  [87] \"h_pylori_infection\"                                        \n##  [88] \"family_history_of_stomach_cancer\"                          \n##  [89] \"number_of_relatives_with_stomach_cancer\"                   \n##  [90] \"relative_family_cancer_history\"                            \n##  [91] \"cancer_first_degree_relative\"                              \n##  [92] \"blood_relative_cancer_history_list\"                        \n##  [93] \"history_hepato_carcinoma_risk_factors\"                     \n##  [94] \"post_op_ablation_embolization_tx\"                          \n##  [95] \"eastern_cancer_oncology_group\"                             \n##  [96] \"primary_pathology_tumor_tissue_site\"                       \n##  [97] \"primary_pathology_histological_type\"                       \n##  [98] \"primary_pathology_specimen_collection_method_name\"         \n##  [99] \"primary_pathology_history_prior_surgery_type_other\"        \n## [100] \"primary_pathology_days_to_initial_pathologic_diagnosis\"    \n## [101] \"primary_pathology_age_at_initial_pathologic_diagnosis\"     \n## [102] \"primary_pathology_year_of_initial_pathologic_diagnosis\"    \n## [103] \"primary_pathology_neoplasm_histologic_grade\"               \n## [104] \"primary_pathology_residual_tumor\"                          \n## [105] \"primary_pathology_vascular_tumor_cell_type\"                \n## [106] \"primary_pathology_perineural_invasion_present\"             \n## [107] \"primary_pathology_child_pugh_classification_grade\"         \n## [108] \"primary_pathology_ca_19_9_level\"                           \n## [109] \"primary_pathology_ca_19_9_level_lower\"                     \n## [110] \"primary_pathology_ca_19_9_level_upper\"                     \n## [111] \"primary_pathology_fetoprotein_outcome_value\"               \n## [112] \"primary_pathology_fetoprotein_outcome_lower_limit\"         \n## [113] \"primary_pathology_fetoprotein_outcome_upper_limit\"         \n## [114] \"primary_pathology_platelet_result_count\"                   \n## [115] \"primary_pathology_platelet_result_lower_limit\"             \n## [116] \"primary_pathology_platelet_result_upper_limit\"             \n## [117] \"primary_pathology_prothrombin_time_result_value\"           \n## [118] \"primary_pathology_inter_norm_ratio_lower_limit\"            \n## [119] \"primary_pathology_intern_norm_ratio_upper_limit\"           \n## [120] \"primary_pathology_albumin_result_specified_value\"          \n## [121] \"primary_pathology_albumin_result_lower_limit\"              \n## [122] \"primary_pathology_albumin_result_upper_limit\"              \n## [123] \"primary_pathology_bilirubin_upper_limit\"                   \n## [124] \"primary_pathology_bilirubin_lower_limit\"                   \n## [125] \"primary_pathology_total_bilirubin_upper_limit\"             \n## [126] \"primary_pathology_creatinine_value_in_mg_dl\"               \n## [127] \"primary_pathology_creatinine_lower_level\"                  \n## [128] \"primary_pathology_creatinine_upper_limit\"                  \n## [129] \"primary_pathology_fibrosis_ishak_score\"                    \n## [130] \"primary_pathology_cholangitis_tissue_evidence\"             \n## [131] \"adenocarcinoma_invasion\"                                   \n## [132] \"histological_type_other\"                                   \n## [133] \"tumor_type\"                                                \n## [134] \"initial_pathologic_diagnosis_method\"                       \n## [135] \"init_pathology_dx_method_other\"                            \n## [136] \"surgery_performed_type\"                                    \n## [137] \"histologic_grading_tier_category\"                          \n## [138] \"maximum_tumor_dimension\"                                   \n## [139] \"source_of_patient_death_reason\"                            \n## [140] \"tobacco_smoking_history\"                                   \n## [141] \"year_of_tobacco_smoking_onset\"                             \n## [142] \"stopped_smoking_year\"                                      \n## [143] \"number_pack_years_smoked\"                                  \n## [144] \"alcohol_history_documented\"                                \n## [145] \"alcoholic_exposure_category\"                               \n## [146] \"frequency_of_alcohol_consumption\"                          \n## [147] \"amount_of_alcohol_consumption_per_day\"                     \n## [148] \"history_of_diabetes\"                                       \n## [149] \"days_to_diabetes_onset\"                                    \n## [150] \"history_of_chronic_pancreatitis\"                           \n## [151] \"days_to_pancreatitis_onset\"                                \n## [152] \"family_history_of_cancer\"                                  \n## [153] \"relative_cancer_types\"                                     \n## [154] \"history_prior_surgery_type_other\"\n</code></pre> <pre><code># Visualizar estructura del conjunto de datos y un resumen estad\u00edstico\nstr(df_TCGA)\n</code></pre> <pre><code>## 'data.frame':    1308 obs. of  154 variables:\n##  $ bcr_patient_barcode                                       : chr  \"TCGA-3L-AA1B\" \"TCGA-4N-A93T\" \"TCGA-4T-AA8H\" \"TCGA-5M-AAT4\" ...\n##  $ additional_studies                                        : chr  NA NA NA NA ...\n##  $ tumor_tissue_site                                         : chr  \"Colon\" \"Colon\" \"Colon\" \"Colon\" ...\n##  $ histological_type                                         : chr  \"Colon Adenocarcinoma\" \"Colon Adenocarcinoma\" \"Colon Mucinous Adenocarcinoma\" \"Colon Adenocarcinoma\" ...\n##  $ other_dx                                                  : chr  \"No\" \"No\" \"No\" \"No\" ...\n##  $ gender                                                    : chr  \"FEMALE\" \"MALE\" \"FEMALE\" \"MALE\" ...\n##  $ vital_status                                              : chr  NA NA NA \"Dead\" ...\n##  $ days_to_birth                                             : int  -22379 -24523 -15494 -27095 -14852 -27870 -16512 -31329 -30237 -26292 ...\n##  $ days_to_last_known_alive                                  : int  NA NA NA NA NA NA 424 NA NA NA ...\n##  $ days_to_death                                             : int  NA NA NA 49 290 NA NA 1126 NA NA ...\n##  $ days_to_last_followup                                     : num  475 146 385 -Inf -Inf ...\n##  $ race_list                                                 : chr  \"BLACK OR AFRICAN AMERICAN\" \"BLACK OR AFRICAN AMERICAN\" \"BLACK OR AFRICAN AMERICAN\" \"BLACK OR AFRICAN AMERICAN\" ...\n##  $ tissue_source_site                                        : chr  \"3L\" \"4N\" \"4T\" \"5M\" ...\n##  $ patient_id                                                : chr  \"AA1B\" \"A93T\" \"AA8H\" \"AAT4\" ...\n##  $ bcr_patient_uuid                                          : chr  \"A94E1279-A975-480A-93E9-7B1FF05CBCBF\" \"92554413-9EBC-4354-8E1B-9682F3A031D9\" \"A5E14ADD-1552-4606-9FFE-3A03BCF76640\" \"1136DD50-242A-4659-AAD4-C53F9E759BB3\" ...\n##  $ history_of_neoadjuvant_treatment                          : chr  \"No\" \"No\" \"No\" \"No\" ...\n##  $ informed_consent_verified                                 : chr  \"YES\" \"YES\" \"YES\" \"YES\" ...\n##  $ icd_o_3_site                                              : chr  \"C18.0\" \"C18.2\" \"C18.6\" \"C18.2\" ...\n##  $ icd_o_3_histology                                         : chr  \"01/03/8140\" \"01/03/8140\" \"01/03/8480\" \"01/03/8140\" ...\n##  $ icd_10                                                    : chr  \"C18.0\" \"C18.2\" \"C18.6\" \"C18.2\" ...\n##  $ tissue_prospective_collection_indicator                   : chr  \"YES\" \"YES\" \"NO\" \"NO\" ...\n##  $ tissue_retrospective_collection_indicator                 : chr  \"NO\" \"NO\" \"YES\" \"YES\" ...\n##  $ days_to_initial_pathologic_diagnosis                      : int  0 0 0 0 0 0 0 0 0 0 ...\n##  $ age_at_initial_pathologic_diagnosis                       : int  61 67 42 74 40 76 45 85 82 71 ...\n##  $ year_of_initial_pathologic_diagnosis                      : int  2013 2013 2013 2009 2009 2011 2009 2009 2009 2009 ...\n##  $ person_neoplasm_cancer_status                             : chr  \"TUMOR FREE\" \"WITH TUMOR\" \"TUMOR FREE\" \"WITH TUMOR\" ...\n##  $ ethnicity                                                 : chr  \"NOT HISPANIC OR LATINO\" \"NOT HISPANIC OR LATINO\" \"NOT HISPANIC OR LATINO\" \"HISPANIC OR LATINO\" ...\n##  $ weight                                                    : num  63.3 134 108 NA 99.1 ...\n##  $ height                                                    : num  173 168 168 NA 162 ...\n##  $ day_of_form_completion                                    : int  22 1 5 27 27 27 14 28 4 15 ...\n##  $ month_of_form_completion                                  : int  4 10 6 1 1 1 10 1 10 10 ...\n##  $ year_of_form_completion                                   : int  2014 2014 2014 2015 2015 2015 2010 2011 2010 2010 ...\n##  $ residual_tumor                                            : chr  \"R0\" \"R0\" \"R0\" \"R0\" ...\n##  $ anatomic_neoplasm_subdivision                             : chr  \"Cecum\" \"Ascending Colon\" \"Descending Colon\" \"Ascending Colon\" ...\n##  $ primary_lymph_node_presentation_assessment                : chr  \"YES\" \"YES\" \"YES\" \"YES\" ...\n##  $ lymph_node_examined_count                                 : int  28 25 24 3 11 15 22 27 29 20 ...\n##  $ number_of_lymphnodes_positive_by_he                       : int  0 NA 0 0 10 0 NA 3 1 7 ...\n##  $ number_of_lymphnodes_positive_by_ihc                      : int  0 2 NA 0 0 0 NA NA NA 0 ...\n##  $ preoperative_pretreatment_cea_level                       : num  NA 2 NA 550 2.61 2.91 NA 17.4 3.4 32.8 ...\n##  $ non_nodal_tumor_deposits                                  : chr  \"NO\" \"YES\" \"NO\" \"NO\" ...\n##  $ circumferential_resection_margin                          : num  NA 30 20 NA NA NA NA NA NA NA ...\n##  $ venous_invasion                                           : chr  \"NO\" \"NO\" \"NO\" \"YES\" ...\n##  $ lymphatic_invasion                                        : chr  \"NO\" \"NO\" \"NO\" NA ...\n##  $ perineural_invasion_present                               : chr  \"NO\" \"NO\" \"NO\" NA ...\n##  $ microsatellite_instability                                : chr  \"NO\" NA \"NO\" NA ...\n##  $ number_of_loci_tested                                     : int  NA NA NA NA NA NA NA NA NA NA ...\n##  $ number_of_abnormal_loci                                   : int  NA NA NA NA NA NA NA NA NA NA ...\n##  $ kras_gene_analysis_performed                              : chr  \"NO\" \"NO\" \"NO\" \"NO\" ...\n##  $ kras_mutation_found                                       : chr  NA NA NA NA ...\n##  $ kras_mutation_codon                                       : int  NA NA NA NA NA NA NA NA NA NA ...\n##  $ braf_gene_analysis_performed                              : chr  \"NO\" \"NO\" \"NO\" \"NO\" ...\n##  $ braf_gene_analysis_result                                 : chr  NA NA NA NA ...\n##  $ synchronous_colon_cancer_present                          : chr  \"NO\" \"YES\" \"NO\" \"NO\" ...\n##  $ history_of_colon_polyps                                   : chr  \"YES\" \"NO\" \"NO\" \"NO\" ...\n##  $ colon_polyps_present                                      : chr  \"YES\" \"YES\" \"NO\" \"YES\" ...\n##  $ loss_expression_of_mismatch_repair_proteins_by_ihc        : chr  \"YES\" \"YES\" \"YES\" \"NO\" ...\n##  $ loss_expression_of_mismatch_repair_proteins_by_ihc_results: chr  \"MLH1-ExpressedMSH2-ExpressedPMS2-ExpressedMSH6-Expressed\" \"MLH1-ExpressedMSH2-ExpressedPMS2-ExpressedMSH6-Expressed\" \"MLH1-ExpressedMSH2-ExpressedPMS2-ExpressedMSH6-Expressed\" NA ...\n##  $ number_of_first_degree_relatives_with_cancer_diagnosis    : int  0 0 0 0 NA 0 0 0 0 0 ...\n##  $ radiation_therapy                                         : chr  \"NO\" \"NO\" \"NO\" \"NO\" ...\n##  $ postoperative_rx_tx                                       : chr  \"NO\" \"YES\" \"NO\" \"NO\" ...\n##  $ primary_therapy_outcome_success                           : chr  \"Complete Remission/Response\" \"Stable Disease\" \"Complete Remission/Response\" \"Progressive Disease\" ...\n##  $ has_new_tumor_events_information                          : chr  \"NO\" \"NO\" \"NO\" \"NO\" ...\n##  $ has_drugs_information                                     : chr  \"NO\" \"YES\" \"NO\" \"NO\" ...\n##  $ has_radiations_information                                : chr  \"NO\" \"NO\" \"NO\" \"NO\" ...\n##  $ has_follow_ups_information                                : chr  \"YES\" \"YES\" \"YES\" \"YES\" ...\n##  $ project                                                   : chr  \"TCGA-COAD\" \"TCGA-COAD\" \"TCGA-COAD\" \"TCGA-COAD\" ...\n##  $ stage_event_system_version                                : chr  \"7th\" \"7th\" \"7th\" \"6th\" ...\n##  $ stage_event_clinical_stage                                : logi  NA NA NA NA NA NA ...\n##  $ stage_event_pathologic_stage                              : chr  \"Stage I\" \"Stage IIIB\" \"Stage IIA\" \"Stage IV\" ...\n##  $ stage_event_tnm_categories                                : chr  \"T2N0M0\" \"T4aN1bM0\" \"T3N0MX\" \"T3N0M1b\" ...\n##  $ stage_event_psa                                           : logi  NA NA NA NA NA NA ...\n##  $ stage_event_gleason_grading                               : logi  NA NA NA NA NA NA ...\n##  $ stage_event_ann_arbor                                     : logi  NA NA NA NA NA NA ...\n##  $ stage_event_serum_markers                                 : logi  NA NA NA NA NA NA ...\n##  $ stage_event_igcccg_stage                                  : logi  NA NA NA NA NA NA ...\n##  $ stage_event_masaoka_stage                                 : logi  NA NA NA NA NA NA ...\n##  $ cancer_type                                               : chr  \"TCGA-COAD\" \"TCGA-COAD\" \"TCGA-COAD\" \"TCGA-COAD\" ...\n##  $ patient_death_reason                                      : chr  NA NA NA NA ...\n##  $ anatomic_neoplasm_subdivision_other                       : chr  NA NA NA NA ...\n##  $ neoplasm_histologic_grade                                 : chr  NA NA NA NA ...\n##  $ country_of_procurement                                    : chr  NA NA NA NA ...\n##  $ city_of_procurement                                       : chr  NA NA NA NA ...\n##  $ reflux_history                                            : chr  NA NA NA NA ...\n##  $ antireflux_treatment                                      : chr  NA NA NA NA ...\n##  $ antireflux_treatment_types                                : chr  NA NA NA NA ...\n##  $ barretts_esophagus                                        : chr  NA NA NA NA ...\n##  $ h_pylori_infection                                        : chr  NA NA NA NA ...\n##  $ family_history_of_stomach_cancer                          : chr  NA NA NA NA ...\n##  $ number_of_relatives_with_stomach_cancer                   : int  NA NA NA NA NA NA NA NA NA NA ...\n##  $ relative_family_cancer_history                            : chr  NA NA NA NA ...\n##  $ cancer_first_degree_relative                              : int  NA NA NA NA NA NA NA NA NA NA ...\n##  $ blood_relative_cancer_history_list                        : chr  NA NA NA NA ...\n##  $ history_hepato_carcinoma_risk_factors                     : chr  NA NA NA NA ...\n##  $ post_op_ablation_embolization_tx                          : chr  NA NA NA NA ...\n##  $ eastern_cancer_oncology_group                             : int  NA NA NA NA NA NA NA NA NA NA ...\n##  $ primary_pathology_tumor_tissue_site                       : chr  NA NA NA NA ...\n##  $ primary_pathology_histological_type                       : chr  NA NA NA NA ...\n##  $ primary_pathology_specimen_collection_method_name         : chr  NA NA NA NA ...\n##  $ primary_pathology_history_prior_surgery_type_other        : chr  NA NA NA NA ...\n##   [list output truncated]\n</code></pre> <pre><code>summary(df_TCGA)\n</code></pre> <pre><code>##  bcr_patient_barcode additional_studies tumor_tissue_site  histological_type \n##  Length:1308         Length:1308        Length:1308        Length:1308       \n##  Class :character    Class :character   Class :character   Class :character  \n##  Mode  :character    Mode  :character   Mode  :character   Mode  :character  \n##                                                                              \n##                                                                              \n##                                                                              \n##                                                                              \n##    other_dx            gender          vital_status       days_to_birth   \n##  Length:1308        Length:1308        Length:1308        Min.   :-32873  \n##  Class :character   Class :character   Class :character   1st Qu.:-27321  \n##  Mode  :character   Mode  :character   Mode  :character   Median :-24655  \n##                                                           Mean   :-24204  \n##                                                           3rd Qu.:-21319  \n##                                                           Max.   :-10659  \n##                                                           NA's   :17      \n##  days_to_last_known_alive days_to_death    days_to_last_followup\n##  Min.   :   0.0           Min.   :   0.0   Min.   :-Inf         \n##  1st Qu.: 343.5           1st Qu.: 145.0   1st Qu.:   0         \n##  Median : 992.0           Median : 366.0   Median : 485         \n##  Mean   :1635.1           Mean   : 503.5   Mean   :-Inf         \n##  3rd Qu.:3223.0           3rd Qu.: 641.0   3rd Qu.: 942         \n##  Max.   :3920.0           Max.   :3042.0   Max.   :4502         \n##  NA's   :1289             NA's   :1071     NA's   :205          \n##   race_list         tissue_source_site  patient_id        bcr_patient_uuid  \n##  Length:1308        Length:1308        Length:1308        Length:1308       \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##                                                                             \n##  history_of_neoadjuvant_treatment informed_consent_verified icd_o_3_site      \n##  Length:1308                      Length:1308               Length:1308       \n##  Class :character                 Class :character          Class :character  \n##  Mode  :character                 Mode  :character          Mode  :character  \n##                                                                               \n##                                                                               \n##                                                                               \n##                                                                               \n##  icd_o_3_histology     icd_10          tissue_prospective_collection_indicator\n##  Length:1308        Length:1308        Length:1308                            \n##  Class :character   Class :character   Class :character                       \n##  Mode  :character   Mode  :character   Mode  :character                       \n##                                                                               \n##                                                                               \n##                                                                               \n##                                                                               \n##  tissue_retrospective_collection_indicator days_to_initial_pathologic_diagnosis\n##  Length:1308                               Min.   :0                           \n##  Class :character                          1st Qu.:0                           \n##  Mode  :character                          Median :0                           \n##                                            Mean   :0                           \n##                                            3rd Qu.:0                           \n##                                            Max.   :0                           \n##                                            NA's   :58                          \n##  age_at_initial_pathologic_diagnosis year_of_initial_pathologic_diagnosis\n##  Min.   :30.00                       Min.   :1996                        \n##  1st Qu.:58.00                       1st Qu.:2008                        \n##  Median :67.00                       Median :2010                        \n##  Mean   :65.85                       Mean   :2009                        \n##  3rd Qu.:74.00                       3rd Qu.:2011                        \n##  Max.   :90.00                       Max.   :2013                        \n##  NA's   :56                          NA's   :53                          \n##  person_neoplasm_cancer_status  ethnicity             weight      \n##  Length:1308                   Length:1308        Min.   : 34.00  \n##  Class :character              Class :character   1st Qu.: 65.00  \n##  Mode  :character              Mode  :character   Median : 78.05  \n##                                                   Mean   : 80.44  \n##                                                   3rd Qu.: 91.85  \n##                                                   Max.   :175.30  \n##                                                   NA's   :936     \n##      height      day_of_form_completion month_of_form_completion\n##  Min.   : 80.3   Min.   : 1.00          Min.   : 1.000          \n##  1st Qu.:162.0   1st Qu.:11.00          1st Qu.: 4.000          \n##  Median :170.0   Median :18.00          Median : 6.000          \n##  Mean   :168.9   Mean   :16.62          Mean   : 5.881          \n##  3rd Qu.:176.0   3rd Qu.:22.00          3rd Qu.: 7.000          \n##  Max.   :193.0   Max.   :31.00          Max.   :12.000          \n##  NA's   :957     NA's   :41             NA's   :5               \n##  year_of_form_completion residual_tumor     anatomic_neoplasm_subdivision\n##  Min.   :2010            Length:1308        Length:1308                  \n##  1st Qu.:2011            Class :character   Class :character             \n##  Median :2011            Mode  :character   Mode  :character             \n##  Mean   :2012                                                            \n##  3rd Qu.:2013                                                            \n##  Max.   :2015                                                            \n##  NA's   :5                                                               \n##  primary_lymph_node_presentation_assessment lymph_node_examined_count\n##  Length:1308                                Min.   :  0.00           \n##  Class :character                           1st Qu.: 12.00           \n##  Mode  :character                           Median : 18.00           \n##                                             Mean   : 21.42           \n##                                             3rd Qu.: 27.00           \n##                                             Max.   :109.00           \n##                                             NA's   :138              \n##  number_of_lymphnodes_positive_by_he number_of_lymphnodes_positive_by_ihc\n##  Min.   : 0.000                      Min.   : 0.0000                     \n##  1st Qu.: 0.000                      1st Qu.: 0.0000                     \n##  Median : 1.000                      Median : 0.0000                     \n##  Mean   : 3.493                      Mean   : 0.2833                     \n##  3rd Qu.: 4.000                      3rd Qu.: 0.0000                     \n##  Max.   :57.000                      Max.   :12.0000                     \n##  NA's   :143                         NA's   :1188                        \n##  preoperative_pretreatment_cea_level non_nodal_tumor_deposits\n##  Min.   :   0.000                    Length:1308             \n##  1st Qu.:   1.700                    Class :character        \n##  Median :   3.160                    Mode  :character        \n##  Mean   :  65.074                                            \n##  3rd Qu.:   8.982                                            \n##  Max.   :7868.000                                            \n##  NA's   :906                                                 \n##  circumferential_resection_margin venous_invasion    lymphatic_invasion\n##  Min.   :  0.00                   Length:1308        Length:1308       \n##  1st Qu.:  2.50                   Class :character   Class :character  \n##  Median : 13.00                   Mode  :character   Mode  :character  \n##  Mean   : 22.96                                                        \n##  3rd Qu.: 30.00                                                        \n##  Max.   :165.00                                                        \n##  NA's   :1185                                                          \n##  perineural_invasion_present microsatellite_instability number_of_loci_tested\n##  Length:1308                 Length:1308                Min.   : 0.000       \n##  Class :character            Class :character           1st Qu.: 5.000       \n##  Mode  :character            Mode  :character           Median : 5.000       \n##                                                         Mean   : 4.944       \n##                                                         3rd Qu.: 5.000       \n##                                                         Max.   :10.000       \n##                                                         NA's   :1236         \n##  number_of_abnormal_loci kras_gene_analysis_performed kras_mutation_found\n##  Min.   :0.0000          Length:1308                  Length:1308        \n##  1st Qu.:0.0000          Class :character             Class :character   \n##  Median :0.0000          Mode  :character             Mode  :character   \n##  Mean   :0.5915                                                          \n##  3rd Qu.:0.0000                                                          \n##  Max.   :9.0000                                                          \n##  NA's   :1237                                                            \n##  kras_mutation_codon braf_gene_analysis_performed braf_gene_analysis_result\n##  Min.   :12.00       Length:1308                  Length:1308              \n##  1st Qu.:12.00       Class :character             Class :character         \n##  Median :12.00       Mode  :character             Mode  :character         \n##  Mean   :13.83                                                             \n##  3rd Qu.:12.00                                                             \n##  Max.   :61.00                                                             \n##  NA's   :1278                                                              \n##  synchronous_colon_cancer_present history_of_colon_polyps colon_polyps_present\n##  Length:1308                      Length:1308             Length:1308         \n##  Class :character                 Class :character        Class :character    \n##  Mode  :character                 Mode  :character        Mode  :character    \n##                                                                               \n##                                                                               \n##                                                                               \n##                                                                               \n##  loss_expression_of_mismatch_repair_proteins_by_ihc\n##  Length:1308                                       \n##  Class :character                                  \n##  Mode  :character                                  \n##                                                    \n##                                                    \n##                                                    \n##                                                    \n##  loss_expression_of_mismatch_repair_proteins_by_ihc_results\n##  Length:1308                                               \n##  Class :character                                          \n##  Mode  :character                                          \n##                                                            \n##                                                            \n##                                                            \n##                                                            \n##  number_of_first_degree_relatives_with_cancer_diagnosis radiation_therapy \n##  Min.   :0.0000                                         Length:1308       \n##  1st Qu.:0.0000                                         Class :character  \n##  Median :0.0000                                         Mode  :character  \n##  Mean   :0.1654                                                           \n##  3rd Qu.:0.0000                                                           \n##  Max.   :3.0000                                                           \n##  NA's   :770                                                              \n##  postoperative_rx_tx primary_therapy_outcome_success\n##  Length:1308         Length:1308                    \n##  Class :character    Class :character               \n##  Mode  :character    Mode  :character               \n##                                                     \n##                                                     \n##                                                     \n##                                                     \n##  has_new_tumor_events_information has_drugs_information\n##  Length:1308                      Length:1308          \n##  Class :character                 Class :character     \n##  Mode  :character                 Mode  :character     \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##  has_radiations_information has_follow_ups_information   project         \n##  Length:1308                Length:1308                Length:1308       \n##  Class :character           Class :character           Class :character  \n##  Mode  :character           Mode  :character           Mode  :character  \n##                                                                          \n##                                                                          \n##                                                                          \n##                                                                          \n##  stage_event_system_version stage_event_clinical_stage\n##  Length:1308                Mode:logical              \n##  Class :character           NA's:1308                 \n##  Mode  :character                                     \n##                                                       \n##                                                       \n##                                                       \n##                                                       \n##  stage_event_pathologic_stage stage_event_tnm_categories stage_event_psa\n##  Length:1308                  Length:1308                Mode:logical   \n##  Class :character             Class :character           NA's:1308      \n##  Mode  :character             Mode  :character                          \n##                                                                         \n##                                                                         \n##                                                                         \n##                                                                         \n##  stage_event_gleason_grading stage_event_ann_arbor stage_event_serum_markers\n##  Mode:logical                Mode:logical          Mode:logical             \n##  NA's:1308                   NA's:1308             NA's:1308                \n##                                                                             \n##                                                                             \n##                                                                             \n##                                                                             \n##                                                                             \n##  stage_event_igcccg_stage stage_event_masaoka_stage cancer_type       \n##  Mode:logical             Mode:logical              Length:1308       \n##  NA's:1308                NA's:1308                 Class :character  \n##                                                     Mode  :character  \n##                                                                       \n##                                                                       \n##                                                                       \n##                                                                       \n##  patient_death_reason anatomic_neoplasm_subdivision_other\n##  Length:1308          Length:1308                        \n##  Class :character     Class :character                   \n##  Mode  :character     Mode  :character                   \n##                                                          \n##                                                          \n##                                                          \n##                                                          \n##  neoplasm_histologic_grade country_of_procurement city_of_procurement\n##  Length:1308               Length:1308            Length:1308        \n##  Class :character          Class :character       Class :character   \n##  Mode  :character          Mode  :character       Mode  :character   \n##                                                                      \n##                                                                      \n##                                                                      \n##                                                                      \n##  reflux_history     antireflux_treatment antireflux_treatment_types\n##  Length:1308        Length:1308          Length:1308               \n##  Class :character   Class :character     Class :character          \n##  Mode  :character   Mode  :character     Mode  :character          \n##                                                                    \n##                                                                    \n##                                                                    \n##                                                                    \n##  barretts_esophagus h_pylori_infection family_history_of_stomach_cancer\n##  Length:1308        Length:1308        Length:1308                     \n##  Class :character   Class :character   Class :character                \n##  Mode  :character   Mode  :character   Mode  :character                \n##                                                                        \n##                                                                        \n##                                                                        \n##                                                                        \n##  number_of_relatives_with_stomach_cancer relative_family_cancer_history\n##  Min.   :0.0000                          Length:1308                   \n##  1st Qu.:0.0000                          Class :character              \n##  Median :0.0000                          Mode  :character              \n##  Mean   :0.2632                                                        \n##  3rd Qu.:0.0000                                                        \n##  Max.   :2.0000                                                        \n##  NA's   :1232                                                          \n##  cancer_first_degree_relative blood_relative_cancer_history_list\n##  Min.   :1.000                Length:1308                       \n##  1st Qu.:1.000                Class :character                  \n##  Median :1.000                Mode  :character                  \n##  Mean   :1.696                                                  \n##  3rd Qu.:2.000                                                  \n##  Max.   :4.000                                                  \n##  NA's   :1285                                                   \n##  history_hepato_carcinoma_risk_factors post_op_ablation_embolization_tx\n##  Length:1308                           Length:1308                     \n##  Class :character                      Class :character                \n##  Mode  :character                      Mode  :character                \n##                                                                        \n##                                                                        \n##                                                                        \n##                                                                        \n##  eastern_cancer_oncology_group primary_pathology_tumor_tissue_site\n##  Min.   :0.0000                Length:1308                        \n##  1st Qu.:0.0000                Class :character                   \n##  Median :0.0000                Mode  :character                   \n##  Mean   :0.3514                                                   \n##  3rd Qu.:1.0000                                                   \n##  Max.   :3.0000                                                   \n##  NA's   :1271                                                     \n##  primary_pathology_histological_type\n##  Length:1308                        \n##  Class :character                   \n##  Mode  :character                   \n##                                     \n##                                     \n##                                     \n##                                     \n##  primary_pathology_specimen_collection_method_name\n##  Length:1308                                      \n##  Class :character                                 \n##  Mode  :character                                 \n##                                                   \n##                                                   \n##                                                   \n##                                                   \n##  primary_pathology_history_prior_surgery_type_other\n##  Length:1308                                       \n##  Class :character                                  \n##  Mode  :character                                  \n##                                                    \n##                                                    \n##                                                    \n##                                                    \n##  primary_pathology_days_to_initial_pathologic_diagnosis\n##  Min.   :0                                             \n##  1st Qu.:0                                             \n##  Median :0                                             \n##  Mean   :0                                             \n##  3rd Qu.:0                                             \n##  Max.   :0                                             \n##  NA's   :1260                                          \n##  primary_pathology_age_at_initial_pathologic_diagnosis\n##  Min.   :29.00                                        \n##  1st Qu.:58.00                                        \n##  Median :66.00                                        \n##  Mean   :63.64                                        \n##  3rd Qu.:73.00                                        \n##  Max.   :82.00                                        \n##  NA's   :1263                                         \n##  primary_pathology_year_of_initial_pathologic_diagnosis\n##  Min.   :2005                                          \n##  1st Qu.:2010                                          \n##  Median :2011                                          \n##  Mean   :2011                                          \n##  3rd Qu.:2012                                          \n##  Max.   :2013                                          \n##  NA's   :1260                                          \n##  primary_pathology_neoplasm_histologic_grade primary_pathology_residual_tumor\n##  Length:1308                                 Length:1308                     \n##  Class :character                            Class :character                \n##  Mode  :character                            Mode  :character                \n##                                                                              \n##                                                                              \n##                                                                              \n##                                                                              \n##  primary_pathology_vascular_tumor_cell_type\n##  Length:1308                               \n##  Class :character                          \n##  Mode  :character                          \n##                                            \n##                                            \n##                                            \n##                                            \n##  primary_pathology_perineural_invasion_present\n##  Length:1308                                  \n##  Class :character                             \n##  Mode  :character                             \n##                                               \n##                                               \n##                                               \n##                                               \n##  primary_pathology_child_pugh_classification_grade\n##  Length:1308                                      \n##  Class :character                                 \n##  Mode  :character                                 \n##                                                   \n##                                                   \n##                                                   \n##                                                   \n##  primary_pathology_ca_19_9_level primary_pathology_ca_19_9_level_lower\n##  Min.   :   1.0                  Min.   :0.0000                       \n##  1st Qu.:  26.5                  1st Qu.:0.0000                       \n##  Median :  54.9                  Median :0.0000                       \n##  Mean   : 345.8                  Mean   :0.1351                       \n##  3rd Qu.: 200.0                  3rd Qu.:0.0000                       \n##  Max.   :6910.0                  Max.   :5.0000                       \n##  NA's   :1268                    NA's   :1271                         \n##  primary_pathology_ca_19_9_level_upper\n##  Min.   : 7.00                        \n##  1st Qu.:35.00                        \n##  Median :55.00                        \n##  Mean   :45.56                        \n##  3rd Qu.:55.00                        \n##  Max.   :55.00                        \n##  NA's   :1267                         \n##  primary_pathology_fetoprotein_outcome_value\n##  Min.   : 1.380                             \n##  1st Qu.: 2.350                             \n##  Median : 3.100                             \n##  Mean   : 3.803                             \n##  3rd Qu.: 4.325                             \n##  Max.   :14.000                             \n##  NA's   :1280                               \n##  primary_pathology_fetoprotein_outcome_lower_limit\n##  Min.   :0                                        \n##  1st Qu.:0                                        \n##  Median :0                                        \n##  Mean   :0                                        \n##  3rd Qu.:0                                        \n##  Max.   :0                                        \n##  NA's   :1275                                     \n##  primary_pathology_fetoprotein_outcome_upper_limit\n##  Min.   : 6.000                                   \n##  1st Qu.: 6.000                                   \n##  Median : 6.000                                   \n##  Mean   : 7.579                                   \n##  3rd Qu.: 9.000                                   \n##  Max.   :15.000                                   \n##  NA's   :1275                                     \n##  primary_pathology_platelet_result_count\n##  Min.   :   134.0                       \n##  1st Qu.:   211.0                       \n##  Median :   269.5                       \n##  Mean   : 68013.6                       \n##  3rd Qu.:179500.0                       \n##  Max.   :354000.0                       \n##  NA's   :1264                           \n##  primary_pathology_platelet_result_lower_limit\n##  Min.   :   140                               \n##  1st Qu.:   150                               \n##  Median :   150                               \n##  Mean   : 32560                               \n##  3rd Qu.: 15000                               \n##  Max.   :150000                               \n##  NA's   :1263                                 \n##  primary_pathology_platelet_result_upper_limit\n##  Min.   :   400                               \n##  1st Qu.:   450                               \n##  Median :   450                               \n##  Mean   :126091                               \n##  3rd Qu.:400000                               \n##  Max.   :450000                               \n##  NA's   :1263                                 \n##  primary_pathology_prothrombin_time_result_value\n##  Min.   : 0.800                                 \n##  1st Qu.: 1.000                                 \n##  Median : 1.100                                 \n##  Mean   : 2.012                                 \n##  3rd Qu.: 1.100                                 \n##  Max.   :12.200                                 \n##  NA's   :1266                                   \n##  primary_pathology_inter_norm_ratio_lower_limit\n##  Min.   : 0.000                                \n##  1st Qu.: 0.800                                \n##  Median : 0.900                                \n##  Mean   : 1.865                                \n##  3rd Qu.: 0.900                                \n##  Max.   :10.400                                \n##  NA's   :1274                                  \n##  primary_pathology_intern_norm_ratio_upper_limit\n##  Min.   : 1.100                                 \n##  1st Qu.: 1.200                                 \n##  Median : 1.200                                 \n##  Mean   : 2.553                                 \n##  3rd Qu.: 1.200                                 \n##  Max.   :13.100                                 \n##  NA's   :1274                                   \n##  primary_pathology_albumin_result_specified_value\n##  Min.   :2.400                                   \n##  1st Qu.:3.775                                   \n##  Median :4.150                                   \n##  Mean   :3.998                                   \n##  3rd Qu.:4.325                                   \n##  Max.   :4.800                                   \n##  NA's   :1268                                    \n##  primary_pathology_albumin_result_lower_limit\n##  Min.   :3.300                               \n##  1st Qu.:3.500                               \n##  Median :3.500                               \n##  Mean   :3.467                               \n##  3rd Qu.:3.500                               \n##  Max.   :3.500                               \n##  NA's   :1268                                \n##  primary_pathology_albumin_result_upper_limit\n##  Min.   :4.50                                \n##  1st Qu.:4.80                                \n##  Median :5.00                                \n##  Mean   :4.91                                \n##  3rd Qu.:5.00                                \n##  Max.   :5.20                                \n##  NA's   :1268                                \n##  primary_pathology_bilirubin_upper_limit\n##  Min.   : 0.200                         \n##  1st Qu.: 0.400                         \n##  Median : 0.700                         \n##  Mean   : 2.859                         \n##  3rd Qu.: 1.025                         \n##  Max.   :84.000                         \n##  NA's   :1264                           \n##  primary_pathology_bilirubin_lower_limit\n##  Min.   : 0.000                         \n##  1st Qu.: 0.100                         \n##  Median : 0.100                         \n##  Mean   : 2.221                         \n##  3rd Qu.: 0.150                         \n##  Max.   :78.000                         \n##  NA's   :1265                           \n##  primary_pathology_total_bilirubin_upper_limit\n##  Min.   : 0.300                               \n##  1st Qu.: 1.000                               \n##  Median : 1.000                               \n##  Mean   : 5.505                               \n##  3rd Qu.: 1.200                               \n##  Max.   :96.000                               \n##  NA's   :1265                                 \n##  primary_pathology_creatinine_value_in_mg_dl\n##  Min.   :0.5000                             \n##  1st Qu.:0.8000                             \n##  Median :0.9000                             \n##  Mean   :0.8651                             \n##  3rd Qu.:1.0000                             \n##  Max.   :1.4000                             \n##  NA's   :1265                               \n##  primary_pathology_creatinine_lower_level\n##  Min.   :0.4000                          \n##  1st Qu.:0.6000                          \n##  Median :0.6000                          \n##  Mean   :0.6349                          \n##  3rd Qu.:0.7000                          \n##  Max.   :0.9000                          \n##  NA's   :1265                            \n##  primary_pathology_creatinine_upper_limit\n##  Min.   :1.000                           \n##  1st Qu.:1.100                           \n##  Median :1.100                           \n##  Mean   :1.198                           \n##  3rd Qu.:1.300                           \n##  Max.   :1.400                           \n##  NA's   :1265                            \n##  primary_pathology_fibrosis_ishak_score\n##  Length:1308                           \n##  Class :character                      \n##  Mode  :character                      \n##                                        \n##                                        \n##                                        \n##                                        \n##  primary_pathology_cholangitis_tissue_evidence adenocarcinoma_invasion\n##  Length:1308                                   Length:1308            \n##  Class :character                              Class :character       \n##  Mode  :character                              Mode  :character       \n##                                                                       \n##                                                                       \n##                                                                       \n##                                                                       \n##  histological_type_other  tumor_type        initial_pathologic_diagnosis_method\n##  Length:1308             Length:1308        Length:1308                        \n##  Class :character        Class :character   Class :character                   \n##  Mode  :character        Mode  :character   Mode  :character                   \n##                                                                                \n##                                                                                \n##                                                                                \n##                                                                                \n##  init_pathology_dx_method_other surgery_performed_type\n##  Length:1308                    Length:1308           \n##  Class :character               Class :character      \n##  Mode  :character               Mode  :character      \n##                                                       \n##                                                       \n##                                                       \n##                                                       \n##  histologic_grading_tier_category maximum_tumor_dimension\n##  Length:1308                      Min.   : 0.300         \n##  Class :character                 1st Qu.: 2.925         \n##  Mode  :character                 Median : 3.500         \n##                                   Mean   : 3.840         \n##                                   3rd Qu.: 4.500         \n##                                   Max.   :14.000         \n##                                   NA's   :1138           \n##  source_of_patient_death_reason tobacco_smoking_history\n##  Length:1308                    Min.   :1.000          \n##  Class :character               1st Qu.:1.000          \n##  Mode  :character               Median :2.000          \n##                                 Mean   :2.201          \n##                                 3rd Qu.:3.000          \n##                                 Max.   :5.000          \n##                                 NA's   :1159           \n##  year_of_tobacco_smoking_onset stopped_smoking_year number_pack_years_smoked\n##  Min.   :1948                  Min.   :1952         Min.   : 0.30           \n##  1st Qu.:1960                  1st Qu.:1980         1st Qu.:15.00           \n##  Median :1971                  Median :1988         Median :25.00           \n##  Mean   :1971                  Mean   :1990         Mean   :26.84           \n##  3rd Qu.:1982                  3rd Qu.:2004         3rd Qu.:40.00           \n##  Max.   :1993                  Max.   :2013         Max.   :75.00           \n##  NA's   :1261                  NA's   :1258         NA's   :1251            \n##  alcohol_history_documented alcoholic_exposure_category\n##  Length:1308                Length:1308                \n##  Class :character           Class :character           \n##  Mode  :character           Mode  :character           \n##                                                        \n##                                                        \n##                                                        \n##                                                        \n##  frequency_of_alcohol_consumption amount_of_alcohol_consumption_per_day\n##  Min.   :0.500                    Min.   :0.500                        \n##  1st Qu.:3.000                    1st Qu.:1.000                        \n##  Median :6.500                    Median :1.000                        \n##  Mean   :4.812                    Mean   :1.581                        \n##  3rd Qu.:7.000                    3rd Qu.:2.000                        \n##  Max.   :7.000                    Max.   :4.000                        \n##  NA's   :1276                     NA's   :1277                         \n##  history_of_diabetes days_to_diabetes_onset history_of_chronic_pancreatitis\n##  Length:1308         Min.   :-9070.00       Length:1308                    \n##  Class :character    1st Qu.: -163.00       Class :character               \n##  Mode  :character    Median :  -52.00       Mode  :character               \n##                      Mean   : -706.79                                      \n##                      3rd Qu.:  -17.75                                      \n##                      Max.   :  504.00                                      \n##                      NA's   :1294                                          \n##  days_to_pancreatitis_onset family_history_of_cancer relative_cancer_types\n##  Min.   :-18029.0           Length:1308              Length:1308          \n##  1st Qu.:  -231.5           Class :character         Class :character     \n##  Median :   -71.0           Mode  :character         Mode  :character     \n##  Mean   : -1744.2                                                         \n##  3rd Qu.:   -38.0                                                         \n##  Max.   :     1.0                                                         \n##  NA's   :1297                                                             \n##  history_prior_surgery_type_other\n##  Length:1308                     \n##  Class :character                \n##  Mode  :character                \n##                                  \n##                                  \n##                                  \n##\n</code></pre> <pre><code># Verificar valores nulos\nprint(table(is.na(df_TCGA)))\n</code></pre> <pre><code>## \n##  FALSE   TRUE \n##  66141 135291\n</code></pre> <pre><code># Verificar valores nulos por columnas\n# print(colSums(is.na(df_TCGA)))\n\n# Verificar valores nulos por filas\n# print(rowSums(is.na(df_TCGA)))\n</code></pre> <p>Let's identify the column that could serve as the main record ID of the dataset. </p> <pre><code># Visualizar los primeros 5 elementos de las siguientes columnas tipo ID\nhead(df_TCGA[,c(\"bcr_patient_barcode\", \"bcr_patient_uuid\", \"patient_id\")], 5)\n</code></pre> <pre><code>##   bcr_patient_barcode                     bcr_patient_uuid patient_id\n## 1        TCGA-3L-AA1B A94E1279-A975-480A-93E9-7B1FF05CBCBF       AA1B\n## 2        TCGA-4N-A93T 92554413-9EBC-4354-8E1B-9682F3A031D9       A93T\n## 3        TCGA-4T-AA8H A5E14ADD-1552-4606-9FFE-3A03BCF76640       AA8H\n## 4        TCGA-5M-AAT4 1136DD50-242A-4659-AAD4-C53F9E759BB3       AAT4\n## 5        TCGA-5M-AAT6 CE00896A-F7D2-4123-BB95-24CB6E53FC32       AAT6\n</code></pre> <p>We observed three different identifiers: the full TCGA barcode, UUID (Universal Unique Identifier), and internal patient code, respectively. In this project, we will use the first one, as it is the standard patient-level identifier in TCGA. We also observed that it contains the internal patient code.</p> <pre><code># Crear un subconjunto del dataset TCGA con variables relevantes desde una perspectiva oncologica\ndf_gastro &lt;-subset(df_TCGA, select = c(bcr_patient_barcode, tumor_tissue_site, histological_type,\n                               gender, vital_status, days_to_death, days_to_last_followup,\n                               tissue_source_site,\n                               age_at_initial_pathologic_diagnosis, person_neoplasm_cancer_status,\n                               weight, height, residual_tumor, anatomic_neoplasm_subdivision,\n                               number_of_lymphnodes_positive_by_he, number_of_lymphnodes_positive_by_ihc,\n                               preoperative_pretreatment_cea_level, non_nodal_tumor_deposits,\n                               circumferential_resection_margin, venous_invasion, lymphatic_invasion,\n                               perineural_invasion_present, microsatellite_instability,\n                               number_of_loci_tested, number_of_abnormal_loci, kras_gene_analysis_performed,\n                               kras_mutation_found, kras_mutation_codon, braf_gene_analysis_performed,\n                               braf_gene_analysis_result, synchronous_colon_cancer_present,\n                               stage_event_pathologic_stage, stage_event_tnm_categories, cancer_type))\n</code></pre> <p>We then visualized in more detail the customized TCGA dataset with the 34 variables of interest.</p> <pre><code># Usar funci\u00f3n skimr()\nskim(df_gastro)\n</code></pre> <pre><code>## Warning: There was 1 warning in `dplyr::summarize()`.\n## \u2139 In argument: `dplyr::across(tidyselect::any_of(variable_names),\n##   mangled_skimmers$funs)`.\n## \u2139 In group 0: .\n## Caused by warning:\n## ! There was 1 warning in `dplyr::summarize()`.\n## \u2139 In argument: `dplyr::across(tidyselect::any_of(variable_names),\n##   mangled_skimmers$funs)`.\n## Caused by warning in `inline_hist()`:\n## ! Variable contains Inf or -Inf value(s) that were converted to NA.\n</code></pre> Name df_gastro Number of rows 1308 Number of columns 34 _______________________ Column type frequency: character 22 numeric 12 ________________________ Group variables None <p>Data summary</p> <p>Variable type: character</p> skim_variable n_missing complete_rate min max empty n_unique whitespace bcr_patient_barcode 1 1.00 0 12 1 1307 0 tumor_tissue_site 6 1.00 0 9 1 6 0 histological_type 17 0.99 0 65 1 19 0 gender 2 1.00 0 6 1 3 0 vital_status 636 0.51 4 5 0 4 0 tissue_source_site 1 1.00 0 2 1 99 0 person_neoplasm_cancer_status 194 0.85 0 10 1 3 0 residual_tumor 142 0.89 0 2 1 5 0 anatomic_neoplasm_subdivision 81 0.94 0 25 1 19 0 non_nodal_tumor_deposits 1010 0.23 0 3 1 3 0 venous_invasion 762 0.42 0 3 1 3 0 lymphatic_invasion 740 0.43 0 3 1 3 0 perineural_invasion_present 1030 0.21 0 3 1 3 0 microsatellite_instability 1190 0.09 0 3 1 3 0 kras_gene_analysis_performed 733 0.44 0 3 1 3 0 kras_mutation_found 1245 0.05 0 3 1 3 0 braf_gene_analysis_performed 747 0.43 0 3 1 3 0 braf_gene_analysis_result 1272 0.03 0 8 1 3 0 synchronous_colon_cancer_present 744 0.43 0 3 1 3 0 stage_event_pathologic_stage 52 0.96 0 10 1 15 0 stage_event_tnm_categories 3 1.00 0 9 1 141 0 cancer_type 1 1.00 0 9 1 6 0 <p>Variable type: numeric</p> skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist days_to_death 1071 0.18 503.46 524.96 0.0 145.0 366.00 641.00 3042.0 \u2587\u2582\u2581\u2581\u2581 days_to_last_followup 205 0.84 -Inf NaN -Inf 0.0 485.00 942.00 4502.0 \u2587\u2583\u2581\u2581\u2581 age_at_initial_pathologic_diagnosis 56 0.96 65.85 11.88 30.0 58.0 67.00 74.00 90.0 \u2581\u2583\u2586\u2587\u2583 weight 936 0.28 80.44 20.88 34.0 65.0 78.05 91.85 175.3 \u2583\u2587\u2583\u2581\u2581 height 957 0.27 168.87 11.64 80.3 162.0 170.00 176.00 193.0 \u2581\u2581\u2581\u2587\u2586 number_of_lymphnodes_positive_by_he 143 0.89 3.49 6.26 0.0 0.0 1.00 4.00 57.0 \u2587\u2581\u2581\u2581\u2581 number_of_lymphnodes_positive_by_ihc 1188 0.09 0.28 1.64 0.0 0.0 0.00 0.00 12.0 \u2587\u2581\u2581\u2581\u2581 preoperative_pretreatment_cea_level 906 0.31 65.07 468.84 0.0 1.7 3.16 8.98 7868.0 \u2587\u2581\u2581\u2581\u2581 circumferential_resection_margin 1185 0.09 22.96 28.31 0.0 2.5 13.00 30.00 165.0 \u2587\u2582\u2581\u2581\u2581 number_of_loci_tested 1236 0.06 4.94 2.14 0.0 5.0 5.00 5.00 10.0 \u2581\u2581\u2587\u2581\u2581 number_of_abnormal_loci 1237 0.05 0.59 1.70 0.0 0.0 0.00 0.00 9.0 \u2587\u2581\u2581\u2581\u2581 kras_mutation_codon 1278 0.02 13.83 8.92 12.0 12.0 12.00 12.00 61.0 \u2587\u2581\u2581\u2581\u2581 <pre><code># Obtener el n\u00famero de variables y sus nombres\ndim(df_gastro) \n</code></pre> <pre><code>## [1] 1308   34\n</code></pre> <pre><code># Verificar valores nulos\nprint(table(is.na(df_gastro)))\n</code></pre> <pre><code>## \n## FALSE  TRUE \n## 23465 21007\n</code></pre> <p>Data Transformation</p> <p>The first transformation to the dataframe <code>df_gastro</code> consisted in creating a new patient survival variable <code>os_time</code> to identify those who are still alive and those who are not. </p> <pre><code># Visualizar algunas variables relacionadas a la supervivencia de pacientes\ndf_gastro[1:10, c(\"vital_status\", \"days_to_last_followup\", \"days_to_death\")]\n</code></pre> <pre><code>##    vital_status days_to_last_followup days_to_death\n## 1          &lt;NA&gt;                   475            NA\n## 2          &lt;NA&gt;                   146            NA\n## 3          &lt;NA&gt;                   385            NA\n## 4          Dead                  -Inf            49\n## 5          Dead                  -Inf           290\n## 6          &lt;NA&gt;                  1200            NA\n## 7          &lt;NA&gt;                   775            NA\n## 8          Dead                  1126          1126\n## 9          &lt;NA&gt;                  1419            NA\n## 10         &lt;NA&gt;                  1331            NA\n</code></pre> <p>where,</p> <ul> <li>vital_status: patient\u2019s vital status at the time of follow-up closure</li> <li>days_to_last_followup: number of days from diagnosis to the last clinical contact with the patient</li> <li>days_to_death: number of days from diagnosis to death</li> </ul> <p>The <code>os_time</code> variable is a combination of <code>days_to_last_followup</code> and <code>days_to_death</code> where NA values of <code>days_to_death</code> are imputed by those in <code>days_to_last_followup</code>.</p> <pre><code># Crear la variable de supervivencia de pacientes (los que siguen vivos y los que no)\ndf_gastro$os_time &lt;- ifelse(\n  !is.na(df_gastro$days_to_death), #si hay un valor que no es nulo en la variable \"days_to_death\"\n  df_gastro$days_to_death,\n  df_gastro$days_to_last_followup #en caso contrario, el valor es lo que aparece en la columna \"days_to_last_followup\"\n)\n\n# Transformar los valores infinitos de os_time en NA\ndf_gastro$os_time[is.infinite(df_gastro$os_time)] &lt;- NA\n</code></pre> <p>In TCGA clinical data, <code>vital_status</code> is sometimes missing or incorrectly analyzed, but if the patient follow-up data are present, it can be assumed that the patient was alive at their last follow-up. This assumption was used in this project, although a more conservative approach would be to exclude those patients.</p> <pre><code># Asignar \"Alive\" a los NAs de vital_status si la variable \"days_to_last_followup\" tiene un valor valido.\ndf_gastro$vital_status[is.na(df_gastro$vital_status) \n                    &amp; !is.na(df_gastro$days_to_last_followup)] &lt;- \"Alive\"  \n\n# Definir os_event, si el paciente est\u00e1 vivo con 0, caso contrario con 1. \ndf_gastro$os_event &lt;- ifelse(df_gastro$vital_status == \"Dead\", 1, 0) \n\n# Eliminar la especificaci\u00f3n TCGA del tipo de c\u00e1ncer para simplificar.\ndf_gastro$cancer_type &lt;-gsub(\"TCGA-\",\"\", df_gastro$cancer_type) #ref. The R book page 124\n\n# Eliminar la especificaci\u00f3n Stage para simplificar.\ndf_gastro$stage_event_pathologic_stage &lt;-gsub(\"Stage \",\"\", df_gastro$stage_event_pathologic_stage) #ref. The R book page 124\n\n# Estructura del conjunto de datos y resumen estad\u00edstico\nstr(df_gastro)\n</code></pre> <pre><code>## 'data.frame':    1308 obs. of  36 variables:\n##  $ bcr_patient_barcode                 : chr  \"TCGA-3L-AA1B\" \"TCGA-4N-A93T\" \"TCGA-4T-AA8H\" \"TCGA-5M-AAT4\" ...\n##  $ tumor_tissue_site                   : chr  \"Colon\" \"Colon\" \"Colon\" \"Colon\" ...\n##  $ histological_type                   : chr  \"Colon Adenocarcinoma\" \"Colon Adenocarcinoma\" \"Colon Mucinous Adenocarcinoma\" \"Colon Adenocarcinoma\" ...\n##  $ gender                              : chr  \"FEMALE\" \"MALE\" \"FEMALE\" \"MALE\" ...\n##  $ vital_status                        : chr  \"Alive\" \"Alive\" \"Alive\" \"Dead\" ...\n##  $ days_to_death                       : int  NA NA NA 49 290 NA NA 1126 NA NA ...\n##  $ days_to_last_followup               : num  475 146 385 -Inf -Inf ...\n##  $ tissue_source_site                  : chr  \"3L\" \"4N\" \"4T\" \"5M\" ...\n##  $ age_at_initial_pathologic_diagnosis : int  61 67 42 74 40 76 45 85 82 71 ...\n##  $ person_neoplasm_cancer_status       : chr  \"TUMOR FREE\" \"WITH TUMOR\" \"TUMOR FREE\" \"WITH TUMOR\" ...\n##  $ weight                              : num  63.3 134 108 NA 99.1 ...\n##  $ height                              : num  173 168 168 NA 162 ...\n##  $ residual_tumor                      : chr  \"R0\" \"R0\" \"R0\" \"R0\" ...\n##  $ anatomic_neoplasm_subdivision       : chr  \"Cecum\" \"Ascending Colon\" \"Descending Colon\" \"Ascending Colon\" ...\n##  $ number_of_lymphnodes_positive_by_he : int  0 NA 0 0 10 0 NA 3 1 7 ...\n##  $ number_of_lymphnodes_positive_by_ihc: int  0 2 NA 0 0 0 NA NA NA 0 ...\n##  $ preoperative_pretreatment_cea_level : num  NA 2 NA 550 2.61 2.91 NA 17.4 3.4 32.8 ...\n##  $ non_nodal_tumor_deposits            : chr  \"NO\" \"YES\" \"NO\" \"NO\" ...\n##  $ circumferential_resection_margin    : num  NA 30 20 NA NA NA NA NA NA NA ...\n##  $ venous_invasion                     : chr  \"NO\" \"NO\" \"NO\" \"YES\" ...\n##  $ lymphatic_invasion                  : chr  \"NO\" \"NO\" \"NO\" NA ...\n##  $ perineural_invasion_present         : chr  \"NO\" \"NO\" \"NO\" NA ...\n##  $ microsatellite_instability          : chr  \"NO\" NA \"NO\" NA ...\n##  $ number_of_loci_tested               : int  NA NA NA NA NA NA NA NA NA NA ...\n##  $ number_of_abnormal_loci             : int  NA NA NA NA NA NA NA NA NA NA ...\n##  $ kras_gene_analysis_performed        : chr  \"NO\" \"NO\" \"NO\" \"NO\" ...\n##  $ kras_mutation_found                 : chr  NA NA NA NA ...\n##  $ kras_mutation_codon                 : int  NA NA NA NA NA NA NA NA NA NA ...\n##  $ braf_gene_analysis_performed        : chr  \"NO\" \"NO\" \"NO\" \"NO\" ...\n##  $ braf_gene_analysis_result           : chr  NA NA NA NA ...\n##  $ synchronous_colon_cancer_present    : chr  \"NO\" \"YES\" \"NO\" \"NO\" ...\n##  $ stage_event_pathologic_stage        : chr  \"I\" \"IIIB\" \"IIA\" \"IV\" ...\n##  $ stage_event_tnm_categories          : chr  \"T2N0M0\" \"T4aN1bM0\" \"T3N0MX\" \"T3N0M1b\" ...\n##  $ cancer_type                         : chr  \"COAD\" \"COAD\" \"COAD\" \"COAD\" ...\n##  $ os_time                             : num  475 146 385 49 290 ...\n##  $ os_event                            : num  0 0 0 1 1 0 0 1 0 0 ...\n</code></pre>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#22-target-questions","title":"2.2 Target Questions","text":"<ol> <li> <p>What is the correlation between the pathological stage of the tumor at diagnosis and survival?</p> </li> <li> <p>What survival differences are observed by patient gender and age?</p> </li> <li> <p>Are there significant differences in survival according to the type of gastrointestinal cancer?</p> </li> <li> <p>What is the impact of venous, lymphatic, or perineural invasion on survival?</p> </li> </ol>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#3-exploratory-data-analysis","title":"3 Exploratory Data Analysis","text":""},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#31-descriptive-statistics-and-visualization","title":"3.1 Descriptive Statistics and Visualization","text":"<p>In this section we implement an exploratory and visualization analysis to address the target questions from section 2.2.</p> <pre><code># Convertir a factor las variables tipo `chr`\ndf_gastro$gender&lt;- factor(df_gastro$gender)\ndf_gastro$residual_tumor&lt;- factor(df_gastro$residual_tumor)\ndf_gastro$venous_invasion&lt;- factor(df_gastro$venous_invasion)\ndf_gastro$lymphatic_invasion&lt;- factor(df_gastro$lymphatic_invasion)\ndf_gastro$perineural_invasion_present&lt;- factor(df_gastro$perineural_invasion_present)\ndf_gastro$microsatellite_instability&lt;- factor(df_gastro$microsatellite_instability)\ndf_gastro$kras_mutation_found&lt;- factor(df_gastro$kras_mutation_found)\ndf_gastro$stage_event_pathologic_stage&lt;- factor(df_gastro$stage_event_pathologic_stage)\ndf_gastro$cancer_type&lt;- factor(df_gastro$cancer_type)\n</code></pre> <pre><code># Visualizar la distribucion de edades en los distintos tipos de cancer\nggplot(data=subset(df_gastro, !is.na(age_at_initial_pathologic_diagnosis)), aes(x = cancer_type, y = age_at_initial_pathologic_diagnosis, fill = cancer_type))+  \ngeom_boxplot(col = 'black')+\n  labs(title = \"Edad Paciente al Momento del Diagn\u00f3stico\", x =\"Tipo de C\u00e1ncer\", y = \"Edad (a\u00f1os)\") +\n  theme_minimal()+\n  scale_fill_brewer(palette=\"PRGn\")+\n  theme(plot.title = element_text(size=16, color='Darkblue', face='bold', hjust = 0.5))\n</code></pre>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#question-1-what-is-the-correlation-between-the-pathological-stage-of-the-tumor-at-diagnosis-and-survival","title":"Question 1: What is the correlation between the pathological stage of the tumor at diagnosis and survival?","text":"<pre><code># Correlacion entre el estadio tumoral y la supervivencia de los pacientes\nggplot(data=subset(df_gastro, !is.na(stage_event_pathologic_stage)), aes(x = stage_event_pathologic_stage, y = os_time, fill = stage_event_pathologic_stage))+  \ngeom_boxplot(col = 'black')+\n  labs(title = \"Supervivencia en funci\u00f3n del Estadio Tumoral\", x =\"Estadio Tumoral\", y = \"Supervivencia (d\u00edas)\") +\n  theme_minimal()+\n  #scale_fill_brewer(palette=\"PRGn\")+\n  theme(plot.title = element_text(size=16, color='Darkblue', face='bold', hjust = 0.5))\n</code></pre> <pre><code>## Warning: Removed 225 rows containing non-finite outside the scale range\n## (`stat_boxplot()`).\n</code></pre> <p>The survival plot as a function of tumor stage shows that survival progressively decreases as tumor stage increases, as indicated by Singh et al. (https://doi.org/10.5114/pg.2024.141834). Hence the critical importance of early diagnosis in cancer patients. To provide stronger statistical support for this observation, a more rigorous analysis using a Cox proportional-hazards model would be needed, which is beyond the scope of this project.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#question-2-are-there-significant-differences-in-survival-according-to-the-type-of-gastrointestinal-cancer","title":"Question 2: Are there significant differences in survival according to the type of gastrointestinal cancer?","text":"<pre><code># Supervivencia en funci\u00f3n del tipo de c\u00e1ncer \nggplot(data=subset(df_gastro, !is.na(os_time)), aes(x = cancer_type, y = os_time, fill = cancer_type))+ \ngeom_boxplot(col = 'black')+\n  labs(title = \"Supervivencia en funci\u00f3n del tipo de c\u00e1ncer \", x =\"Tipo C\u00e1ncer\", y = \"Superviviencia (d\u00edas)\") +\n  theme_minimal()+\n  #scale_fill_brewer(palette=\"RdBu\")+\n  theme(plot.title = element_text(size=16, color='Darkblue', face='bold', hjust = 0.5))\n</code></pre> <p>The survival plot as a function of cancer type shows that the mean expected survival in PAAD patients is lower than that observed in other cancer types, consistent with the literature on pancreatic ductal adenocarcinoma (PDAC). Singh et al. (https://doi.org/10.5114/pg.2024.141834) point out that PDAC patients have the worst prognosis, with a five-year survival rate barely reaching 12\u201313%. In contrast, other gastrointestinal cancers such as colorectal cancer have a more favorable prognosis, partly thanks to early detection programs and the development of more specific and effective therapies.</p> <p>To identify statistically significant differences between cancer types and patient survival time, we implemented the following tests: ANOVA test, validation of ANOVA residual normality assumptions, Kruskal-Wallis test, and Kaplan-Meier survival analysis.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#anova-test","title":"ANOVA Test","text":"<p>An ANOVA test was implemented assuming normality, then residual normality was checked to analyze whether there are statistically significant differences between cancer types.</p> <pre><code># Eliminar NAs en variables de inter\u00e9s y crear un nuevo dataframe \ndf_anova &lt;- df_gastro[!is.na(df_gastro$os_time) &amp; !is.na(df_gastro$cancer_type), ]\n\n# Implementar test ANOVA variables os_time y cancer_type \nmodelo_anova &lt;- aov(os_time ~ cancer_type, data = df_anova)\nsummary(modelo_anova)\n</code></pre> <pre><code>##               Df    Sum Sq Mean Sq F value   Pr(&gt;F)    \n## cancer_type    4  17419280 4354820   10.36 3.12e-08 ***\n## Residuals   1052 442183622  420327                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n</code></pre> <p>From the results, the p-value is 3.12e-08 ($p &lt; 0.05$), indicating that there are significant differences between at least two groups. Furthermore, the F-value is 10.36 (&gt; 1), and the higher this value, the stronger the evidence of significant differences between groups. Since ANOVA is significant, we next identified which groups differ from each other using TukeyHSD post-hoc test.</p> <pre><code># Aplicar test TukeyHSD\nTukeyHSD(modelo_anova)\n</code></pre> <pre><code>##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = os_time ~ cancer_type, data = df_anova)\n## \n## $cancer_type\n##                 diff       lwr        upr     p adj\n## COAD-CHOL  139.08982 -161.7348  439.91445 0.7137666\n## PAAD-CHOL -147.08580 -469.7097  175.53807 0.7244215\n## READ-CHOL   89.86958 -235.1946  414.93381 0.9430886\n## STAD-CHOL -127.06130 -430.0807  175.95811 0.7820316\n## PAAD-COAD -286.17562 -457.6440 -114.70723 0.0000559\n## READ-COAD  -49.22024 -225.2373  126.79680 0.9407864\n## STAD-COAD -266.15112 -397.0557 -135.24654 0.0000003\n## READ-PAAD  236.95538   25.8329  448.07786 0.0188148\n## STAD-PAAD   20.02450 -155.2659  195.31487 0.9979407\n## STAD-READ -216.93088 -396.6732  -37.18855 0.0089008\n</code></pre> <p>From the <code>TukeyHSD post-hoc test</code> results, the adjusted p-values 0.0000003, 0.0000559, 0.0089008, and 0.0188148 (all &lt; 0.05) show that the greatest significant differences are between groups STAD-COAD, PAAD-COAD, STAD-READ, and READ-PAAD, respectively.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#validation-of-model-residual-normality-assumptions","title":"Validation of Model Residual Normality Assumptions","text":"<p>To validate ANOVA results, we checked the following assumptions: normality of residuals, homoscedasticity, and independence.</p> <pre><code># Validar test ANOVA\nplot(modelo_anova)\n</code></pre> <p>Normality</p> <p>Observing the Q-Q residuals plot (Quantile-Quantile), we can compare the theoretical quantiles of a normal distribution with the actual residual quantiles. In our case, points tend to align close to the diagonal line, especially in the middle part. However, there is evidence of asymmetry in the right tail where an S-shaped curve appears. This may affect the validity of p-values and post-hoc comparisons.</p> <p>Homoscedasticity</p> <p>The homoscedasticity assumption requires that residual variance be constant for all fitted values of the model for each group. From the Scale-Location plot, there is no evident relationship between residuals and fitted values (the mean of each group). The band is almost horizontal and flat. Thus, we can assume homogeneity of variances.</p> <p>Some ideas for answering this question were taken from the post ANOVA in R section Another method to test normality and homogeneity.</p> <p>Independence</p> <p>Since the data are not temporal, we can assume a priori there is no autocorrelation. Independence is assumed by design.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#kruskal-wallis-test","title":"Kruskal-Wallis Test","text":"<p>If we assume that the normality assumption is not met, we can apply the following non-parametric tests: <code>Kruskal-Wallis</code> and <code>Dunn\u2019s post-hoc test</code> to validate our results.</p> <pre><code># Aplicar test Kruskal-Wallis\nkruskal.test(os_time ~ cancer_type, data = df_anova)\n</code></pre> <pre><code>## \n##  Kruskal-Wallis rank sum test\n## \n## data:  os_time by cancer_type\n## Kruskal-Wallis chi-squared = 50.438, df = 4, p-value = 2.925e-10\n</code></pre> <pre><code># Aplicar post-hoc test Dunn\ndunn.test(df_anova$os_time, df_anova$cancer_type, method = \"bonferroni\")\n</code></pre> <pre><code>##   Kruskal-Wallis rank sum test\n## \n## data: x and group\n## Kruskal-Wallis chi-squared = 50.4378, df = 4, p-value = 0\n## \n## \n##                            Comparison of x by group                            \n##                                  (Bonferroni)                                  \n## Col Mean-|\n## Row Mean |       CHOL       COAD       PAAD       READ\n## ---------+--------------------------------------------\n##     COAD |  -0.670817\n##          |     1.0000\n##          |\n##     PAAD |   1.809028   4.580635\n##          |     0.3522    0.0000*\n##          |\n##     READ |  -0.718359  -0.180178  -3.870495\n##          |     1.0000     1.0000    0.0005*\n##          |\n##     STAD |   1.892153   5.921548  -0.058624   4.489049\n##          |     0.2924    0.0000*     1.0000    0.0000*\n## \n## alpha = 0.05\n## Reject Ho if p &lt;= alpha/2\n</code></pre> <p>The non-parametric tests confirmed that the following groups have statistically significant differences: PAAD-COAD, READ-PAAD, STAD-COAD, and STAD-READ.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#kaplan-meier-survival-studies","title":"Kaplan-Meier Survival Studies","text":"<pre><code># Implementar Kaplan-Meier por tipo de cancer\n# Crear objeto de supervivencia\nsurvival_obj &lt;- Surv(time = df_gastro$os_time, event=df_gastro$os_event)\n\n# Ajustar el model Kaplan-Meier por g\u00e9nero\najuste_cancer_type &lt;- survfit(survival_obj ~ cancer_type, type=\"kaplan-meier\", data=df_gastro)\n\n# Para evitar nombres arbitrarios para el tipo de c\u00e1ncer necesitamos utilizar los nombres de grupo que figuran en los datos\nnombres_tipos &lt;- names(ajuste_cancer_type$strata)\n\n# Visualizar gr\u00e1fico\nplot(ajuste_cancer_type, ylab = \"Probabilidad\", xlab='Tiempo (d\u00edas)',\n     mark.time = TRUE, col = hue_pal ()(length(nombres_tipos)), main= \"Supervivencia en Funci\u00f3n del Tipo de Cancer\")\n\nlegend(\"bottomleft\", legend = nombres_tipos,\n       fill = hue_pal ()(length(nombres_tipos)), bty = \"n\")\n</code></pre> <pre><code># Comparamos la supervivencia entre categor\u00edas de tipo de cancer\ncomparar_cancer_type&lt;- survdiff(survival_obj ~ cancer_type, data=df_gastro)\ncomparar_cancer_type\n</code></pre> <pre><code>## Call:\n## survdiff(formula = survival_obj ~ cancer_type, data = df_gastro)\n## \n## n=1057, 251 observations deleted due to missingness.\n## \n##                    N Observed Expected (O-E)^2/E (O-E)^2/V\n## cancer_type=CHOL  38       20     8.76      14.4     15.01\n## cancer_type=COAD 397       57   101.75      19.7     34.75\n## cancer_type=PAAD 146       66    28.17      50.8     57.88\n## cancer_type=READ 136        9    33.79      18.2     21.23\n## cancer_type=STAD 340       87    66.53       6.3      8.79\n## \n##  Chisq= 111  on 4 degrees of freedom, p= &lt;2e-16\n</code></pre> <p>Kaplan-Meier survival test results also show significant differences in survival according to cancer type, with a Chi-square value of 111, 4 degrees of freedom, and a p-value below 2e-16.</p> <p>As previously reported in the literature, patients with pancreatic adenocarcinoma (PAAC) and cholangiocarcinoma have the worst prognoses. In our analysis, we confirmed that patients with pancreatic and biliary tract cancers have a number of deaths clearly higher than expected, reflecting a significantly lower overall survival compared to other tumor types such as colon or rectal cancer.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#question-3-what-survival-differences-are-observed-by-patient-gender-and-age","title":"Question 3: What survival differences are observed by patient gender and age?","text":"<p>Before implementing survival analysis by age, it is recommended to categorize the variable <code>age_at_initial_pathologic_diagnosis</code> ensuring a balanced distribution between groups.</p> <pre><code># Categorizar la variable age_at_initial_pathologic_diagnosis\ndf_gastro$age_category &lt;- cut(df_gastro$age_at_initial_pathologic_diagnosis,\n                       breaks = c(-Inf,59,69,79,Inf),\n                       labels = c(\"Personas_60-\",\"Personas_60\",\"Personas_70\",\"Personas_80+\"),\n                       include.lowest = FALSE)\n\n# Verificar distribuci\u00f3n de categorias\ntable(df_gastro$age_category)\n</code></pre> <pre><code>## \n## Personas_60-  Personas_60  Personas_70 Personas_80+ \n##          368          363          368          153\n</code></pre> <pre><code># Distribucion de edades de pacientes por g\u00e9nero\nggplot(data=subset(df_gastro, !is.na(age_category)), aes(x= age_category, fill = gender))+   \n  geom_bar(position = \"dodge\", alpha = 0.7)+\n  labs(title= \"Distribuci\u00f3n de edades de pacientes por g\u00e9nero\", \n       x= \"Edad (a\u00f1os)\",\n       y= \"Frecuencia\")+\n  theme(plot.title = element_text(size=16, color='Darkblue', face='bold', hjust = 0.5))\n</code></pre>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#kaplan-meier-survival-studies_1","title":"Kaplan-Meier Survival Studies","text":"<p>Kaplan-Meier survival studies by patient gender and age are shown below. For implementation, the following resources were consulted: Survival analysis with low-dimensional input data, Analysis of Cancer Genome Atlas in R and The R Book, 3rd Edition by Elinor Jones, Simon Harden, Michael J. Crawley, cap. 15 Survial Analysis.</p> <pre><code># Implementar Kaplan-Meier por g\u00e9nero \n# Crear objeto de supervivencia\nsurvival_obj &lt;- Surv(time = df_gastro$os_time, event=df_gastro$os_event)\n\n# Ajustar el model Kaplan-Meier por g\u00e9nero\najuste_genero &lt;- survfit(survival_obj ~ gender, type=\"kaplan-meier\", data=df_gastro)\n\n# Visualizar gr\u00e1fico\nplot(ajuste_genero, ylab = \"Probabilidad\", xlab='Tiempo (d\u00edas)',\n     mark.time = TRUE, col = hue_pal ()(2)[1:2], main= \"Supervivencia en Funci\u00f3n del G\u00e9nero\")\n\nlegend(\"bottomleft\", legend = c(\"Female\", \"Male\"),\n       fill = hue_pal ()(2)[1:2], bty = \"n\")\n</code></pre> <pre><code># Comparamos la supervivencia entre g\u00e9neros\ncomparar_generos &lt;- survdiff(survival_obj ~ gender, data=df_gastro)\ncomparar_generos\n</code></pre> <pre><code>## Call:\n## survdiff(formula = survival_obj ~ gender, data = df_gastro)\n## \n## n=1057, 251 observations deleted due to missingness.\n## \n##                 N Observed Expected (O-E)^2/E (O-E)^2/V\n## gender=FEMALE 433       86      102      2.54      4.44\n## gender=MALE   624      153      137      1.89      4.44\n## \n##  Chisq= 4.4  on 1 degrees of freedom, p= 0.04\n</code></pre> <p>Regarding survival and gender, in the Kaplan-Meier plot, the curve for women reaches a stable value of approximately 0.65 at 1,800 days, indicating that about 65% survive beyond that time. In contrast, the men\u2019s curve shows a longer decline, stabilizing at different points, then dropping to about 0.35 at 3,000 days. This implies that only 35% of men reach that survival time threshold. The log-rank test (Chi\u00b2 = 4.4, p = 0.04) supports this visual inspection, where the observed differences are statistically significant. Women have fewer events (deaths) than expected, suggesting better clinical outcomes compared to men.</p> <pre><code># Implementar Kaplan-Meier por edad\n# Ajustar el model Kaplan-Meier por edad\najuste_edad &lt;- survfit(survival_obj ~ age_category, type=\"kaplan-meier\", data=df_gastro)\n\n# Visualizar gr\u00e1fico\nplot(ajuste_edad, ylab = \"Probabilidad\", xlab='Tiempo (d\u00edas)',\n     mark.time = TRUE, col = hue_pal ()(4)[1:4], main= \"Superviviencia en Funci\u00f3n de la Edad\")\n\nlegend(\"bottomleft\", legend = c(\"Personas_60-\",\"Personas_60\",\"Personas_70\",\"Personas_80+\"),\n       fill = hue_pal ()(4)[1:4], bty = \"n\")\n</code></pre> <pre><code># Comparamos la supervivencia entre categor\u00edas de edades\ncomparar_edades &lt;- survdiff(survival_obj ~ age_category, data=df_gastro)\ncomparar_edades\n</code></pre> <pre><code>## Call:\n## survdiff(formula = survival_obj ~ age_category, data = df_gastro)\n## \n## n=1016, 292 observations deleted due to missingness.\n## \n##                             N Observed Expected (O-E)^2/E (O-E)^2/V\n## age_category=Personas_60- 311       51     70.2     5.268     7.789\n## age_category=Personas_60  298       58     64.7     0.695     0.989\n## age_category=Personas_70  298       78     65.3     2.482     3.540\n## age_category=Personas_80+ 109       32     18.8     9.290    10.197\n## \n##  Chisq= 17.8  on 3 degrees of freedom, p= 5e-04\n</code></pre> <p>Regarding survival and age, in the Kaplan-Meier plot, younger groups maintain higher and more sustained survival curves over time, while curves for those over 70 and 80 years show a sharper decline. The log-rank test (Chi\u00b2 = 17.8, df = 3, p = 0.0005) confirms that there are statistically significant differences between age groups. In particular, patients over 80 years have an observed mortality much higher than expected (32 observed vs 18.8 expected). By contrast, patients under 60 have a lower observed mortality than expected (51 vs 70.2), indicating better relative survival.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#question-4-what-is-the-impact-of-venous-lymphatic-or-perineural-invasion-on-survival","title":"Question 4: What is the impact of venous, lymphatic, or perineural invasion on survival?","text":"<pre><code># Reorganizar los datos \ndf_long &lt;- df_gastro %&gt;%\n  select(os_time, perineural_invasion_present, venous_invasion, lymphatic_invasion) %&gt;%\n  pivot_longer(\n    cols = c(perineural_invasion_present, venous_invasion, lymphatic_invasion),\n    names_to = \"tipo_invasion\",\n    values_to = \"presencia\"\n  ) %&gt;%\n  filter(!is.na(presencia))  \n\n# Crear el gr\u00e1fico combinado\nggplot(df_long, aes(x = presencia, y = os_time, fill = presencia)) +\n  geom_boxplot(color = \"black\") +\n  facet_wrap(~ tipo_invasion, scales = \"free_x\") +\n  labs(\n    title = \"Supervivencia seg\u00fan tipo de invasi\u00f3n tumoral\",\n    x = \"Presencia de invasi\u00f3n\",\n    y = \"Supervivencia (d\u00edas)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, color = 'Darkblue', face = 'bold', hjust = 0.5)\n  )\n</code></pre> <pre><code>## Warning: Removed 195 rows containing non-finite outside the scale range\n## (`stat_boxplot()`).\n</code></pre>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#kaplan-meier-survival-studies_2","title":"Kaplan-Meier Survival Studies","text":"<pre><code># Implementar Kaplan-Meier inv\u00e1sion venosa\n# Crear objeto de supervivencia\nsurvival_obj &lt;- Surv(time = df_gastro$os_time, event=df_gastro$os_event)\n\n# Ajustar el model Kaplan-Meier por g\u00e9nero\najuste_invasion_venosa&lt;- survfit(survival_obj ~ venous_invasion, type=\"kaplan-meier\", data=df_gastro)\n\n# Visualizar gr\u00e1fico\nplot(ajuste_invasion_venosa, ylab = \"Probabilidad\", xlab='Tiempo (d\u00edas)',\n     mark.time = TRUE, col = hue_pal ()(2)[1:2], main= \"Supervivencia en funci\u00f3n de la inv\u00e1sion venosa\")\n\nlegend(\"bottomleft\", legend = c(\"Present\", \"Absent\"),\n       fill = hue_pal ()(2)[1:2], bty = \"n\")\n</code></pre> <pre><code># Comparamos la supervivencia entre pacientes con o sin inv\u00e1sion\ncomparar_invasion_venosa &lt;- survdiff(survival_obj ~ venous_invasion, data=df_gastro)\ncomparar_invasion_venosa\n</code></pre> <pre><code>## Call:\n## survdiff(formula = survival_obj ~ venous_invasion, data = df_gastro)\n## \n## n=467, 841 observations deleted due to missingness.\n## \n##                       N Observed Expected (O-E)^2/E (O-E)^2/V\n## venous_invasion=NO  361       29     42.1      4.08        20\n## venous_invasion=YES 106       24     10.9     15.77        20\n## \n##  Chisq= 20  on 1 degrees of freedom, p= 8e-06\n</code></pre> <pre><code># Implementar Kaplan-Meier invasion perineural\n# Crear objeto de supervivencia\nsurvival_obj &lt;- Surv(time = df_gastro$os_time, event=df_gastro$os_event)\n\n# Ajustar el model Kaplan-Meier por g\u00e9nero\najuste_invasion_perineural&lt;- survfit(survival_obj ~ perineural_invasion_present, type=\"kaplan-meier\", data=df_gastro)\n\n# Visualizar gr\u00e1fico\nplot(ajuste_invasion_perineural, ylab = \"Probabilidad\", xlab='Tiempo (d\u00edas)',\n     mark.time = TRUE, col = hue_pal ()(2)[1:2], main= \"Supervivencia en Funci\u00f3n de la Invasi\u00f3n perineural\")\n\nlegend(\"bottomleft\", legend = c(\"Present\", \"Absent\"),\n       fill = hue_pal ()(2)[1:2], bty = \"n\")\n</code></pre> <pre><code># Comparamos la supervivencia entre los pacientes con y sin invasi\u00f3n perineural\ncomparar_invasion_perineural &lt;- survdiff(survival_obj ~ perineural_invasion_present, data=df_gastro)\ncomparar_invasion_perineural\n</code></pre> <pre><code>## Call:\n## survdiff(formula = survival_obj ~ perineural_invasion_present, \n##     data = df_gastro)\n## \n## n=247, 1061 observations deleted due to missingness.\n## \n##                                   N Observed Expected (O-E)^2/E (O-E)^2/V\n## perineural_invasion_present=NO  182       25    31.74      1.43      6.43\n## perineural_invasion_present=YES  65       16     9.26      4.91      6.43\n## \n##  Chisq= 6.4  on 1 degrees of freedom, p= 0.01\n</code></pre> <pre><code># Implementar Kaplan-Meier Invasi\u00f3n linf\u00e1tica\n# Crear objeto de supervivencia\nsurvival_obj &lt;- Surv(time = df_gastro$os_time, event=df_gastro$os_event)\n\n# Ajustar el model Kaplan-Meier por g\u00e9nero\najuste_invasion_linfatica&lt;- survfit(survival_obj ~ lymphatic_invasion, type=\"kaplan-meier\", data=df_gastro)\n\n# Visualizar gr\u00e1fico\nplot(ajuste_invasion_linfatica, ylab = \"Probabilidad\", xlab='Tiempo (d\u00edas)',\n     mark.time = TRUE, col = hue_pal ()(2)[1:2], main= \"Supervivencia en Funci\u00f3n de la Invasi\u00f3n Linf\u00e1tica\")\n\nlegend(\"bottomleft\", legend = c(\"Present\", \"Absent\"),\n       fill = hue_pal ()(2)[1:2], bty = \"n\")\n</code></pre> <pre><code># Comparamos la supervivencia entre los pacientes con y sin invasi\u00f3n perineural\ncomparar_invasion_linfatica &lt;- survdiff(survival_obj ~ lymphatic_invasion, data=df_gastro)\ncomparar_invasion_linfatica\n</code></pre> <pre><code>## Call:\n## survdiff(formula = survival_obj ~ lymphatic_invasion, data = df_gastro)\n## \n## n=483, 825 observations deleted due to missingness.\n## \n##                          N Observed Expected (O-E)^2/E (O-E)^2/V\n## lymphatic_invasion=NO  305       25       36      3.37      10.3\n## lymphatic_invasion=YES 178       29       18      6.75      10.3\n## \n##  Chisq= 10.3  on 1 degrees of freedom, p= 0.001\n</code></pre> <p>Survival analysis by histopathological variables revealed significant differences associated with venous, lymphatic, and perineural invasion. These three features are known to reflect greater tumor aggressiveness, and our results statistically confirmed this.</p> <p>First, venous invasion showed a highly significant association with survival (Chi-square = 20, p = 8e-06), with a number of deaths much higher than expected in patients with positive invasion. Similarly, lymphatic invasion was associated with worse prognosis (Chi-square = 10.3, p = 0.001), with higher mortality in the invasion-positive group.</p> <p>Finally, perineural invasion was significantly associated with reduced survival (Chi-square = 6.4, p = 0.01), although with a smaller cohort size.</p> <p>These results support the clinical value of these variables as potential negative prognostic markers in gastrointestinal cancer patients. They also highlight the importance of including them in risk stratification and therapeutic decision-making.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#question-5-what-is-the-bmi-distribution-of-patients-by-gender","title":"Question 5: What is the BMI distribution of patients by gender?","text":"<p>BMI (Body Mass Index) is a widely used indicator in oncology to assess metabolic risk, prognosis, and treatment response, according to the IARC Working Group report Body Fatness and Cancer \u2014 Viewpoint of the IARC Working Group. BMI at diagnosis may reflect comorbidities (diabetes, hypertension) that can influence cancer progression. For this reason, we included BMI as a key variable for patient stratification, referencing NIH-proposed categories. For more details, consult Obesity and Cancer.</p> <pre><code># Funci\u00f3n para calcular el IMC de un paciente \ncalcular_imc &lt;- function(peso, altura){\n  imc &lt;- peso / (altura/100)^2\n  return (imc)\n}\n\n# calcular_imc &lt;- function(peso, altura){\n#   tryCatch(\n#     {\n#       # Convertir en valores num\u00e9ricos por default\n#       #peso      &lt;- as.numeric(peso)\n#       #altura    &lt;- as.numeric(altura)\n#       \n#       # Validar rango de valores de entrada. \n#       if(altura &lt;= 0.0){\n#         stop(\"Error: La variable altura no debe ser negativa o cero\")\n#       }\n#       \n#       imc &lt;- peso / (altura^2)\n#       return(imc)\n#     },\n#     error = function(e){\n#         message(\"Error en el c\u00e1lculo: \", e$message)\n#         return(NA)\n#     }\n#   )\n# }\n\n# Crear la variable imc_at_initial_pathologic_diagnosis\ndf_gastro$imc &lt;- calcular_imc(df_gastro$weight, df_gastro$height)\n\n# Categorizar la variable imc seg\u00fan las categor\u00edas propuestas por NIH\ndf_gastro$imc_category &lt;- cut(df_gastro$imc,\n                       breaks = c(-Inf,18.5,25.0,30.0,Inf),\n                       labels = c(\"Bajo_Peso\",\"Normal\",\"Sobrepeso\",\"Obesidad\"),\n                       include.lowest = FALSE)\n\n# Verificar distribuci\u00f3n de categorias\ntable(df_gastro$imc_category)\n</code></pre> <pre><code>## \n## Bajo_Peso    Normal Sobrepeso  Obesidad \n##         6       108       138        99\n</code></pre> <pre><code># Distribucion de IMC de pacientes por g\u00e9nero\nggplot(data=subset(df_gastro, !is.na(imc_category)), aes(x= imc_category, fill = gender))+   \n  geom_bar(position = \"dodge\", alpha = 0.7)+\n  labs(title= \"Distribuci\u00f3n de IMC de pacientes por g\u00e9nero\", \n       x= \"Edad (a\u00f1os)\",\n       y= \"Frecuencia\")+\n  theme(plot.title = element_text(size=16, color='Darkblue', face='bold', hjust = 0.5))\n</code></pre>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#32-probability-study","title":"3.2 Probability Study","text":"<p>This section proposes three probability problems related to the TCGA dataset.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#exercise-a","title":"Exercise A","text":"<p>In the customized TCGA dataset (<code>df_gastro</code>), approximately 14.15% of patients have pancreatic cancer, according to the <code>cancer_type</code> variable. If we take a random sample of 100 patients:</p> <ul> <li> <p>What is the probability that exactly 5 patients have pancreatic cancer?</p> </li> <li> <p>What is the probability that at most 10 patients have pancreatic cancer?</p> </li> </ul> <pre><code># Verificar distribuci\u00f3n de categorias por tipo de c\u00e1ncer\nfrecuencias &lt;- table(df_gastro$cancer_type)\nfrecuencias\n</code></pre> <pre><code>## \n##      CHOL COAD PAAD READ STAD \n##    1   48  459  185  171  443\n</code></pre> <pre><code># Verificar porcentaje de distribuci\u00f3n de categor\u00edas por tipo de c\u00e1ncer\n# porcentajes &lt;- prop.table(frecuencias) * 100\nporcentajes &lt;- round(prop.table(frecuencias) * 100, 2)\nporcentajes\n</code></pre> <pre><code>## \n##        CHOL  COAD  PAAD  READ  STAD \n##  0.08  3.67 35.12 14.15 13.08 33.89\n</code></pre> <pre><code># Describir datos para calcular P(X = 5)\nn &lt;- 100\np &lt;- 0.1415\nx &lt;- 5\n\n# Aplicar distribuci\u00f3n binomial para P(X = 5)\npaciente_5 &lt;- dbinom(x,n,p)\n\nprint(paste(\"La probabilidad de que exactamente 5 pacientes tengan c\u00e1ncer de p\u00e1ncreas es:\",\n            round(paciente_5,4)*100,\"%\"))\n</code></pre> <pre><code>## [1] \"La probabilidad de que exactamente 5 pacientes tengan c\u00e1ncer de p\u00e1ncreas es: 0.22 %\"\n</code></pre> <pre><code># Describir datos para calcular P(X &lt;= 10)\nn &lt;- 100\np &lt;- 0.1415\nx &lt;- 10\n\n# Aplicar distribuci\u00f3n binomial para P(X &lt;= 10)\npaciente_10 &lt;- pbinom(x, n, p)\n\nprint(paste(\"La probabilidad de que un m\u00e1ximo de 10 pacientes tengan c\u00e1ncer de p\u00e1ncreas es:\",\n            round(paciente_10, 4)*100,\"%\"))\n</code></pre> <pre><code>## [1] \"La probabilidad de que un m\u00e1ximo de 10 pacientes tengan c\u00e1ncer de p\u00e1ncreas es: 14.62 %\"\n</code></pre>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#exercise-b","title":"Exercise B","text":"<p>Of the patients with perineural invasion, what is the probability that they have an advanced pathological stage, equal to or greater than type III?</p> <p>This is a conditional probability problem where we are asked: P(Stage &gt;= III | Perineural Invasion = YES)</p> <pre><code># Visualizar variables categ\u00f3ricas de inter\u00e9s\nprint(\"Distribuci\u00f3n de categor\u00edas para la variable perineural_invasion_present\")\n</code></pre> <pre><code>## [1] \"Distribuci\u00f3n de categor\u00edas para la variable perineural_invasion_present\"\n</code></pre> <pre><code>table(df_gastro$perineural_invasion_present)\n</code></pre> <pre><code>## \n##      NO YES \n##   1 206  71\n</code></pre> <pre><code>print(\"Distribuci\u00f3n de categor\u00edas para la variable stage_event_pathologic_stage\")\n</code></pre> <pre><code>## [1] \"Distribuci\u00f3n de categor\u00edas para la variable stage_event_pathologic_stage\"\n</code></pre> <pre><code>table(df_gastro$stage_event_pathologic_stage)\n</code></pre> <pre><code>## \n##         I   IA   IB   II  IIA  IIB  IIC  III IIIA IIIB IIIC   IV  IVA  IVB \n##    1  131   22   56   82  248  193    2   36   94  149   93  115   27    7\n</code></pre> <pre><code># Eliminar NAs en variables de inter\u00e9s y crear un nuevo dataframe \ndf_estate_peri &lt;- df_gastro[!is.na(df_gastro$perineural_invasion_present) &amp; \n                      !is.na(df_gastro$stage_event_pathologic_stage), ]\n</code></pre> <p>To study the relationship between categorical variable groups, the next step is to build a 14 x 2 contingency table, showing counts for each invasion_perineural\u2013stage combination, resulting in 28 observed frequencies.</p> <pre><code># Crear tabla de contingencia \ntabla_conti &lt;- table(df_estate_peri$perineural_invasion_present,\n               df_estate_peri$stage_event_pathologic_stage)\nprint(\"Tabla de contingencia de variables categ\u00f3ricas\")\n</code></pre> <pre><code>## [1] \"Tabla de contingencia de variables categ\u00f3ricas\"\n</code></pre> <pre><code>tabla_conti\n</code></pre> <pre><code>##      \n##           I IA IB II IIA IIB IIC III IIIA IIIB IIIC IV IVA IVB\n##        1  0  0  0  0   0   0   0   0    0    0    0  0   0   0\n##   NO   0 53  1  0  9  52   4   1   7    9   31   14  6  10   4\n##   YES  0  8  0  0  4  13   5   0   1    0   13    4 10   6   3\n</code></pre> <pre><code># Calcular proporciones para cada grupo de pacientes con invasi\u00f3n perineural \nprop_conti &lt;- prop.table(tabla_conti, margin = 1)  # margin = 1 obtiene proporciones por fila\nprint(\"Tabla de proporciones\")\n</code></pre> <pre><code>## [1] \"Tabla de proporciones\"\n</code></pre> <pre><code>prop_conti\n</code></pre> <pre><code>##      \n##                             I          IA          IB          II         IIA\n##       1.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000\n##   NO  0.000000000 0.263681592 0.004975124 0.000000000 0.044776119 0.258706468\n##   YES 0.000000000 0.119402985 0.000000000 0.000000000 0.059701493 0.194029851\n##      \n##               IIB         IIC         III        IIIA        IIIB        IIIC\n##       0.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000\n##   NO  0.019900498 0.004975124 0.034825871 0.044776119 0.154228856 0.069651741\n##   YES 0.074626866 0.000000000 0.014925373 0.000000000 0.194029851 0.059701493\n##      \n##                IV         IVA         IVB\n##       0.000000000 0.000000000 0.000000000\n##   NO  0.029850746 0.049751244 0.019900498\n##   YES 0.149253731 0.089552239 0.044776119\n</code></pre> <pre><code># Obtener la proporci\u00f3n solo para el grupo con invasi\u00f3n perineural = 'YES' y estadios &gt; III\nprint(\"Tabla de proporciones para pacientes con invasi\u00f3n perineural = 'YES' y estadios &gt; III\")\n</code></pre> <pre><code>## [1] \"Tabla de proporciones para pacientes con invasi\u00f3n perineural = 'YES' y estadios &gt; III\"\n</code></pre> <pre><code>prop_conti[\"YES\", c(\"III\", \"IIIA\", \"IIIB\", \"IIIC\", \"IV\", \"IVA\", \"IVB\")]\n</code></pre> <pre><code>##        III       IIIA       IIIB       IIIC         IV        IVA        IVB \n## 0.01492537 0.00000000 0.19402985 0.05970149 0.14925373 0.08955224 0.04477612\n</code></pre> <pre><code># Obtener la probabilidad total conjunta\nprob_conjunta &lt;- sum(prop_conti[\"YES\", c(\"III\", \"IIIA\", \"IIIB\", \"IIIC\", \"IV\", \"IVA\", \"IVB\")])\npaste(\"Probabilidad estadio igual o superior a tipo III:\", round(prob_conjunta, 4)*100,\"%\")\n</code></pre> <pre><code>## [1] \"Probabilidad estadio igual o superior a tipo III: 55.22 %\"\n</code></pre>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#exercise-c","title":"Exercise C","text":"<p>The mean age at diagnosis is 65.85 years with a standard deviation of 11.88 years. Assuming a normal distribution, what is the probability that a patient is between 60 and 70 years old?</p> <p>In this problem we will use the normal distribution to obtain P(60 &lt; X &lt; 70)</p> <pre><code># Describir datos para obtener P(60 &lt; X &lt; 70)\nmedia  &lt;- mean(df_gastro$age_at_initial_pathologic_diagnosis, na.rm = TRUE)\nsigma  &lt;- sd(df_gastro$age_at_initial_pathologic_diagnosis, na.rm = TRUE)\nx1     &lt;- 60\nx2     &lt;- 70\n\n# Aplicar distribuci\u00f3n normal para obtener la probabilidad P(60 &lt; X &lt; 70)\nmenor_a_70   &lt;- pnorm(x2, media, sigma)\nmenor_a_60   &lt;- pnorm(x1, media, sigma)\nentre_70_60 &lt;- menor_a_70 - menor_a_60\n\nprint(paste(\"La probabilidad de que un paciente tenga entre 60 y 70 a\u00f1os es:\",\n            round(entre_70_60, 4)*100,\"%\"))\n</code></pre> <pre><code>## [1] \"La probabilidad de que un paciente tenga entre 60 y 70 a\u00f1os es: 32.55 %\"\n</code></pre>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#4-machine-learning-models","title":"4 Machine Learning Models","text":"<p>Since our goal is to identify factors associated with cancer patient survival, we opted to apply supervised learning models, as we have a well-defined dependent variable: overall survival (<code>os_event</code>).</p> <p>We trained a supervised learning model using Support Vector Machine (SVM). For this, we selected a subset of clinical variables including age at diagnosis, gender, residual tumor type, pathological stage, number of positive nodes, cancer type, and presence of venous, lymphatic, or perineural invasion.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#41-supervised-model-svm","title":"4.1 Supervised Model (SVM)","text":"<pre><code># Filtramos filas sin valores NA en variables clave\nmodel_df_supervised &lt;- na.omit(df_gastro[, c(\"age_at_initial_pathologic_diagnosis\",\n                                 \"gender\", \"residual_tumor\", \"stage_event_pathologic_stage\",\n                                \"number_of_lymphnodes_positive_by_he\",\n                                \"cancer_type\", \n                                \"venous_invasion\", \"lymphatic_invasion\",\n                                \"perineural_invasion_present\",\n                                \"os_event\")])\n\n# Convertimos os_event en factor (para clasificaci\u00f3n)\nmodel_df_supervised$os_event &lt;- as.factor(model_df_supervised$os_event)\n\n# Dividimos el conjunto de datos en entrenamiento y prueba\nset.seed(123)  # Para reproducibilidad\nindices_entrenamiento &lt;- sample(1:nrow(model_df_supervised), 0.5 * nrow(model_df_supervised))\nconjunto_entrenamiento &lt;- model_df_supervised[indices_entrenamiento, ]\nconjunto_prueba &lt;- model_df_supervised[-indices_entrenamiento, ]\n\n# Cargamos el paquete \"kernlab\" para SVM\nlibrary(kernlab)\n</code></pre> <pre><code>## Warning: package 'kernlab' was built under R version 4.4.1\n\n## \n## Attaching package: 'kernlab'\n\n## The following object is masked from 'package:ggplot2':\n## \n##     alpha\n\n## The following object is masked from 'package:scales':\n## \n##     alpha\n</code></pre> <pre><code># Entrenamos un modelo SVM\nmodelo_svm &lt;- ksvm(os_event ~ ., data = conjunto_entrenamiento, kernel = \"rbfdot\", type = \"C-svc\") #indicamos que el objetivo es clasificacion\n\n# Realizar predicciones en el conjunto de prueba\npredicciones &lt;- predict(modelo_svm, newdata = conjunto_prueba)\n\n# Calcular la matriz de confusi\u00f3n\nconfusion_matriz &lt;- table(Real = conjunto_prueba$os_event, Predicci\u00f3n =\npredicciones)\n\n# Calcular la precisi\u00f3n\nprecision &lt;- sum(diag(confusion_matriz)) / sum(confusion_matriz)\ncat(\"Precisi\u00f3n del modelo SVM:\", precision, \"\\n\")\n</code></pre> <pre><code>## Precisi\u00f3n del modelo SVM: 0.9354839\n</code></pre> <pre><code># Mostrar la matriz de confusi\u00f3n\nconfusion_matriz\n</code></pre> <pre><code>##     Predicci\u00f3n\n## Real  0  1\n##    0 87  0\n##    1  6  0\n</code></pre> <p>Dataset Preprocessing</p> <p>Rows with NA values in the selected variables were filtered out. Additionally, <code>os_event</code> was converted into a factor to enable classification. The resulting dataset was randomly split into two subsets: training (50%) and testing (50%).</p> <p>Model Training</p> <p>The <code>ksvm()</code> function from the <code>kernlab</code> package was used to train an SVM model with an <code>rbfdot</code> radial kernel. The model was fitted using the training data and then evaluated on the test set.</p> <p>Results</p> <p>The model achieved an accuracy of 93.5% on the test set. However, the confusion matrix reveals that the model failed to correctly classify any deceased patients: all deceased patients were classified as alive. This indicates that the model is biased toward the majority class (alive patients), likely due to a strong imbalance in the alive/deceased distribution. In other words, while the model appears accurate overall, its ability to detect patients with poor prognosis is zero. To correct this imbalance, we would need to add more patients with poor prognosis.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#42-pca-unsupervised-model","title":"4.2 PCA - Unsupervised Model","text":"<p>In this part of the work we used an unsupervised learning model to explore whether patients\u2019 clinical data show any natural pattern or grouping without telling the model what to predict. For this, we used Principal Component Analysis (PCA).</p> <pre><code># # Filtramos filas sin valores NA en variables clave\nmodel_df_unsupervised &lt;- na.omit(df_gastro[, c(\"age_at_initial_pathologic_diagnosis\",\n                                 \"gender\", \"residual_tumor\", \"stage_event_pathologic_stage\",\n                                \"number_of_lymphnodes_positive_by_he\",\n                                \"cancer_type\", \n                                \"venous_invasion\", \"lymphatic_invasion\",\n                                \"perineural_invasion_present\")])\n\n# Transformamos las variables categoricas en numericas \n# Ref. \"The model.matrix function\" The R Book page 255\ndf_numeric &lt;- model.matrix(~ gender + age_at_initial_pathologic_diagnosis + \n                              residual_tumor + \n                              number_of_lymphnodes_positive_by_he + \n                              venous_invasion + \n                              lymphatic_invasion + \n                              perineural_invasion_present + \n                              stage_event_pathologic_stage - 1, data = model_df_unsupervised)\n\n# Aplicamos PCA\npca_resultado &lt;- prcomp(df_numeric, center = TRUE)\n\n# Resumen de la varianza explicada por cada variable\nsummary(pca_resultado)\n</code></pre> <pre><code>## Importance of components:\n##                           PC1    PC2     PC3    PC4     PC5     PC6     PC7\n## Standard deviation     12.855 4.6032 0.81452 0.7006 0.52639 0.48330 0.43598\n## Proportion of Variance  0.875 0.1122 0.00351 0.0026 0.00147 0.00124 0.00101\n## Cumulative Proportion   0.875 0.9872 0.99070 0.9933 0.99477 0.99600 0.99701\n##                            PC8     PC9    PC10   PC11    PC12   PC13    PC14\n## Standard deviation     0.41054 0.29120 0.28441 0.2380 0.22723 0.1941 0.15327\n## Proportion of Variance 0.00089 0.00045 0.00043 0.0003 0.00027 0.0002 0.00012\n## Cumulative Proportion  0.99790 0.99835 0.99878 0.9991 0.99935 0.9996 0.99968\n##                           PC15    PC16    PC17    PC18    PC19    PC20\n## Standard deviation     0.14545 0.11274 0.10065 0.07723 0.07598 0.07337\n## Proportion of Variance 0.00011 0.00007 0.00005 0.00003 0.00003 0.00003\n## Cumulative Proportion  0.99979 0.99986 0.99991 0.99994 0.99997 1.00000\n##                             PC21      PC22      PC23      PC24      PC25\n## Standard deviation     9.744e-16 8.971e-16 8.971e-16 8.971e-16 8.971e-16\n## Proportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n## Cumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n##                             PC26      PC27      PC28      PC29\n## Standard deviation     8.971e-16 8.971e-16 8.971e-16 8.112e-16\n## Proportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n## Cumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00\n</code></pre> <pre><code># Representamos la varianza explicada por cada componente\nplot(pca_resultado)\n</code></pre> <pre><code>cargos &lt;- pca_resultado$rotation\n#print(cargos)\n</code></pre> <p>PCA Results</p> <p>The first component (PC1) explains 87.5% of the total variance of the dataset, while the second component (PC2) adds an additional 11.2%. Together, PC1 and PC2 explain approximately 98.7% of the total variance, indicating that the main information in the data can be effectively summarized in two dimensions.</p> <p>Loadings analysis revealed that:</p> <ul> <li> <p>PC1 is strongly influenced by age at diagnosis, suggesting that this variable is the main driver of variability among patients.</p> </li> <li> <p>PC2 is dominated by the number of positive lymph nodes, followed by a smaller contribution from variables related to tumor invasion (lymphatic, venous, and perineural).</p> </li> </ul> <pre><code># Realizamos agrupamiento jer\u00e1rquico aglomerativo\ndist_matrix &lt;- dist(df_numeric)\n# Aplicamos agrupamiento jer\u00e1rquico aglomerativo con el m\u00e9todo de Ward\nhc_aglomerativo &lt;- hclust(dist_matrix, method = \"ward.D2\") \n# Reprsentamos el dendrograma\nplot(hc_aglomerativo, main = \"Dendrograma de Agrupamiento Jer\u00e1rquico\nAglomerativo\", xlab = \"Pacientes\")\n</code></pre>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#5-visualization-with-shiny","title":"5 Visualization with Shiny","text":"<p>The Shiny application was developed to enable interactive analysis of clinical data from gastrointestinal cancer patients, using a CSV file uploaded by the user and an exploratory and visual approach based on the <code>.Rmd file</code>. The <code>gastro_app.R</code> file includes the application source code (attached in the PEC4 submission), and some screenshots are shown below to illustrate its operation.</p> <p>Overall, the application incorporates key dataset transformations, including calculating survival time (<code>os_time</code>), death event (<code>os_event</code>), and BMI, from variables such as weight and height. In addition, new categorical variables are generated for age (<code>age_category</code>) and BMI (<code>imc_category</code>), which facilitate comparative analysis among different patient groups.</p> General view of the application after loading the CSV data file <p>The application contains a sidebar with selectors that allow the user to choose variables for three main sections:</p> <ul> <li>Survival by Clinical Variables: Shows a boxplot representing the relationship between a categorical variable (such as cancer type or tumor stage) and a quantitative variable (for example, survival time or age).</li> </ul> Survival by Clinical Variables <ul> <li>Age and BMI Distribution: Uses a bar chart to analyze frequency distribution of variables such as age and BMI, stratified by gender.  </li> </ul> Age and BMI Distribution <ul> <li>Kaplan-Meier Survival Analysis: Using the survival library, a Kaplan-Meier model is fitted to assess the probability of survival over time, based on selected variables such as gender or age group.  </li> </ul> Kaplan-Meier Survival Analysis"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#6-conclusions","title":"6 Conclusions","text":"<p>The project\u2019s objective was to analyze clinical data from gastrointestinal cancer patients obtained from the public TCGA repository (The Cancer Genome Atlas), to study possible clinical factors influencing patient survival. From the dataset of more than 1,300 cases: 1) We selected relevant variables, cleaned and transformed the data, 2) Created exploratory plots and performed statistical analyses, 3) Implemented Kaplan-Meier survival analysis, 4) Trained both a supervised model (SVM) to predict survival and an unsupervised model (PCA and hierarchical clustering) to explore natural groupings among patients, and 5) Implemented a Shiny application to facilitate interactive visualization of these data.</p> <p>Below we describe the main findings of the TCGA dataset analysis. We also discuss limitations and areas of opportunity, and close with a comment on the team experience developing this project.</p> <p>Most Interesting Findings of Our Analysis</p> <ul> <li> <p>Results from parametric <code>ANOVA</code> and <code>TukeyHSD</code> tests, as well as from <code>non-parametric Kruskal-Wallis</code> and <code>Dunn tests</code>, confirmed that there are statistically significant differences between gastrointestinal cancer types and survival days for the following groups: Pancreas-Colon, Pancreas-Rectum, Stomach-Colon, and Stomach-Rectum.</p> </li> <li> <p>Kaplan-Meier plots showed that women have greater survival than men, with about 65% surviving beyond 1,800 days, compared to 35% of men at 3,000 days. The log-rank test (p = 0.04) confirms that these differences are statistically significant, suggesting a more favorable clinical course for women.</p> </li> <li> <p>Regarding age, younger patients have higher and more sustained survival curves, while those over 70 and 80 show faster decline. The log-rank test (p = 0.0005) shows evidence of significant differences between age groups. This is consistent with the fact that older age can be associated with higher mortality and lower treatment tolerance.</p> </li> <li> <p>Finally, conditional probability analysis showed that 55.22% of patients with perineural invasion had an advanced pathological stage (type III or higher) at diagnosis. This suggests a strong association between perineural invasion and greater tumor progression.</p> </li> </ul> <p>Limitations of the Data and Possible Improvements or Extensions</p> <p>One of the main limitations of the analysis was the high proportion of missing data, especially in categorical variables such as <code>venous_invasion</code>, <code>lymphatic_invasion</code>, and <code>perineural_invasion_present</code>, as well as some continuous variables such as <code>days_to_death</code>, <code>weight</code>, and <code>height</code>. This reduced the effective sample size. Moreover, since this is a retrospective observational study, i.e., data come from an existing repository rather than a controlled experimental design, associations could be identified but not causal relationships. As an improvement, we could apply imputation techniques to handle missing data and validate the results with external cohorts. Additionally, some studies of this type also incorporate genomic data to enrich the analysis and provide a more comprehensive view of survival factors.</p> <p>On Teamwork and Project Evaluation</p> <p>Salomon\u2019s Comment: Teamwork with Sefora was key to selecting the dataset, thanks to her experience in cancer research. This helped us define key questions about how certain clinical variables could affect survival in gastrointestinal cancer patients. I really enjoyed this project because it allowed me to put theoretical concepts into practice with a real dataset. I think the collaboration was effective as we divided tasks, maintained good communication, and reviewed each other\u2019s work.</p> <p>Sefora\u2019s Comment: The project allowed us to apply multiple data analysis tools in R and deepen our study of survival in gastrointestinal cancer, which is the focus of my research at Vall d\u2019Hebron Institute of Oncology. I hope to apply the same tools to analyze data from patients at my center and advance understanding of these devastating diseases. It was a pleasure working with Salomon, as we have complementary approaches and perspectives that allowed us to mutually improve our work. We had good communication and divided tasks equally.</p>"},{"location":"bioinformatics/clinical-factors-associated-with-survival-in-gastrointestinal-cancer/#7-bibliography","title":"7 Bibliography","text":"<ol> <li> <p>M. F. Bijlsma, A. Sadanandam, P. Tan, L. Vermeulen, Molecular subtypes in cancers of the gastrointestinal tract. Nat Rev Gastroenterol Hepatol 14, 333\u2013342 (2017).</p> </li> <li> <p>R. L. Siegel, K. D. Miller, H. E. Fuchs, A. Jemal, Cancer statistics, 2022. CA: A Cancer Journal for Clinicians 72, 7\u201333 (2022).</p> </li> <li> <p>J. Drost, H. Clevers, Organoids in cancer research. Nat Rev Cancer 18, 407\u2013418 (2018).</p> </li> <li> <p>P. W. Nagle, J. Th. M. Plukker, C. T. Muijs, P. van Luijk, R. P. Coppes, Patient-derived tumor organoids for prediction of cancer treatment response. Seminars in Cancer Biology 53, 258\u2013264 (2018).</p> </li> <li> <p>C. A. Pasch, P. F. Favreau, A. E. Yueh, C. P. Babiarz, A. A. Gillette, J. T. Sharick, M. R. Karim, K. P. Nickel, A. K. DeZeeuw, C. M. Sprackling, P. B. Emmerich, R. A. DeStefanis, R. T. Pitera, S. N. Payne, D. P. Korkos, L. Clipson, C. M. Walsh, D. Miller, E. H. Carchman, M. E. Burkard, K. K. Lemmon, K. A. Matkowskyj, M. A. Newton, I. M. Ong, M. F. Bassetti, R. J. Kimple, M. C. Skala, D. A. Deming, Patient-Derived Cancer Organoid Cultures to Predict Sensitivity to Chemotherapy and Radiation. Clinical Cancer Research 25, 5376\u20135387 (2019).</p> </li> <li> <p>E. R. Zanella, E. Grassi, L. Trusolino, Towards precision oncology with patient-derived xenografts. Nat Rev Clin Oncol 19, 719\u2013732 (2022).</p> </li> <li> <p>X. Sun, B. Hu, Mathematical modeling and computational prediction of cancer drug resistance. Briefings in Bioinformatics 19, 1382\u20131399 (2018).</p> </li> <li> <p>E. J. Mucaki, J. Z. L. Zhao, D. J. Lizotte, P. K. Rogan, Predicting responses to platin chemotherapy agents with biochemically-inspired machine learning. Sig Transduct Target Ther 4, 1\u201312 (2019).</p> </li> <li> <p>C. Yang, X. Huang, Y. Li, J. Chen, Y. Lv, S. Dai, Prognosis and personalized treatment prediction in TP53-mutant hepatocellular carcinoma: an in silico strategy towards precision oncology. Briefings in Bioinformatics 22, bbaa164 (2021).</p> </li> <li> <p>F. Li, Y. Yang, Y. Wei, P. He, J. Chen, Z. Zheng, H. Bu, Deep learning-based predictive biomarker of pathological complete response to neoadjuvant chemotherapy from histological images in breast cancer. J Transl Med 19, 348 (2021). 11. Z. Liu, Z. Li, J. Qu, R. Zhang, X. Zhou, L. Li, K. Sun, Z. Tang, H. Jiang, H. Li, Q. Xiong, Y. Ding, X. Zhao, K. Wang, Z. Liu, J. Tian, Radiomics of Multiparametric MRI for Pretreatment Prediction of Pathologic Complete Response to Neoadjuvant Chemotherapy in Breast Cancer: A Multicenter Study. Clinical Cancer Research 25, 3538\u20133547 (2019).</p> </li> </ol>"},{"location":"bioinformatics/nextflow-for-rnaseq-aws/","title":"Nextflow RNA-seq pipelines Executed on AWS","text":"<p>by Salomon Marquez</p> <p>01/08/2025 </p> <p>This project provides a cloud-based implementation of RNA-seq workflows for training and experimentation purposes \ud83d\udcda. It combines content from the official Nextflow for RNA-seq training course with real-world RNA-seq data from the DIY Transcriptomics course, executed entirely on AWS infrastructure.</p> <p>The goal is to demonstrate how to run scalable and reproducible RNA-seq pipelines using Nextflow, while leveraging AWS compute resources for performance, flexibility, and cost optimization.</p> <p>We would like to thank AWS and the University of Navarra for providing cloud resources (1,000 AWS credits) to support this project \ud83d\udcb0.</p> <p>\u26a1 Click here to view this project on GitHub. </p>"},{"location":"bioinformatics/nextflow-for-rnaseq-aws/#motivation","title":"Motivation","text":"<p>Nextflow is an open-source tool used to run bioinformatics pipelines for analyzing multiple samples in parallel. As of 2025, Nextflow has established a solid community that actively contributes to its well-documented pipelines and templates for building custom workflows. It has become a popular and practical tool among bioinformaticians.</p> <p>One of Nextflow\u2019s key strengths is its portability. Whether you're running workflows on your laptop, a high-performance computing (HPC) cluster, or cloud platforms like AWS, Nextflow abstracts the complexity of the compute environment so you can focus on your analysis. </p> <p>This project began after we received $1,000 in AWS Cloud credits. Our goal was to lay the foundation for running a Bulk RNA-seq analysis \ud83e\uddec using AWS infrastructure \ud83d\udcbb. To avoid discarding valuable insights that could benefit newcomers to Nextflow, we created this practical guide for running RNA-seq analyses on the AWS Cloud.</p>"},{"location":"bioinformatics/nextflow-for-rnaseq-aws/#rna-seq-pipelines","title":"RNA-seq Pipelines","text":"<p>The content of this project is structured as follows:</p> Pipeline Description <code>nextflow-for-rnaseq-aws</code> Includes a reproduction of the Nextflow for RNA-seq pipeline, adapted for AWS execution. <code>rnaseq-aws-diy-transcriptomics</code> Implements an RNA-seq pipeline using skin tissue data (10 samples) from the DIY Transcriptomics course. It builds a real-world workflow using HISAT2 for the alignment of reads, and explores AWS insfrastructure setup. <code>rnaseq-aws-diy-kallisto</code> Adapts the <code>rnaseq-aws-diy-transcriptomics</code> workflow to use Kallisto, a pseudoaligner, for lightweight RNA-seq analysis. This pipeline generates expression quantification (<code>abundance.tsv</code>) files for the same dataset."},{"location":"bioinformatics/nextflow-for-rnaseq-aws/#rna-seq-pipelines-results","title":"RNA-seq Pipelines: Results","text":"<p>This is a summary of results \ud83c\udfaf from RNA-seq pipelines run with Nextflow on AWS Batch.</p>"},{"location":"bioinformatics/nextflow-for-rnaseq-aws/#pipeline-1-nextflow-for-rnaseq-aws-pipeline","title":"PIPELINE 1: nextflow-for-rnaseq-aws pipeline","text":"<ul> <li>MULTIQC Report</li> <li>Nextflow Report</li> </ul>"},{"location":"bioinformatics/nextflow-for-rnaseq-aws/#pipeline-2-rnaseq-aws-diy-transcriptomics-pipeline","title":"PIPELINE 2: rnaseq-aws-diy-transcriptomics pipeline","text":"<ul> <li>MULTIQC Report</li> <li>Nextflow Report</li> <li>Nextflow Timeline Report</li> </ul>"},{"location":"bioinformatics/nextflow-for-rnaseq-aws/#pipeline-3-rnaseq-aws-diy-kallisto-pipeline","title":"PIPELINE 3: rnaseq-aws-diy-kallisto pipeline","text":"<ul> <li>MULTIQC Report</li> <li>Nextflow Report</li> <li>Nextflow Timeline Report</li> </ul> <p>Feel free to explore the code and run the pipeline directly from the repository \ud83d\ude4c.</p>"},{"location":"machine-learning/classification-splice-junction/","title":"Classification of DNA Splice Junctions","text":"<p>by Salomon Marquez</p> <p>25/06/2025</p> <p>Splice junctions are points within a DNA sequence where \u201csuperfluous\u201d DNA is removed during the protein synthesis process in higher organisms.</p> <p>The goal of this project is to identify, given a DNA sequence, the boundaries between exons (the parts of the DNA sequence retained after splicing) and introns (the parts of the DNA sequence that are cut out). In the biological community, exon-intron (EI) boundaries are referred to as acceptors, while intron-exon (IE) boundaries are known as donors.</p> <p>To predict the type of splicing site, several machine learning algorithms will be implemented, including: k-Nearest Neighbour, Naive Bayes, Artificial Neural Network, Support Vector Machine, Decision Tree, and Random Forest. Finally, the following metrics will be evaluated: precision, recall, f1-score, and error, in order to determine which algorithms performed best for this task.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Conv1D, Conv1DTranspose, Flatten, Dense, Reshape, Dropout\n\nfrom IPython.display import Image\n</pre> import numpy as np import pandas as pd import os import random import matplotlib.pyplot as plt import seaborn as sns  from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import classification_report from sklearn.metrics import roc_curve, roc_auc_score from sklearn.naive_bayes import BernoulliNB from sklearn.naive_bayes import GaussianNB from sklearn.naive_bayes import MultinomialNB from sklearn.naive_bayes import CategoricalNB from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import RandomForestClassifier  import tensorflow as tf from tensorflow import keras from tensorflow.keras.models import Model, Sequential from tensorflow.keras.layers import Input, Conv1D, Conv1DTranspose, Flatten, Dense, Reshape, Dropout  from IPython.display import Image In\u00a0[\u00a0]: Copied! <pre># Fijar semilla para obtener resultados reproducibles cuando se ejecuta el notebook\n# Fijar la semilla\nseed_value = 123\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)\n</pre> # Fijar semilla para obtener resultados reproducibles cuando se ejecuta el notebook # Fijar la semilla seed_value = 123 os.environ['PYTHONHASHSEED'] = str(seed_value) random.seed(seed_value) np.random.seed(seed_value) tf.random.set_seed(seed_value) In\u00a0[\u00a0]: Copied! <pre># Obtener el directorio actual\nactual_wd = os.getcwd()\nprint(\"Ruta actual:\", actual_wd)\n\n# Establecer directorio de trabajo en Gdrive\n# os.chdir(\"/content/drive/MyDrive/ASIGNATURAS/M0.163 MACHINE LEARNING/[28 MAY - 17 JUN] RETO 4/PEC4\")\n</pre> # Obtener el directorio actual actual_wd = os.getcwd() print(\"Ruta actual:\", actual_wd)  # Establecer directorio de trabajo en Gdrive # os.chdir(\"/content/drive/MyDrive/ASIGNATURAS/M0.163 MACHINE LEARNING/[28 MAY - 17 JUN] RETO 4/PEC4\") In\u00a0[\u00a0]: Copied! <pre># Visualizar contenido del directorio de trabajo\n!ls\n</pre> # Visualizar contenido del directorio de trabajo !ls <pre>autoencoder_image.png\t enunciado_PEC4_2425_2.pdf    splice.csv\ndeep_descriptors.csv\t PEC4_Machine_Learning.html\ndeep_descriptors.gsheet  PEC4_Machine_Learning.ipynb\n</pre> In\u00a0[\u00a0]: Copied! <pre># Especificar el nombre del archivo de origen\nfile_name = \"splice.csv\"\nfile_path = os.path.join(actual_wd, file_name)\n</pre> # Especificar el nombre del archivo de origen file_name = \"splice.csv\" file_path = os.path.join(actual_wd, file_name) In\u00a0[\u00a0]: Copied! <pre># Guardar en un dataframe el contenido de splice.csv\ndf_splice = pd.read_csv(file_path, delimiter=',')\n</pre> # Guardar en un dataframe el contenido de splice.csv df_splice = pd.read_csv(file_path, delimiter=',') In\u00a0[\u00a0]: Copied! <pre># Visualizar contenido del dataframe splice\ndf_splice.info()\n</pre> # Visualizar contenido del dataframe splice df_splice.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3190 entries, 0 to 3189\nColumns: 482 entries, class to V480\ndtypes: int64(480), object(2)\nmemory usage: 11.7+ MB\n</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar las 5 primeras filas\ndf_splice.head(5)\n</pre> # Visualizar las 5 primeras filas df_splice.head(5) Out[\u00a0]: class seq_name V1 V2 V3 V4 V5 V6 V7 V8 ... V471 V472 V473 V474 V475 V476 V477 V478 V479 V480 0 EI ATRINS-DONOR-521 0 0 0 1 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 1 EI ATRINS-DONOR-905 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 2 EI BABAPOE-DONOR-30 0 1 0 0 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 3 EI BABAPOE-DONOR-867 0 1 0 0 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 4 EI BABAPOE-DONOR-2817 0 1 0 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 <p>5 rows \u00d7 482 columns</p> In\u00a0[\u00a0]: Copied! <pre># Visualizar las 5 \u00faltimas filas\ndf_splice.tail(5)\n</pre> # Visualizar las 5 \u00faltimas filas df_splice.tail(5) Out[\u00a0]: class seq_name V1 V2 V3 V4 V5 V6 V7 V8 ... V471 V472 V473 V474 V475 V476 V477 V478 V479 V480 3185 N ORAHBPSBD-NEG-2881 0 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 3186 N ORAINVOL-NEG-2161 0 1 0 0 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 3187 N ORARGIT-NEG-241 0 0 1 0 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 3188 N TARHBB-NEG-541 1 0 0 0 0 0 0 0 ... 0 0 1 0 0 0 0 0 0 0 3189 N TARHBD-NEG-1981 1 0 0 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 <p>5 rows \u00d7 482 columns</p> In\u00a0[\u00a0]: Copied! <pre># Obtener el n\u00famero de secuencias \u00fanicas\nn_unique = df_splice['seq_name'].nunique()\nn_unique\n</pre> # Obtener el n\u00famero de secuencias \u00fanicas n_unique = df_splice['seq_name'].nunique() n_unique Out[\u00a0]: <pre>3178</pre> In\u00a0[\u00a0]: Copied! <pre># Calcular el n\u00famero de secuencias repetidas\nprint(f\"El dataset contiene {len(df_splice)-n_unique} secuencia repetidas\")\n</pre> # Calcular el n\u00famero de secuencias repetidas print(f\"El dataset contiene {len(df_splice)-n_unique} secuencia repetidas\") <pre>El dataset contiene 12 secuencia repetidas\n</pre> In\u00a0[\u00a0]: Copied! <pre># Alternativamente para calcular duplicados\ndf_splice.duplicated().sum()\n</pre> # Alternativamente para calcular duplicados df_splice.duplicated().sum() Out[\u00a0]: <pre>np.int64(12)</pre> In\u00a0[\u00a0]: Copied! <pre># Identificar las secuencias repetidas\ndf_splice_unique = df_splice['seq_name'].value_counts()\ndf_splice_unique.head(15)\n</pre> # Identificar las secuencias repetidas df_splice_unique = df_splice['seq_name'].value_counts() df_splice_unique.head(15) Out[\u00a0]: count seq_name HUMMYC3L-ACCEPTOR-4242 2 HUMALBGC-DONOR-17044 2 HUMMYLCA-DONOR-2559 2 HUMMYLCA-DONOR-2388 2 HUMMYLCA-DONOR-1975 2 HUMMYLCA-DONOR-952 2 HUMALBGC-ACCEPTOR-18496 2 HUMMYLCA-ACCEPTOR-924 2 HUMMYLCA-ACCEPTOR-1831 2 HUMMYLCA-ACCEPTOR-2214 2 HUMMYLCA-ACCEPTOR-2481 2 HUMMYLCA-DONOR-644 2 HUMGAPJR-NEG-961 1 HUMGBR-NEG-2521 1 HUMGALAB-NEG-901 1 dtype: int64 In\u00a0[\u00a0]: Copied! <pre># Visualizar un par de secuencias repetidas\ndf_splice[df_splice['seq_name'].str.contains('HUMMYC3L-ACCEPTOR-4242')]\n</pre> # Visualizar un par de secuencias repetidas df_splice[df_splice['seq_name'].str.contains('HUMMYC3L-ACCEPTOR-4242')] Out[\u00a0]: class seq_name V1 V2 V3 V4 V5 V6 V7 V8 ... V471 V472 V473 V474 V475 V476 V477 V478 V479 V480 1316 IE HUMMYC3L-ACCEPTOR-4242 0 0 1 0 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 1317 IE HUMMYC3L-ACCEPTOR-4242 0 0 1 0 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 <p>2 rows \u00d7 482 columns</p> In\u00a0[\u00a0]: Copied! <pre># Eliminar duplicados\ndf_splice_n = df_splice.drop_duplicates()\nprint(f\"El dataset originalmente conten\u00eda {len(df_splice)} registros y despu\u00e9s de eliminar duplicados se tienen {len(df_splice_n)} registros \")\n</pre> # Eliminar duplicados df_splice_n = df_splice.drop_duplicates() print(f\"El dataset originalmente conten\u00eda {len(df_splice)} registros y despu\u00e9s de eliminar duplicados se tienen {len(df_splice_n)} registros \") <pre>El dataset originalmente conten\u00eda 3190 registros y despu\u00e9s de eliminar duplicados se tienen 3178 registros \n</pre> In\u00a0[\u00a0]: Copied! <pre>df_splice_label = df_splice_n['class'].value_counts()\ndf_splice_label\n</pre> df_splice_label = df_splice_n['class'].value_counts() df_splice_label Out[\u00a0]: count class N 1655 IE 762 EI 761 dtype: int64 <p>We have an imbalanced dataset where slightly more than 50% of the sequences belong to the \u201cno splicing\u201d category. This must be taken into account because the classification models proposed below could become biased toward predicting \"N.\"</p> In\u00a0[\u00a0]: Copied! <pre># Seleccionar desde V1 hasta V480\nseq_encoded = df_splice_n.iloc[:,2:]\n</pre> # Seleccionar desde V1 hasta V480 seq_encoded = df_splice_n.iloc[:,2:] In\u00a0[\u00a0]: Copied! <pre># Verificar el tipo de variable\ntype(seq_encoded)\n</pre> # Verificar el tipo de variable type(seq_encoded) Out[\u00a0]: <pre>pandas.core.frame.DataFramedef __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre>/usr/local/lib/python3.11/dist-packages/pandas/core/frame.pyTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n&gt;&gt;&gt; d = {'col1': [1, 2], 'col2': [3, 4]}\n&gt;&gt;&gt; df = pd.DataFrame(data=d)\n&gt;&gt;&gt; df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n&gt;&gt;&gt; df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n&gt;&gt;&gt; df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n&gt;&gt;&gt; d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n&gt;&gt;&gt; df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n&gt;&gt;&gt; df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n&gt;&gt;&gt; from dataclasses import make_dataclass\n&gt;&gt;&gt; Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df\n   0\na  1\nc  3\n\n&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df2\n   x\na  1\nc  3</pre> In\u00a0[\u00a0]: Copied! <pre># Convertir datos a numpy.array\nseq_encoded_array = seq_encoded.to_numpy()\nseq_encoded_array[:5]\n</pre> # Convertir datos a numpy.array seq_encoded_array = seq_encoded.to_numpy() seq_encoded_array[:5] Out[\u00a0]: <pre>array([[0, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0],\n       [0, 1, 0, ..., 0, 0, 0],\n       [0, 1, 0, ..., 0, 0, 0],\n       [0, 1, 0, ..., 0, 0, 0]])</pre> In\u00a0[\u00a0]: Copied! <pre>labels = df_splice_n['class'].map({'EI': 0, 'IE': 1, 'N': 2})\n</pre> labels = df_splice_n['class'].map({'EI': 0, 'IE': 1, 'N': 2}) In\u00a0[\u00a0]: Copied! <pre># Convertir datos a numpy.array\nlabels_array = labels.to_numpy()\nlabels_array[:5]\n</pre> # Convertir datos a numpy.array labels_array = labels.to_numpy() labels_array[:5] Out[\u00a0]: <pre>array([0, 0, 0, 0, 0])</pre> In\u00a0[\u00a0]: Copied! <pre># Comprobar que las variables y los labels para entrenar los modelos de clasificaci\u00f3n sean tipo array\nprint(f\"La secuencias codificadas por one-hot encoding son de tipo {type(seq_encoded_array)} con dimensi\u00f3n {seq_encoded_array.shape}\\n\"\n      f\"y la variable label es de tipo {type(labels_array)} con dimensi\u00f3n {labels_array.shape}\")\n</pre> # Comprobar que las variables y los labels para entrenar los modelos de clasificaci\u00f3n sean tipo array print(f\"La secuencias codificadas por one-hot encoding son de tipo {type(seq_encoded_array)} con dimensi\u00f3n {seq_encoded_array.shape}\\n\"       f\"y la variable label es de tipo {type(labels_array)} con dimensi\u00f3n {labels_array.shape}\") <pre>La secuencias codificadas por one-hot encoding son de tipo &lt;class 'numpy.ndarray'&gt; con dimensi\u00f3n (3178, 480)\ny la variable label es de tipo &lt;class 'numpy.ndarray'&gt; con dimensi\u00f3n (3178,)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Crear train y test datasets\nX_train, X_test, y_train, y_test = train_test_split(\n    seq_encoded_array,\n    labels_array,\n    test_size = 0.33, # 33% para el dataset test como lo indica el enunciado de la PEC4\n    random_state= 123 # Fijamos la semilla aleatoria en 123\n)\n</pre> # Crear train y test datasets X_train, X_test, y_train, y_test = train_test_split(     seq_encoded_array,     labels_array,     test_size = 0.33, # 33% para el dataset test como lo indica el enunciado de la PEC4     random_state= 123 # Fijamos la semilla aleatoria en 123 ) In\u00a0[\u00a0]: Copied! <pre># Visualizar la dimensiones de los 4 datases creados a partir de train_test_split()\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n</pre> # Visualizar la dimensiones de los 4 datases creados a partir de train_test_split() X_train.shape, y_train.shape, X_test.shape, y_test.shape Out[\u00a0]: <pre>((2129, 480), (2129,), (1049, 480), (1049,))</pre> In\u00a0[\u00a0]: Copied! <pre>Image(\"autoencoder_image.png\",width=600, height=400)\n</pre> Image(\"autoencoder_image.png\",width=600, height=400) Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre># Reajustar las dimensiones de X_train puesto que Conv1D requiere datos en formato 3D\nm, n   = X_train.shape # Dimensiones del dataset train\np, r   = X_test.shape  # Dimensiones del dataset test\n\nX_train_reshaped = X_train.reshape((m, 60, 8))\nX_test_reshaped = X_test.reshape((p, 60, 8))\n</pre> # Reajustar las dimensiones de X_train puesto que Conv1D requiere datos en formato 3D m, n   = X_train.shape # Dimensiones del dataset train p, r   = X_test.shape  # Dimensiones del dataset test  X_train_reshaped = X_train.reshape((m, 60, 8)) X_test_reshaped = X_test.reshape((p, 60, 8)) In\u00a0[\u00a0]: Copied! <pre># AUTOENCODER\n# Definir capa de entrada\ninput_layer = Input(shape=(60, 8))\n\n# Definir encoder\nx           = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(input_layer)\nbottleneck  = Flatten()(x)\n#bottleneck  = Dense(32, activation='relu')(x)\n\n# Definir decoder\n#x             = Dense(60 * 8, activation='relu')(bottleneck)\nx             = Reshape((60, 8))(bottleneck)\noutput_layer  = Conv1DTranspose(filters=8, kernel_size=3, activation='sigmoid', padding='same')(x)\n\n# Definir modelo\nautoencoder = Model(inputs=input_layer, outputs=output_layer)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Entrenar autoencoder\nhistory = autoencoder.fit(X_train_reshaped, X_train_reshaped, epochs=20, batch_size=32, validation_split=0.2)\n</pre> # AUTOENCODER # Definir capa de entrada input_layer = Input(shape=(60, 8))  # Definir encoder x           = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(input_layer) bottleneck  = Flatten()(x) #bottleneck  = Dense(32, activation='relu')(x)  # Definir decoder #x             = Dense(60 * 8, activation='relu')(bottleneck) x             = Reshape((60, 8))(bottleneck) output_layer  = Conv1DTranspose(filters=8, kernel_size=3, activation='sigmoid', padding='same')(x)  # Definir modelo autoencoder = Model(inputs=input_layer, outputs=output_layer) autoencoder.compile(optimizer='adam', loss='binary_crossentropy')  # Entrenar autoencoder history = autoencoder.fit(X_train_reshaped, X_train_reshaped, epochs=20, batch_size=32, validation_split=0.2) <pre>Epoch 1/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 13ms/step - loss: 0.7044 - val_loss: 0.6261\nEpoch 2/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 10ms/step - loss: 0.5937 - val_loss: 0.4684\nEpoch 3/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.4215 - val_loss: 0.3057\nEpoch 4/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - loss: 0.2815 - val_loss: 0.2242\nEpoch 5/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.2105 - val_loss: 0.1758\nEpoch 6/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.1660 - val_loss: 0.1404\nEpoch 7/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - loss: 0.1325 - val_loss: 0.1122\nEpoch 8/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.1057 - val_loss: 0.0897\nEpoch 9/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0842 - val_loss: 0.0718\nEpoch 10/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0673 - val_loss: 0.0577\nEpoch 11/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0539 - val_loss: 0.0466\nEpoch 12/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0435 - val_loss: 0.0380\nEpoch 13/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0354 - val_loss: 0.0313\nEpoch 14/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - loss: 0.0291 - val_loss: 0.0261\nEpoch 15/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0241 - val_loss: 0.0220\nEpoch 16/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0202 - val_loss: 0.0187\nEpoch 17/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0172 - val_loss: 0.0162\nEpoch 18/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0147 - val_loss: 0.0141\nEpoch 19/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0127 - val_loss: 0.0124\nEpoch 20/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0111 - val_loss: 0.0110\n</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluar la p\u00e9rdida de reconstrucci\u00f3n del autoencoder\ntrain_loss  = autoencoder.evaluate(X_train_reshaped, X_train_reshaped)\ntest_loss   = autoencoder.evaluate(X_test_reshaped, X_test_reshaped)\n\nprint(f\"\\nTrain loss: {train_loss:.4f}\")\nprint(f\"Test loss: {test_loss:.4f}\")\n</pre> # Evaluar la p\u00e9rdida de reconstrucci\u00f3n del autoencoder train_loss  = autoencoder.evaluate(X_train_reshaped, X_train_reshaped) test_loss   = autoencoder.evaluate(X_test_reshaped, X_test_reshaped)  print(f\"\\nTrain loss: {train_loss:.4f}\") print(f\"Test loss: {test_loss:.4f}\") <pre>67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 0.0101\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 0.0101\n\nTrain loss: 0.0103\nTest loss: 0.0101\n</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar la curva de entrenamiento del autoencoder\nplt.plot(history.history['loss'], label='Train loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('P\u00e9rdida de entrenamiento vs validaci\u00f3n')\nplt.show()\n</pre> # Visualizar la curva de entrenamiento del autoencoder plt.plot(history.history['loss'], label='Train loss') plt.plot(history.history['val_loss'], label='Validation loss') plt.legend() plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('P\u00e9rdida de entrenamiento vs validaci\u00f3n') plt.show() <p>We observe that the autoencoder quality is good since the reconstruction loss is very low.</p> In\u00a0[\u00a0]: Copied! <pre># Extraer la parte codificadora \"encoder\" para usarla en otros modelos\nencoder = Model(inputs=input_layer, outputs=bottleneck)\nX_train_encoded = encoder.predict(X_train_reshaped)\n</pre> # Extraer la parte codificadora \"encoder\" para usarla en otros modelos encoder = Model(inputs=input_layer, outputs=bottleneck) X_train_encoded = encoder.predict(X_train_reshaped) <pre>67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step\n</pre> In\u00a0[\u00a0]: Copied! <pre># Verificar las dimensiones de las nuevas coordenadas de X_train_encoded\nX_train_encoded.shape, X_train.shape\n</pre> # Verificar las dimensiones de las nuevas coordenadas de X_train_encoded X_train_encoded.shape, X_train.shape Out[\u00a0]: <pre>((2129, 480), (2129, 480))</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar X_train_encoded\nprint(X_train_encoded)\n</pre> # Visualizar X_train_encoded print(X_train_encoded) <pre>[[0.41865867 1.1792793  0.         ... 1.0533024  1.2378863  1.4537132 ]\n [0.97194517 1.2243328  0.7337566  ... 1.6940844  0.6968196  2.2641408 ]\n [0.7622293  0.27394313 1.5169445  ... 1.1491497  1.7781353  0.6089502 ]\n ...\n [0.         0.19901627 1.6442008  ... 0.92114854 1.2606764  0.        ]\n [0.55828255 1.0124682  0.6177381  ... 1.4002684  0.5707153  1.0705616 ]\n [0.18931112 0.25097656 1.0731099  ... 2.1333911  0.10522419 1.4503322 ]]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener las nuevas coordenadas de X_test\nX_test_encoded = encoder.predict(X_test_reshaped)\n</pre> # Obtener las nuevas coordenadas de X_test X_test_encoded = encoder.predict(X_test_reshaped) <pre>33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step \n</pre> In\u00a0[\u00a0]: Copied! <pre># Verificar las dimensiones de las nuevas coordenadas de X_test_encoded\nX_test_encoded.shape, X_test.shape\n</pre> # Verificar las dimensiones de las nuevas coordenadas de X_test_encoded X_test_encoded.shape, X_test.shape Out[\u00a0]: <pre>((1049, 480), (1049, 480))</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar X_test_encoded\nprint(X_test_encoded)\n</pre> # Visualizar X_test_encoded print(X_test_encoded) <pre>[[0.18931112 0.25097656 1.0731099  ... 0.8388007  1.4651122  1.0704793 ]\n [0.55828255 1.0124682  0.6177381  ... 1.4002684  0.5707153  1.0705616 ]\n [0.55828255 1.0124682  0.6177381  ... 0.8388007  1.4651122  1.0704793 ]\n ...\n [0.34228757 1.3086706  0.33683705 ... 1.0533024  1.2378863  1.4537132 ]\n [1.4116294  0.32590342 0.94585365 ... 1.0568092  1.7025597  1.0396074 ]\n [0.16956863 0.1146785  2.0411205  ... 1.7764323  0.49238378 1.1906232 ]]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Crear modelo k-nn con k = 3\nk     = 3   # Valor del nearest neighbor k = [1, 3, 5, 7]\nmodel = KNeighborsClassifier(n_neighbors=k, metric='euclidean') # Distancia euclidiana\n\n# Entrenar modelo k-nn\nmodel.fit(X_train_encoded, y_train)\n\n# Hacer predicciones con el modelo k-nn entrenado usando X_test\ny_pred = model.predict(X_test_encoded)\n\n# Imprimir reporte de clasificaci\u00f3n\n# Recordar que {'EI': 0, 'IE': 1, 'N': 2}\nprint(classification_report(y_test, y_pred, target_names=['Clase EI', 'Clase IE', 'Clase N']))\n</pre> # Crear modelo k-nn con k = 3 k     = 3   # Valor del nearest neighbor k = [1, 3, 5, 7] model = KNeighborsClassifier(n_neighbors=k, metric='euclidean') # Distancia euclidiana  # Entrenar modelo k-nn model.fit(X_train_encoded, y_train)  # Hacer predicciones con el modelo k-nn entrenado usando X_test y_pred = model.predict(X_test_encoded)  # Imprimir reporte de clasificaci\u00f3n # Recordar que {'EI': 0, 'IE': 1, 'N': 2} print(classification_report(y_test, y_pred, target_names=['Clase EI', 'Clase IE', 'Clase N'])) <pre>              precision    recall  f1-score   support\n\n    Clase EI       0.61      0.93      0.74       243\n    Clase IE       0.78      0.87      0.82       247\n     Clase N       0.96      0.70      0.81       559\n\n    accuracy                           0.79      1049\n   macro avg       0.78      0.83      0.79      1049\nweighted avg       0.83      0.79      0.79      1049\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener un dataframe a partir de la matriz de confusion\nreport             = classification_report(y_test, y_pred, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report          = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\ndf_report['error'] = 1 - df_report['recall']\ndf_report['k']     = k\ndf_report['modelo'] = \"kNN\"\ndf_report.reset_index(inplace=True)\ndf_report.rename(columns={'index': 'clase'}, inplace=True)\ndf_report\n</pre> # Obtener un dataframe a partir de la matriz de confusion report             = classification_report(y_test, y_pred, output_dict=True, target_names=['EI', 'IE', 'N']) df_report          = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']] df_report['error'] = 1 - df_report['recall'] df_report['k']     = k df_report['modelo'] = \"kNN\" df_report.reset_index(inplace=True) df_report.rename(columns={'index': 'clase'}, inplace=True) df_report Out[\u00a0]: clase precision recall f1-score support error k modelo 0 EI 0.614754 0.925926 0.738916 243.0 0.074074 3 kNN 1 IE 0.775362 0.866397 0.818356 247.0 0.133603 3 kNN 2 N 0.955774 0.695886 0.805383 559.0 0.304114 3 kNN In\u00a0[\u00a0]: Copied! <pre># Definir funci\u00f3n evaluar_knn_por_k()\ndef evaluar_knn_por_k(k, X_train, y_train, X_test, y_test):\n    \"\"\"\n    Entrena y eval\u00faa un modelo k-NN con valor k dado.\n    Retorna un DataFrame con precision, recall, f1-score, error por clase y k.\n    \"\"\"\n    # Entrenar modelo\n    model = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n    model.fit(X_train, y_train)\n\n    # Predicci\u00f3n\n    y_pred = model.predict(X_test)\n\n    # Clasification report por clase\n    report = classification_report(\n        y_test,\n        y_pred,\n        output_dict=True,\n        target_names=['EI', 'IE', 'N']\n    )\n\n    # Convertir a DataFrame\n    df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error'] = 1 - df_report['recall']\n    df_report['modelo'] = 'kNN' + ' k=' + str(k)\n    df_report.reset_index(inplace=True)\n    df_report.rename(columns={'index': 'clase'}, inplace=True)\n\n    return df_report\n</pre> # Definir funci\u00f3n evaluar_knn_por_k() def evaluar_knn_por_k(k, X_train, y_train, X_test, y_test):     \"\"\"     Entrena y eval\u00faa un modelo k-NN con valor k dado.     Retorna un DataFrame con precision, recall, f1-score, error por clase y k.     \"\"\"     # Entrenar modelo     model = KNeighborsClassifier(n_neighbors=k, metric='euclidean')     model.fit(X_train, y_train)      # Predicci\u00f3n     y_pred = model.predict(X_test)      # Clasification report por clase     report = classification_report(         y_test,         y_pred,         output_dict=True,         target_names=['EI', 'IE', 'N']     )      # Convertir a DataFrame     df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error'] = 1 - df_report['recall']     df_report['modelo'] = 'kNN' + ' k=' + str(k)     df_report.reset_index(inplace=True)     df_report.rename(columns={'index': 'clase'}, inplace=True)      return df_report In\u00a0[\u00a0]: Copied! <pre># Probar con diferentes valores de k\nresultados = []\n\nfor k in [1, 3, 5, 7]:\n    df_k = evaluar_knn_por_k(k, X_train_encoded, y_train, X_test_encoded, y_test)\n    resultados.append(df_k)\n\n# Unir todos los resultados\ndf_knn_final = pd.concat(resultados, ignore_index=True)\ndf_knn_final\n</pre> # Probar con diferentes valores de k resultados = []  for k in [1, 3, 5, 7]:     df_k = evaluar_knn_por_k(k, X_train_encoded, y_train, X_test_encoded, y_test)     resultados.append(df_k)  # Unir todos los resultados df_knn_final = pd.concat(resultados, ignore_index=True) df_knn_final Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.638554 0.872428 0.737391 243.0 0.127572 kNN k=1 1 IE 0.667722 0.854251 0.749556 247.0 0.145749 kNN k=1 2 N 0.922693 0.661896 0.770833 559.0 0.338104 kNN k=1 3 EI 0.614754 0.925926 0.738916 243.0 0.074074 kNN k=3 4 IE 0.775362 0.866397 0.818356 247.0 0.133603 kNN k=3 5 N 0.955774 0.695886 0.805383 559.0 0.304114 kNN k=3 6 EI 0.684524 0.946502 0.794473 243.0 0.053498 kNN k=5 7 IE 0.751656 0.919028 0.826958 247.0 0.080972 kNN k=5 8 N 0.975669 0.717352 0.826804 559.0 0.282648 kNN k=5 9 EI 0.707317 0.954733 0.812609 243.0 0.045267 kNN k=7 10 IE 0.759868 0.935223 0.838475 247.0 0.064777 kNN k=7 11 N 0.980815 0.731664 0.838115 559.0 0.268336 kNN k=7 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_knn_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_knn_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_knn_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\n### 4.1 Implementar el modelo kNN para k = 1, 3, 5 y 7\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_knn_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() ### 4.1 Implementar el modelo kNN para k = 1, 3, 5 y 7 plt.show() In\u00a0[\u00a0]: Copied! <pre># Entrenar BernoulliNB sin suavizado (alpha=0) con datos one-hot encoded\nmodelNB_alpha0 = BernoulliNB(alpha=0.0)\nmodelNB_alpha0.fit(X_train, y_train)\n\n# Predecir\ny_pred_NB_alpha0 = modelNB_alpha0.predict(X_test)\nprint(\"Predicciones:\", y_pred_NB_alpha0)\n</pre> # Entrenar BernoulliNB sin suavizado (alpha=0) con datos one-hot encoded modelNB_alpha0 = BernoulliNB(alpha=0.0) modelNB_alpha0.fit(X_train, y_train)  # Predecir y_pred_NB_alpha0 = modelNB_alpha0.predict(X_test) print(\"Predicciones:\", y_pred_NB_alpha0) <pre>Predicciones: [0 0 0 ... 0 0 0]\n</pre> <pre>/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py:1209: RuntimeWarning: divide by zero encountered in log\n  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n  ret = a @ b\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener un dataframe a partir de la matriz de confusion\nreport_NB_alpha0              = classification_report(y_test, y_pred_NB_alpha0, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report_NB_alpha0           = pd.DataFrame(report_NB_alpha0 ).T.loc[['EI', 'IE', 'N']]\ndf_report_NB_alpha0 ['error'] = 1 - df_report_NB_alpha0 ['recall']\ndf_report_NB_alpha0 ['modelo']  = \"Bernoulli B alpha=0\"\ndf_report_NB_alpha0\n</pre> # Obtener un dataframe a partir de la matriz de confusion report_NB_alpha0              = classification_report(y_test, y_pred_NB_alpha0, output_dict=True, target_names=['EI', 'IE', 'N']) df_report_NB_alpha0           = pd.DataFrame(report_NB_alpha0 ).T.loc[['EI', 'IE', 'N']] df_report_NB_alpha0 ['error'] = 1 - df_report_NB_alpha0 ['recall'] df_report_NB_alpha0 ['modelo']  = \"Bernoulli B alpha=0\" df_report_NB_alpha0 <pre>/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n</pre> Out[\u00a0]: precision recall f1-score support error modelo EI 0.231649 1.0 0.376161 243.0 0.0 Bernoulli B alpha=0 IE 0.000000 0.0 0.000000 247.0 1.0 Bernoulli B alpha=0 N 0.000000 0.0 0.000000 559.0 1.0 Bernoulli B alpha=0 In\u00a0[\u00a0]: Copied! <pre># Entrenar BernoulliNB con suavizado (alpha=1) con datos one-hot encoded\nmodelNB_alpha1 = BernoulliNB(alpha=1.0)\nmodelNB_alpha1.fit(X_train, y_train)\n\n# Predecir\ny_pred_NB_alpha1 = modelNB_alpha0.predict(X_test)\nprint(\"Predicciones:\", y_pred_NB_alpha1)\n</pre> # Entrenar BernoulliNB con suavizado (alpha=1) con datos one-hot encoded modelNB_alpha1 = BernoulliNB(alpha=1.0) modelNB_alpha1.fit(X_train, y_train)  # Predecir y_pred_NB_alpha1 = modelNB_alpha0.predict(X_test) print(\"Predicciones:\", y_pred_NB_alpha1) <pre>Predicciones: [0 0 0 ... 0 0 0]\n</pre> <pre>/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n  ret = a @ b\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener un dataframe a partir de la matriz de confusion\nreport_NB_alpha1             = classification_report(y_test, y_pred_NB_alpha1, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report_NB_alpha1          = pd.DataFrame(report_NB_alpha1).T.loc[['EI', 'IE', 'N']]\ndf_report_NB_alpha1['error'] = 1 - df_report_NB_alpha1['recall']\ndf_report_NB_alpha1['modelo']  = \"BernoulliNB alpha=1\"\ndf_report_NB_alpha1\n</pre> # Obtener un dataframe a partir de la matriz de confusion report_NB_alpha1             = classification_report(y_test, y_pred_NB_alpha1, output_dict=True, target_names=['EI', 'IE', 'N']) df_report_NB_alpha1          = pd.DataFrame(report_NB_alpha1).T.loc[['EI', 'IE', 'N']] df_report_NB_alpha1['error'] = 1 - df_report_NB_alpha1['recall'] df_report_NB_alpha1['modelo']  = \"BernoulliNB alpha=1\" df_report_NB_alpha1 <pre>/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n</pre> Out[\u00a0]: precision recall f1-score support error modelo EI 0.231649 1.0 0.376161 243.0 0.0 BernoulliNB alpha=1 IE 0.000000 0.0 0.000000 247.0 1.0 BernoulliNB alpha=1 N 0.000000 0.0 0.000000 559.0 1.0 BernoulliNB alpha=1 In\u00a0[\u00a0]: Copied! <pre># Entrenar GaussianNB con datos del autoencoder\nmodelNB = GaussianNB()\nmodelNB.fit(X_train_encoded, y_train)\n\n# Predecir\ny_pred_NB = modelNB.predict(X_test_encoded)\nprint(\"Predicciones:\", y_pred_NB)\n</pre> # Entrenar GaussianNB con datos del autoencoder modelNB = GaussianNB() modelNB.fit(X_train_encoded, y_train)  # Predecir y_pred_NB = modelNB.predict(X_test_encoded) print(\"Predicciones:\", y_pred_NB) <pre>Predicciones: [1 1 2 ... 0 2 1]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener un dataframe a partir de la matriz de confusion\nreport_NB             = classification_report(y_test, y_pred_NB, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report_NB          = pd.DataFrame(report_NB).T.loc[['EI', 'IE', 'N']]\ndf_report_NB['error'] = 1 - df_report_NB['recall']\ndf_report_NB['modelo']  = \"GaussianNB\"\ndf_report_NB\n</pre> # Obtener un dataframe a partir de la matriz de confusion report_NB             = classification_report(y_test, y_pred_NB, output_dict=True, target_names=['EI', 'IE', 'N']) df_report_NB          = pd.DataFrame(report_NB).T.loc[['EI', 'IE', 'N']] df_report_NB['error'] = 1 - df_report_NB['recall'] df_report_NB['modelo']  = \"GaussianNB\" df_report_NB Out[\u00a0]: precision recall f1-score support error modelo EI 0.942623 0.946502 0.944559 243.0 0.053498 GaussianNB IE 0.917647 0.947368 0.932271 247.0 0.052632 GaussianNB N 0.983636 0.967800 0.975654 559.0 0.032200 GaussianNB In\u00a0[\u00a0]: Copied! <pre># Unir todos los resultados\ndf_naivebayes_final = pd.concat([df_report_NB_alpha0, df_report_NB_alpha1, df_report_NB]).reset_index().rename(columns={'index': 'clase'})\ndf_naivebayes_final\n</pre> # Unir todos los resultados df_naivebayes_final = pd.concat([df_report_NB_alpha0, df_report_NB_alpha1, df_report_NB]).reset_index().rename(columns={'index': 'clase'}) df_naivebayes_final Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.231649 1.000000 0.376161 243.0 0.000000 Bernoulli B alpha=0 1 IE 0.000000 0.000000 0.000000 247.0 1.000000 Bernoulli B alpha=0 2 N 0.000000 0.000000 0.000000 559.0 1.000000 Bernoulli B alpha=0 3 EI 0.231649 1.000000 0.376161 243.0 0.000000 BernoulliNB alpha=1 4 IE 0.000000 0.000000 0.000000 247.0 1.000000 BernoulliNB alpha=1 5 N 0.000000 0.000000 0.000000 559.0 1.000000 BernoulliNB alpha=1 6 EI 0.942623 0.946502 0.944559 243.0 0.053498 GaussianNB 7 IE 0.917647 0.947368 0.932271 247.0 0.052632 GaussianNB 8 N 0.983636 0.967800 0.975654 559.0 0.032200 GaussianNB In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_naivebayes_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_naivebayes_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_naivebayes_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_naivebayes_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Definir nodo\np = 20\n</pre> # Definir nodo p = 20 In\u00a0[\u00a0]: Copied! <pre># Definir la arquitectura de la ANN\nmodel = Sequential([\n    Input(shape=(X_train_encoded.shape[1],)),\n    Dense(100, activation='relu'),\n    Dense(p, activation='relu'),\n    Dense(3, activation='softmax') # Ya que se tienen m\u00e1s de 2 categor\u00edas\n    ])\n</pre> # Definir la arquitectura de la ANN model = Sequential([     Input(shape=(X_train_encoded.shape[1],)),     Dense(100, activation='relu'),     Dense(p, activation='relu'),     Dense(3, activation='softmax') # Ya que se tienen m\u00e1s de 2 categor\u00edas     ]) In\u00a0[\u00a0]: Copied! <pre># Ver detalles del modelo\nmodel.summary()\n</pre> # Ver detalles del modelo model.summary() <pre>Model: \"sequential_8\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 dense_24 (Dense)                \u2502 (None, 100)            \u2502        48,100 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_25 (Dense)                \u2502 (None, 20)             \u2502         2,020 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_26 (Dense)                \u2502 (None, 3)              \u2502            63 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 50,183 (196.03 KB)\n</pre> <pre> Trainable params: 50,183 (196.03 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Compilar modelo\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy', # Puesto que las categor\u00edas de y_train son enteras [0, 1, 2]\n              metrics=['accuracy'])\n</pre> # Compilar modelo model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy', # Puesto que las categor\u00edas de y_train son enteras [0, 1, 2]               metrics=['accuracy']) In\u00a0[\u00a0]: Copied! <pre># Definir \u00e9pocas, tama\u00f1o del batch y entrenar el modelo\nn_batch  = 32\nn_epochs = 20\n\nmfit = model.fit(X_train_encoded, y_train,\n          epochs=n_epochs,\n          batch_size=n_batch)\n</pre> # Definir \u00e9pocas, tama\u00f1o del batch y entrenar el modelo n_batch  = 32 n_epochs = 20  mfit = model.fit(X_train_encoded, y_train,           epochs=n_epochs,           batch_size=n_batch) <pre>Epoch 1/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 4ms/step - accuracy: 0.5665 - loss: 0.9898\nEpoch 2/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 4ms/step - accuracy: 0.8900 - loss: 0.3338\nEpoch 3/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - accuracy: 0.9415 - loss: 0.1934\nEpoch 4/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 4ms/step - accuracy: 0.9525 - loss: 0.1493\nEpoch 5/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step - accuracy: 0.9655 - loss: 0.1159\nEpoch 6/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9739 - loss: 0.0986\nEpoch 7/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9715 - loss: 0.0910\nEpoch 8/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9725 - loss: 0.0824\nEpoch 9/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9725 - loss: 0.0859\nEpoch 10/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9717 - loss: 0.0829\nEpoch 11/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9720 - loss: 0.0820\nEpoch 12/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9534 - loss: 0.1180\nEpoch 13/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9412 - loss: 0.1866\nEpoch 14/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9617 - loss: 0.1020\nEpoch 15/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9781 - loss: 0.0762\nEpoch 16/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9778 - loss: 0.0704\nEpoch 17/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9730 - loss: 0.0781\nEpoch 18/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9680 - loss: 0.0903\nEpoch 19/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9769 - loss: 0.0664\nEpoch 20/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9795 - loss: 0.0596\n</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluar modelo\nloss, acc     = model.evaluate(X_test_encoded, y_test)\ny_pred        = model.predict(X_test_encoded)\ny_pred_labels = y_pred.argmax(axis=1)\n</pre> # Evaluar modelo loss, acc     = model.evaluate(X_test_encoded, y_test) y_pred        = model.predict(X_test_encoded) y_pred_labels = y_pred.argmax(axis=1) <pre>33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8993 - loss: 0.2753\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step\n</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar las primeras predicciones\ny_pred_labels[:100]\n</pre> # Visualizar las primeras predicciones y_pred_labels[:100] Out[\u00a0]: <pre>array([1, 1, 2, 2, 2, 2, 2, 1, 0, 0, 1, 0, 0, 1, 2, 0, 2, 0, 2, 0, 2, 0,\n       1, 2, 2, 2, 0, 2, 2, 2, 0, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2,\n       2, 2, 2, 1, 2, 2, 0, 0, 2, 1, 2, 2, 2, 2, 1, 1, 2, 0, 1, 2, 0, 2,\n       2, 0, 2, 1, 2, 1, 1, 2, 1, 0, 2, 0, 1, 2, 2, 0, 2, 1, 2, 1, 1, 2,\n       2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 0, 1])</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar los truth labels\ny_test[:100]\n</pre> # Visualizar los truth labels y_test[:100] Out[\u00a0]: <pre>array([1, 1, 2, 2, 2, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0,\n       1, 2, 2, 2, 1, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 2, 1, 2, 1,\n       2, 2, 2, 1, 2, 2, 0, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 1, 2, 0, 2,\n       1, 0, 2, 1, 2, 1, 1, 2, 1, 0, 1, 0, 1, 2, 2, 0, 2, 1, 2, 1, 1, 2,\n       1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 0, 1])</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener reporte de clasificaci\u00f3n\nreport              = classification_report(y_test, y_pred_labels, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report           = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\ndf_report['error']  = 1 - df_report['recall']\ndf_report['modelo'] = 'ANN'\ndf_report['p']      = p\ndf_report\n</pre> # Obtener reporte de clasificaci\u00f3n report              = classification_report(y_test, y_pred_labels, output_dict=True, target_names=['EI', 'IE', 'N']) df_report           = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']] df_report['error']  = 1 - df_report['recall'] df_report['modelo'] = 'ANN' df_report['p']      = p df_report Out[\u00a0]: precision recall f1-score support error modelo p EI 0.932773 0.913580 0.923077 243.0 0.086420 ANN 20 IE 0.982659 0.688259 0.809524 247.0 0.311741 ANN 20 N 0.873041 0.996422 0.930660 559.0 0.003578 ANN 20 In\u00a0[\u00a0]: Copied! <pre># Validar el modelo ANN con distintos valores de p = [5, 10, 20]\n\n# Lista para guardar resultados\nresultados_ann = []\n\nfor p in [5, 10, 20]:\n    print(f\"\\nEntrenando red con p = {p} nodos en segunda capa oculta...\")\n\n    # Definir modelo\n    model = Sequential([\n        Input(shape=(X_train_encoded.shape[1],)),\n        Dense(100, activation='relu'),\n        Dense(p, activation='relu'),\n        Dense(3, activation='softmax')  # 3 clases\n    ])\n\n    # Compilar\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    # Entrenar\n    model.fit(X_train_encoded, y_train, epochs=20, batch_size=32)\n\n    # Evaluar\n    loss, acc = model.evaluate(X_test_encoded, y_test)\n    y_pred = model.predict(X_test_encoded)\n    y_pred_labels = y_pred.argmax(axis=1)\n\n    # Reporte de clasificaci\u00f3n\n    report = classification_report(y_test, y_pred_labels, output_dict=True, target_names=['EI', 'IE', 'N'])\n    df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error'] = 1 - df_report['recall']\n    df_report['modelo'] = 'ANN' + ' p=' + str(p)\n    resultados_ann.append(df_report)\n</pre> # Validar el modelo ANN con distintos valores de p = [5, 10, 20]  # Lista para guardar resultados resultados_ann = []  for p in [5, 10, 20]:     print(f\"\\nEntrenando red con p = {p} nodos en segunda capa oculta...\")      # Definir modelo     model = Sequential([         Input(shape=(X_train_encoded.shape[1],)),         Dense(100, activation='relu'),         Dense(p, activation='relu'),         Dense(3, activation='softmax')  # 3 clases     ])      # Compilar     model.compile(optimizer='adam',                   loss='sparse_categorical_crossentropy',                   metrics=['accuracy'])      # Entrenar     model.fit(X_train_encoded, y_train, epochs=20, batch_size=32)      # Evaluar     loss, acc = model.evaluate(X_test_encoded, y_test)     y_pred = model.predict(X_test_encoded)     y_pred_labels = y_pred.argmax(axis=1)      # Reporte de clasificaci\u00f3n     report = classification_report(y_test, y_pred_labels, output_dict=True, target_names=['EI', 'IE', 'N'])     df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error'] = 1 - df_report['recall']     df_report['modelo'] = 'ANN' + ' p=' + str(p)     resultados_ann.append(df_report) <pre>\nEntrenando red con p = 5 nodos en segunda capa oculta...\nEpoch 1/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step - accuracy: 0.4817 - loss: 1.0251\nEpoch 2/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7174 - loss: 0.7122\nEpoch 3/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7311 - loss: 0.6508\nEpoch 4/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7394 - loss: 0.6104\nEpoch 5/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7355 - loss: 0.5964\nEpoch 6/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7436 - loss: 0.5715\nEpoch 7/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7389 - loss: 0.5576\nEpoch 8/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7482 - loss: 0.5414\nEpoch 9/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7461 - loss: 0.5322\nEpoch 10/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7450 - loss: 0.5276\nEpoch 11/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7442 - loss: 0.5433\nEpoch 12/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7369 - loss: 0.5390\nEpoch 13/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.7341 - loss: 0.5265\nEpoch 14/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.7325 - loss: 0.5235\nEpoch 15/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.7423 - loss: 0.5057\nEpoch 16/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.7481 - loss: 0.4924\nEpoch 17/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.7528 - loss: 0.4844\nEpoch 18/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.7776 - loss: 0.3549\nEpoch 19/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.9420 - loss: 0.1865\nEpoch 20/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9622 - loss: 0.1296\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9115 - loss: 0.2400\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step\n\nEntrenando red con p = 10 nodos en segunda capa oculta...\nEpoch 1/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step - accuracy: 0.5541 - loss: 0.9439\nEpoch 2/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8711 - loss: 0.3730\nEpoch 3/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9430 - loss: 0.1999\nEpoch 4/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9568 - loss: 0.1497\nEpoch 5/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9606 - loss: 0.1249\nEpoch 6/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9654 - loss: 0.1074\nEpoch 7/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9657 - loss: 0.1014\nEpoch 8/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9686 - loss: 0.0942\nEpoch 9/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9654 - loss: 0.0940\nEpoch 10/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9663 - loss: 0.0922\nEpoch 11/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9588 - loss: 0.1094\nEpoch 12/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9443 - loss: 0.1466\nEpoch 13/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9430 - loss: 0.1606\nEpoch 14/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9715 - loss: 0.0983\nEpoch 15/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9747 - loss: 0.0832\nEpoch 16/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9801 - loss: 0.0753\nEpoch 17/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9808 - loss: 0.0748\nEpoch 18/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9780 - loss: 0.0772\nEpoch 19/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9783 - loss: 0.0756\nEpoch 20/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9752 - loss: 0.0696\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9418 - loss: 0.1558\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step\n\nEntrenando red con p = 20 nodos en segunda capa oculta...\nEpoch 1/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step - accuracy: 0.5639 - loss: 0.9263\nEpoch 2/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9006 - loss: 0.3122\nEpoch 3/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9286 - loss: 0.2126\nEpoch 4/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.9639 - loss: 0.1353\nEpoch 5/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 4ms/step - accuracy: 0.9669 - loss: 0.1155\nEpoch 6/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.9678 - loss: 0.1092\nEpoch 7/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 4ms/step - accuracy: 0.9654 - loss: 0.1084\nEpoch 8/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.9652 - loss: 0.1103\nEpoch 9/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9578 - loss: 0.1172\nEpoch 10/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9519 - loss: 0.1390\nEpoch 11/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9445 - loss: 0.1590\nEpoch 12/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9648 - loss: 0.1040\nEpoch 13/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9733 - loss: 0.0845\nEpoch 14/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9793 - loss: 0.0773\nEpoch 15/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9798 - loss: 0.0695\nEpoch 16/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9836 - loss: 0.0618\nEpoch 17/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9857 - loss: 0.0570\nEpoch 18/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9824 - loss: 0.0534\nEpoch 19/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9822 - loss: 0.0551\nEpoch 20/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9850 - loss: 0.0539\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9065 - loss: 0.3325\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step\n</pre> In\u00a0[\u00a0]: Copied! <pre># Concatenar resultados\ndf_ann_final = pd.concat(resultados_ann).reset_index().rename(columns={'index': 'clase'})\n\n# Mostrar resultados\nprint(\"Resultados por clase para cada valor de p:\")\ndf_ann_final\n</pre> # Concatenar resultados df_ann_final = pd.concat(resultados_ann).reset_index().rename(columns={'index': 'clase'})  # Mostrar resultados print(\"Resultados por clase para cada valor de p:\") df_ann_final <pre>Resultados por clase para cada valor de p:\n</pre> Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.954733 0.954733 0.954733 243.0 0.045267 ANN p=5 1 IE 0.877323 0.955466 0.914729 247.0 0.044534 ANN p=5 2 N 0.990689 0.951699 0.970803 559.0 0.048301 ANN p=5 3 EI 0.881481 0.979424 0.927875 243.0 0.020576 ANN p=10 4 IE 0.906504 0.902834 0.904665 247.0 0.097166 ANN p=10 5 N 0.984991 0.939177 0.961538 559.0 0.060823 ANN p=10 6 EI 0.960870 0.909465 0.934461 243.0 0.090535 ANN p=20 7 IE 0.925620 0.906883 0.916155 247.0 0.093117 ANN p=20 8 N 0.944541 0.974955 0.959507 559.0 0.025045 ANN p=20 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_ann_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_ann_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_ann_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_ann_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Crear modelo SVM\nmodel = SVC(kernel='linear')\n</pre> # Crear modelo SVM model = SVC(kernel='linear') In\u00a0[\u00a0]: Copied! <pre># Entrenar el modelo\nmodel.fit(X_train_encoded, y_train)\n</pre> # Entrenar el modelo model.fit(X_train_encoded, y_train) Out[\u00a0]: <pre>SVC(kernel='linear')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted<pre>SVC(kernel='linear')</pre> In\u00a0[\u00a0]: Copied! <pre># Hacer Predicciones\ny_pred = model.predict(X_test_encoded)\n</pre> # Hacer Predicciones y_pred = model.predict(X_test_encoded) In\u00a0[\u00a0]: Copied! <pre># Crear reporte de clasificaci\u00f3n\nreport = classification_report(\n    y_test,\n    y_pred,\n    output_dict=True,\n    target_names=['EI', 'IE', 'N']\n    )\n</pre> # Crear reporte de clasificaci\u00f3n report = classification_report(     y_test,     y_pred,     output_dict=True,     target_names=['EI', 'IE', 'N']     ) In\u00a0[\u00a0]: Copied! <pre># Crear DataFrame con m\u00e9tricas por clase\ndf_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\ndf_report['error'] = 1 - df_report['recall']\ndf_report['kernel'] = 'lineal'\ndf_report\n</pre> # Crear DataFrame con m\u00e9tricas por clase df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']] df_report['error'] = 1 - df_report['recall'] df_report['kernel'] = 'lineal' df_report Out[\u00a0]: precision recall f1-score support error kernel EI 0.882129 0.954733 0.916996 243.0 0.045267 lineal IE 0.905738 0.894737 0.900204 247.0 0.105263 lineal N 0.968635 0.939177 0.953678 559.0 0.060823 lineal In\u00a0[\u00a0]: Copied! <pre># Explorar las dos opciones de kernel lineal y rbf\n\n# Lista para almacenar resultados\nresultados_svm = []\n\n# Definir kernels a evaluar\nfor kernel in ['linear', 'rbf']:\n    print(f\"\\nEntrenando SVM con kernel = '{kernel}'...\")\n\n    # Crear modelo SVM\n    model = SVC(kernel=kernel)\n\n    # Entrenar el modelo\n    model.fit(X_train_encoded, y_train)\n\n    # Predicciones\n    y_pred = model.predict(X_test_encoded)\n\n    # Reporte de clasificaci\u00f3n\n    report = classification_report(\n        y_test,\n        y_pred,\n        output_dict=True,\n        target_names=['EI', 'IE', 'N']\n    )\n\n    # Crear DataFrame con m\u00e9tricas por clase\n    df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error'] = 1 - df_report['recall']\n    df_report['modelo'] = \"SVM \" + kernel\n    resultados_svm.append(df_report)\n</pre> # Explorar las dos opciones de kernel lineal y rbf  # Lista para almacenar resultados resultados_svm = []  # Definir kernels a evaluar for kernel in ['linear', 'rbf']:     print(f\"\\nEntrenando SVM con kernel = '{kernel}'...\")      # Crear modelo SVM     model = SVC(kernel=kernel)      # Entrenar el modelo     model.fit(X_train_encoded, y_train)      # Predicciones     y_pred = model.predict(X_test_encoded)      # Reporte de clasificaci\u00f3n     report = classification_report(         y_test,         y_pred,         output_dict=True,         target_names=['EI', 'IE', 'N']     )      # Crear DataFrame con m\u00e9tricas por clase     df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error'] = 1 - df_report['recall']     df_report['modelo'] = \"SVM \" + kernel     resultados_svm.append(df_report) <pre>\nEntrenando SVM con kernel = 'linear'...\n\nEntrenando SVM con kernel = 'rbf'...\n</pre> In\u00a0[\u00a0]: Copied! <pre># Concatenar resultados\ndf_svm_final = pd.concat(resultados_svm).reset_index().rename(columns={'index': 'clase'})\n\n# Mostrar tabla final\nprint(\"Resultados por clase para cada kernel:\")\ndf_svm_final\n</pre> # Concatenar resultados df_svm_final = pd.concat(resultados_svm).reset_index().rename(columns={'index': 'clase'})  # Mostrar tabla final print(\"Resultados por clase para cada kernel:\") df_svm_final <pre>Resultados por clase para cada kernel:\n</pre> Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.882129 0.954733 0.916996 243.0 0.045267 SVM linear 1 IE 0.905738 0.894737 0.900204 247.0 0.105263 SVM linear 2 N 0.968635 0.939177 0.953678 559.0 0.060823 SVM linear 3 EI 0.947791 0.971193 0.959350 243.0 0.028807 SVM rbf 4 IE 0.931727 0.939271 0.935484 247.0 0.060729 SVM rbf 5 N 0.985481 0.971377 0.978378 559.0 0.028623 SVM rbf In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_svm_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_svm_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_svm_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_svm_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Definir la funci\u00f3n evaluar_model_arbol()\n\ndef evaluar_modelo_arbol(boosting=False, max_depth=None):\n    # Seleccionar modelo\n    if boosting:\n        model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=max_depth, random_state=42)\n        nombre_modelo = '\u00c1rbol con Boosting'\n    else:\n        model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n        nombre_modelo = '\u00c1rbol de Decisi\u00f3n'\n\n    # Entrenar\n    model.fit(X_train_encoded, y_train)\n\n    # Predecir\n    y_pred = model.predict(X_test_encoded)\n\n    # Reporte de clasificaci\u00f3n\n    report = classification_report(\n        y_test, y_pred,\n        output_dict=True,\n        target_names=['EI', 'IE', 'N']\n    )\n\n    # Formatear DataFrame\n    df_report               = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error']      = 1 - df_report['recall']\n    df_report['modelo']     = nombre_modelo + ' depth=' + str(max_depth)\n\n    return df_report\n</pre> # Definir la funci\u00f3n evaluar_model_arbol()  def evaluar_modelo_arbol(boosting=False, max_depth=None):     # Seleccionar modelo     if boosting:         model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=max_depth, random_state=42)         nombre_modelo = '\u00c1rbol con Boosting'     else:         model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)         nombre_modelo = '\u00c1rbol de Decisi\u00f3n'      # Entrenar     model.fit(X_train_encoded, y_train)      # Predecir     y_pred = model.predict(X_test_encoded)      # Reporte de clasificaci\u00f3n     report = classification_report(         y_test, y_pred,         output_dict=True,         target_names=['EI', 'IE', 'N']     )      # Formatear DataFrame     df_report               = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error']      = 1 - df_report['recall']     df_report['modelo']     = nombre_modelo + ' depth=' + str(max_depth)      return df_report In\u00a0[\u00a0]: Copied! <pre># Evaluar ambos modelos\ndf_arbol = evaluar_modelo_arbol(boosting=False, max_depth=5)\ndf_boost = evaluar_modelo_arbol(boosting=True, max_depth=5)\n\n# Combinar resultados\ndf_arboles_final = pd.concat([df_arbol, df_boost]).reset_index().rename(columns={'index': 'clase'})\n\n# Mostrar\ndf_arboles_final\n</pre> # Evaluar ambos modelos df_arbol = evaluar_modelo_arbol(boosting=False, max_depth=5) df_boost = evaluar_modelo_arbol(boosting=True, max_depth=5)  # Combinar resultados df_arboles_final = pd.concat([df_arbol, df_boost]).reset_index().rename(columns={'index': 'clase'})  # Mostrar df_arboles_final Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.893130 0.962963 0.926733 243.0 0.037037 \u00c1rbol de Decisi\u00f3n depth=5 1 IE 0.946429 0.858300 0.900212 247.0 0.141700 \u00c1rbol de Decisi\u00f3n depth=5 2 N 0.959147 0.966011 0.962567 559.0 0.033989 \u00c1rbol de Decisi\u00f3n depth=5 3 EI 0.932540 0.967078 0.949495 243.0 0.032922 \u00c1rbol con Boosting depth=5 4 IE 0.942857 0.935223 0.939024 247.0 0.064777 \u00c1rbol con Boosting depth=5 5 N 0.983696 0.971377 0.977498 559.0 0.028623 \u00c1rbol con Boosting depth=5 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_arboles_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_arboles_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_arboles_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_arboles_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Definir la funci\u00f3n evaluar_random_forest()\ndef evaluar_random_forest(n_estimators):\n    # Inicializar modelo\n    model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n\n    # Entrenar\n    model.fit(X_train_encoded, y_train)\n\n    # Predecir\n    y_pred = model.predict(X_test_encoded)\n\n    # Obtener reporte por clase\n    report = classification_report(\n        y_test,\n        y_pred,\n        output_dict=True,\n        target_names=['EI', 'IE', 'N']\n    )\n\n    # Formatear resultado en DataFrame\n    df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error'] = 1 - df_report['recall']\n    df_report['modelo'] = 'Random Forest' + ' n=' + str(n_estimators)\n\n    return df_report\n</pre> # Definir la funci\u00f3n evaluar_random_forest() def evaluar_random_forest(n_estimators):     # Inicializar modelo     model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)      # Entrenar     model.fit(X_train_encoded, y_train)      # Predecir     y_pred = model.predict(X_test_encoded)      # Obtener reporte por clase     report = classification_report(         y_test,         y_pred,         output_dict=True,         target_names=['EI', 'IE', 'N']     )      # Formatear resultado en DataFrame     df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error'] = 1 - df_report['recall']     df_report['modelo'] = 'Random Forest' + ' n=' + str(n_estimators)      return df_report In\u00a0[\u00a0]: Copied! <pre># Evaluar Random Forest con 50 y 100 \u00e1rboles\ndf_rf_50  = evaluar_random_forest(n_estimators=50)\ndf_rf_100 = evaluar_random_forest(n_estimators=100)\n\n# Combinar resultados\ndf_rf_total = pd.concat([df_rf_50, df_rf_100]).reset_index().rename(columns={'index': 'clase'})\n\n# Mostrar resultados\ndf_rf_total\n</pre> # Evaluar Random Forest con 50 y 100 \u00e1rboles df_rf_50  = evaluar_random_forest(n_estimators=50) df_rf_100 = evaluar_random_forest(n_estimators=100)  # Combinar resultados df_rf_total = pd.concat([df_rf_50, df_rf_100]).reset_index().rename(columns={'index': 'clase'})  # Mostrar resultados df_rf_total Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.925197 0.967078 0.945674 243.0 0.032922 Random Forest n=50 1 IE 0.941909 0.919028 0.930328 247.0 0.080972 Random Forest n=50 2 N 0.983755 0.974955 0.979335 559.0 0.025045 Random Forest n=50 3 EI 0.928854 0.967078 0.947581 243.0 0.032922 Random Forest n=100 4 IE 0.937759 0.914980 0.926230 247.0 0.085020 Random Forest n=100 5 N 0.980180 0.973166 0.976661 559.0 0.026834 Random Forest n=100 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_rf_total.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_rf_total.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_rf_total.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_rf_total.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Crear tabla comparativa de todos los modelos implementados\n# Combinar resultados\ndf_resultado_modelos = pd.concat([df_knn_final, df_naivebayes_final, df_ann_final, df_svm_final, df_arboles_final, df_rf_total]).reset_index(drop=True)\n\n# Mostrar resultados\ndf_resultado_modelos\n</pre> # Crear tabla comparativa de todos los modelos implementados # Combinar resultados df_resultado_modelos = pd.concat([df_knn_final, df_naivebayes_final, df_ann_final, df_svm_final, df_arboles_final, df_rf_total]).reset_index(drop=True)  # Mostrar resultados df_resultado_modelos Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.633929 0.876543 0.735751 243.0 0.123457 kNN k=1 1 IE 0.662420 0.842105 0.741533 247.0 0.157895 kNN k=1 2 N 0.919799 0.656530 0.766180 559.0 0.343470 kNN k=1 3 EI 0.630986 0.921811 0.749164 243.0 0.078189 kNN k=3 4 IE 0.763699 0.902834 0.827458 247.0 0.097166 kNN k=3 5 N 0.965174 0.694097 0.807492 559.0 0.305903 kNN k=3 6 EI 0.665698 0.942387 0.780239 243.0 0.057613 kNN k=5 7 IE 0.719136 0.943320 0.816112 247.0 0.056680 kNN k=5 8 N 0.989501 0.674419 0.802128 559.0 0.325581 kNN k=5 9 EI 0.728707 0.950617 0.825000 243.0 0.049383 kNN k=7 10 IE 0.750000 0.959514 0.841918 247.0 0.040486 kNN k=7 11 N 0.990385 0.737030 0.845128 559.0 0.262970 kNN k=7 12 EI 0.231649 1.000000 0.376161 243.0 0.000000 Bernoulli B alpha=0 13 IE 0.000000 0.000000 0.000000 247.0 1.000000 Bernoulli B alpha=0 14 N 0.000000 0.000000 0.000000 559.0 1.000000 Bernoulli B alpha=0 15 EI 0.231649 1.000000 0.376161 243.0 0.000000 BernoulliNB alpha=1 16 IE 0.000000 0.000000 0.000000 247.0 1.000000 BernoulliNB alpha=1 17 N 0.000000 0.000000 0.000000 559.0 1.000000 BernoulliNB alpha=1 18 EI 0.958506 0.950617 0.954545 243.0 0.049383 GaussianNB 19 IE 0.905882 0.935223 0.920319 247.0 0.064777 GaussianNB 20 N 0.978300 0.967800 0.973022 559.0 0.032200 GaussianNB 21 EI 0.954733 0.954733 0.954733 243.0 0.045267 ANN p=5 22 IE 0.877323 0.955466 0.914729 247.0 0.044534 ANN p=5 23 N 0.990689 0.951699 0.970803 559.0 0.048301 ANN p=5 24 EI 0.881481 0.979424 0.927875 243.0 0.020576 ANN p=10 25 IE 0.906504 0.902834 0.904665 247.0 0.097166 ANN p=10 26 N 0.984991 0.939177 0.961538 559.0 0.060823 ANN p=10 27 EI 0.960870 0.909465 0.934461 243.0 0.090535 ANN p=20 28 IE 0.925620 0.906883 0.916155 247.0 0.093117 ANN p=20 29 N 0.944541 0.974955 0.959507 559.0 0.025045 ANN p=20 30 EI 0.882129 0.954733 0.916996 243.0 0.045267 SVM linear 31 IE 0.905738 0.894737 0.900204 247.0 0.105263 SVM linear 32 N 0.968635 0.939177 0.953678 559.0 0.060823 SVM linear 33 EI 0.947791 0.971193 0.959350 243.0 0.028807 SVM rbf 34 IE 0.931727 0.939271 0.935484 247.0 0.060729 SVM rbf 35 N 0.985481 0.971377 0.978378 559.0 0.028623 SVM rbf 36 EI 0.893130 0.962963 0.926733 243.0 0.037037 \u00c1rbol de Decisi\u00f3n depth=5 37 IE 0.946429 0.858300 0.900212 247.0 0.141700 \u00c1rbol de Decisi\u00f3n depth=5 38 N 0.959147 0.966011 0.962567 559.0 0.033989 \u00c1rbol de Decisi\u00f3n depth=5 39 EI 0.932540 0.967078 0.949495 243.0 0.032922 \u00c1rbol con Boosting depth=5 40 IE 0.942857 0.935223 0.939024 247.0 0.064777 \u00c1rbol con Boosting depth=5 41 N 0.983696 0.971377 0.977498 559.0 0.028623 \u00c1rbol con Boosting depth=5 42 EI 0.925197 0.967078 0.945674 243.0 0.032922 Random Forest n=50 43 IE 0.941909 0.919028 0.930328 247.0 0.080972 Random Forest n=50 44 N 0.983755 0.974955 0.979335 559.0 0.025045 Random Forest n=50 45 EI 0.928854 0.967078 0.947581 243.0 0.032922 Random Forest n=100 46 IE 0.937759 0.914980 0.926230 247.0 0.085020 Random Forest n=100 47 N 0.980180 0.973166 0.976661 559.0 0.026834 Random Forest n=100 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_resultado_modelos.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_resultado_modelos.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar el error por modelo y clase\npivot_error = df_resultado_modelos.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el error por modelo y clase pivot_error = df_resultado_modelos.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show()"},{"location":"machine-learning/classification-splice-junction/#classification-of-dna-splice-junctions","title":"Classification of DNA Splice Junctions\u00b6","text":""},{"location":"machine-learning/classification-splice-junction/#1-installation-of-dependencies-and-setup-of-the-working-directory","title":"1 Installation of Dependencies and Setup of the Working Directory\u00b6","text":""},{"location":"machine-learning/classification-splice-junction/#2-data-reading-and-preparation","title":"2 Data Reading and Preparation\u00b6","text":"<p>In this section, we will answer some general questions about the dataset contained in the file <code>splice.csv</code></p>"},{"location":"machine-learning/classification-splice-junction/#21-read-data","title":"2.1 Read Data\u00b6","text":""},{"location":"machine-learning/classification-splice-junction/#22-how-many-unique-seq_name-entries-does-the-dataset-have","title":"2.2 How many unique <code>seq_name</code> entries does the dataset have?\u00b6","text":""},{"location":"machine-learning/classification-splice-junction/#23-how-many-label-types-exist","title":"2.3 How many label types exist?\u00b6","text":""},{"location":"machine-learning/classification-splice-junction/#24-prepare-the-encoded-sequences-in-the-dataset","title":"2.4 Prepare the encoded sequences in the dataset\u00b6","text":"<p>Here we define the variable <code>seq_encoded</code>, which will contain the 3,178 records and their 480 features.</p>"},{"location":"machine-learning/classification-splice-junction/#25-prepare-the-dataset-labels","title":"2.5 Prepare the dataset labels\u00b6","text":"<p>The <code>Class</code> variable in the splicing dataset has three categories: EI, IE, and N. No particular positive class is defined here. This variable must be converted to a numeric type so it can be used in our classification models.</p>"},{"location":"machine-learning/classification-splice-junction/#26-create-training-and-test-datasets","title":"2.6 Create training and test datasets\u00b6","text":""},{"location":"machine-learning/classification-splice-junction/#3-implementation-of-the-convolutional-autoencoder","title":"3 Implementation of the Convolutional Autoencoder\u00b6","text":"<p>In this section, we build an autoencoder with the architecture shown in the figure, considering the purpose of each component as described below:</p> <p>The encoder is a regular CNN composed of convolutional layers and pooling layers. It typically reduces the spatial dimensionality of the inputs (i.e., height and width) while increasing the depth (i.e., the number of feature maps). The decoder must do the reverse (upscale the image and reduce its depth back to the original dimensions), and for this you can use transpose convolutional layers (alternatively, you could combine upsampling layers with convolutional layers). Texto extra\u00eddo de Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition secci\u00f3n Convolutional Autoencoders.</p> <p>Because DNA sequences are linear \u2014 an ordered chain of nucleotides \u2014 we use <code>Conv1D</code> to detect local motifs (such as splicing patterns).</p> <p>The only way to train the encoder to generate useful representations is to force it to reconstruct the original input \u2014 this is where the decoder comes in. Without a decoder, the encoder would not learn anything meaningful, because there would be no metric to tell whether the compression preserves useful information. Once the decoder is trained, we can use the encoder alone for classification models.</p>"},{"location":"machine-learning/classification-splice-junction/#4-application-of-algorithms","title":"4 Application of Algorithms\u00b6","text":""},{"location":"machine-learning/classification-splice-junction/#41-implementing-the-knn-model-for-k-1-3-5-7","title":"4.1 Implementing the kNN Model for k = 1, 3, 5, 7\u00b6","text":"<p>To generate the kNN model predictions based on the value of k, we create a function that performs the following main tasks:</p> <ol> <li>Build the model</li> <li>Train the model</li> <li>Make predictions</li> <li>Calculate performance metrics</li> </ol> <p>The function takes k, <code>X_train_encoded</code>, <code>y_train</code>, <code>X_test_encoded</code>, and <code>y_test</code> as input arguments and outputs a metrics dataframe. The model for k=3 is created first.</p>"},{"location":"machine-learning/classification-splice-junction/#comment-on-knn-performance","title":"Comment on kNN performance\u00b6","text":"<p>The evaluation strategy for the proposed models is the following:</p> <ul> <li>Focus on F1-score because it balances precision and recall, which is important for multiclass classification.</li> <li>Study the error.</li> <li>Evaluate per-class performance to ensure the model is not just optimizing for class N.</li> </ul> <p>kNN shows progressive improvement in F1-score as k increases, meaning it becomes more robust as more neighbors are considered. The error decreases accordingly.</p> <p>Class-by-class F1-scores reveal that IE and N perform more consistently, while EI is harder to identify correctly. Interestingly, class N shows the highest error, suggesting room for improvement.</p>"},{"location":"machine-learning/classification-splice-junction/#42-implementing-naive-bayes","title":"4.2 Implementing Naive Bayes\u00b6","text":"<p>To evaluate the performance of the Naive Bayes model depending on the type of data in Python, two approaches are proposed. First, the BernoulliNB classifier with Laplace smoothing (<code>alpha = 1</code> in Python) is applied to binary data obtained through one-hot encoding. Next, GaussianNB is used on continuous data derived from the convolutional autoencoder, since this model assumes a normal distribution of the variables. This strategy allows us to compare the impact of different input transformations on the performance of the Naive Bayes classifier.</p> <p>For more information on implementing the Naive Bayes model, refer to BernoulliNB and GaussianNB.</p>"},{"location":"machine-learning/classification-splice-junction/#comment-on-naive-bayes-performance","title":"Comment on Naive Bayes performance\u00b6","text":"<p>As mentioned at the beginning of this section, two variants of Naive Bayes were tested: BernoulliNB (to evaluate Laplace smoothing) and GaussianNB.</p> <p>The BernoulliNB model, which used data encoded through one-hot encoding, did not produce useful results. This is because the binary data derived from DNA sequences tend to be sparse and highly correlated, which makes it difficult for the model to capture relevant patterns \u2014 especially given that BernoulliNB assumes feature independence.</p> <p>In contrast, the GaussianNB model, which was fed with data transformed by the encoder, showed very strong performance. It achieved high and consistent F1-scores across all three classes, as well as very low error rates.</p>"},{"location":"machine-learning/classification-splice-junction/#43-implementing-artificial-neural-network-ann","title":"4.3 Implementing Artificial Neural Network (ANN)\u00b6","text":"<p>In this section, an artificial neural network is built with an architecture containing two hidden layers of 100 and p nodes, exploring p = 5, 10, and 20. We start the model with p = 20 and later scale the number of nodes.</p>"},{"location":"machine-learning/classification-splice-junction/#comment-on-ann-model-performance","title":"Comment on ANN model performance\u00b6","text":"<p>The artificial neural network (ANN) model shows solid overall performance, with high and consistent f1-scores across all classes. As the number of nodes in the 2nd hidden layer (p) increases, the f1-score remains steady, though without the clear improvements seen in the kNN model relative to k.</p> <p>Regarding classes, class N performs best, with f1-scores above 0.96 and minimal errors. Class EI follows, while class IE shows an error increase when p rises from 5 to 10, suggesting possible overfitting.</p>"},{"location":"machine-learning/classification-splice-junction/#44-implement-the-support-vector-machine-model","title":"4.4 Implement the Support Vector Machine model\u00b6","text":"<p>In this section, a Support Vector Machine model is built with two configurations: <code>linear kernel</code> and <code>rbf</code>. We begin with the linear kernel implementation.</p>"},{"location":"machine-learning/classification-splice-junction/#comment-on-svm-linear-and-rbf-performance","title":"Comment on SVM (Linear and RBF) performance\u00b6","text":"<p>The SVM model with linear kernel provides acceptable performance, with f1-scores above 0.90 across all classes. Class N performs best, while class IE has the most error.</p> <p>The SVM model with RBF kernel shows better f1-scores (&gt; 0.94) and considerably lower errors. The quality improvement is especially noticeable in EI and IE classes, while N also improves, achieving more balanced and consistent performance.</p> <p>In summary, the SVM model with RBF kernel outperforms the linear kernel in all relevant metrics, showing better generalization.</p>"},{"location":"machine-learning/classification-splice-junction/#45-implement-the-decision-tree-model","title":"4.5 Implement the Decision Tree model\u00b6","text":"<p>In this section, a decision tree model is built with two configurations: <code>boosting</code> and <code>no boosting</code>.</p>"},{"location":"machine-learning/classification-splice-junction/#comment-on-decision-tree-model","title":"Comment on Decision Tree model\u00b6","text":"<p>The decision tree model with depth 5 shows solid performance, with f1-scores above 0.90 for all classes and low errors. Class N stands out, maintaining an f1-score of 0.96, while EI and IE are stable, though IE has a higher error.</p> <p>Applying Boosting improves all metrics. F1-score increases slightly across all classes, reaching values above 0.94. Errors also decrease, especially for class IE.</p> <p>In summary, the Boosted Tree offers higher and more consistent performance. Combining Boosting with shallow trees seems to better capture the problem structure without overfitting.</p>"},{"location":"machine-learning/classification-splice-junction/#46-implement-the-random-forest-model-with-n-50-and-100","title":"4.6 Implement the Random Forest model with n = 50 and 100\u00b6","text":"<p>In this section, a Random Forest model is built with two numbers of trees: <code>n=50</code> and <code>n=100</code>.</p>"},{"location":"machine-learning/classification-splice-junction/#comment-on-random-forest-model","title":"Comment on Random Forest model\u00b6","text":"<p>The Random Forest model with 50 trees shows very competitive performance. F1-scores exceed 0.93 across all classes, particularly class N. Errors are low and fairly balanced, except for class IE, indicating good generalization.</p> <p>Increasing the number of trees to 100 does not show improvements. F1-scores remain the same, and errors remain low. This suggests that increasing the number of trees could help stabilize and fine-tune predictions, although current performance appears sufficient.</p>"},{"location":"machine-learning/classification-splice-junction/#5-comparison-of-proposed-models-performance","title":"5 Comparison of proposed models' performance\u00b6","text":""},{"location":"machine-learning/classification-splice-junction/#overall-comment-on-model-selection","title":"Overall comment on model selection\u00b6","text":"<p>In this project, where the goal is to predict splicing sites (classes EI and IE), it is crucial to choose models that not only achieve good overall f1-scores but are also precise for these classes without confusing non-splicing regions (class N).</p> <p>The best-performing models for this task are SVM RBF, Random Forest (n&gt;50), and Boosted Tree, as they offer the best balance between EI/IE classes without sacrificing specificity for class N.</p> <p>ANN models also perform very well, with high and balanced f1-scores. However, GaussianNB offers nearly the same performance with lower computational cost.</p> <p>Finally, kNN models, although they improve with higher k values, have lower f1-scores than the other models. The BernoulliNB model was completely discarded as it could not capture the data complexity.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/","title":"Classification of DNA Splice Junctions","text":"<p>by Salomon Marquez</p> <p>25/06/2025</p> <p>Splice junctions are points within a DNA sequence where \u201csuperfluous\u201d DNA is removed during the protein synthesis process in higher organisms.</p> <p>The goal of this project is to identify, given a DNA sequence, the boundaries between exons (the parts of the DNA sequence retained after splicing) and introns (the parts of the DNA sequence that are cut out). In the biological community, exon-intron (EI) boundaries are referred to as acceptors, while intron-exon (IE) boundaries are known as donors.</p> <p>To predict the type of splicing site, several machine learning algorithms will be implemented, including: k-Nearest Neighbour, Naive Bayes, Artificial Neural Network, Support Vector Machine, Decision Tree, and Random Forest. Finally, the following metrics will be evaluated: precision, recall, f1-score, and error, in order to determine which algorithms performed best for this task.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Conv1D, Conv1DTranspose, Flatten, Dense, Reshape, Dropout\n\nfrom IPython.display import Image\n</pre> import numpy as np import pandas as pd import os import random import matplotlib.pyplot as plt import seaborn as sns  from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import classification_report from sklearn.metrics import roc_curve, roc_auc_score from sklearn.naive_bayes import BernoulliNB from sklearn.naive_bayes import GaussianNB from sklearn.naive_bayes import MultinomialNB from sklearn.naive_bayes import CategoricalNB from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import RandomForestClassifier  import tensorflow as tf from tensorflow import keras from tensorflow.keras.models import Model, Sequential from tensorflow.keras.layers import Input, Conv1D, Conv1DTranspose, Flatten, Dense, Reshape, Dropout  from IPython.display import Image In\u00a0[\u00a0]: Copied! <pre># Fijar semilla para obtener resultados reproducibles cuando se ejecuta el notebook\n# Fijar la semilla\nseed_value = 123\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)\n</pre> # Fijar semilla para obtener resultados reproducibles cuando se ejecuta el notebook # Fijar la semilla seed_value = 123 os.environ['PYTHONHASHSEED'] = str(seed_value) random.seed(seed_value) np.random.seed(seed_value) tf.random.set_seed(seed_value) In\u00a0[\u00a0]: Copied! <pre># Obtener el directorio actual\nactual_wd = os.getcwd()\nprint(\"Ruta actual:\", actual_wd)\n\n# Establecer directorio de trabajo en Gdrive\n# os.chdir(\"/content/drive/MyDrive/ASIGNATURAS/M0.163 MACHINE LEARNING/[28 MAY - 17 JUN] RETO 4/PEC4\")\n</pre> # Obtener el directorio actual actual_wd = os.getcwd() print(\"Ruta actual:\", actual_wd)  # Establecer directorio de trabajo en Gdrive # os.chdir(\"/content/drive/MyDrive/ASIGNATURAS/M0.163 MACHINE LEARNING/[28 MAY - 17 JUN] RETO 4/PEC4\") In\u00a0[\u00a0]: Copied! <pre># Visualizar contenido del directorio de trabajo\n!ls\n</pre> # Visualizar contenido del directorio de trabajo !ls <pre>autoencoder_image.png\t enunciado_PEC4_2425_2.pdf    splice.csv\ndeep_descriptors.csv\t PEC4_Machine_Learning.html\ndeep_descriptors.gsheet  PEC4_Machine_Learning.ipynb\n</pre> In\u00a0[\u00a0]: Copied! <pre># Especificar el nombre del archivo de origen\nfile_name = \"splice.csv\"\nfile_path = os.path.join(actual_wd, file_name)\n</pre> # Especificar el nombre del archivo de origen file_name = \"splice.csv\" file_path = os.path.join(actual_wd, file_name) In\u00a0[\u00a0]: Copied! <pre># Guardar en un dataframe el contenido de splice.csv\ndf_splice = pd.read_csv(file_path, delimiter=',')\n</pre> # Guardar en un dataframe el contenido de splice.csv df_splice = pd.read_csv(file_path, delimiter=',') In\u00a0[\u00a0]: Copied! <pre># Visualizar contenido del dataframe splice\ndf_splice.info()\n</pre> # Visualizar contenido del dataframe splice df_splice.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3190 entries, 0 to 3189\nColumns: 482 entries, class to V480\ndtypes: int64(480), object(2)\nmemory usage: 11.7+ MB\n</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar las 5 primeras filas\ndf_splice.head(5)\n</pre> # Visualizar las 5 primeras filas df_splice.head(5) Out[\u00a0]: class seq_name V1 V2 V3 V4 V5 V6 V7 V8 ... V471 V472 V473 V474 V475 V476 V477 V478 V479 V480 0 EI ATRINS-DONOR-521 0 0 0 1 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 1 EI ATRINS-DONOR-905 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 2 EI BABAPOE-DONOR-30 0 1 0 0 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 3 EI BABAPOE-DONOR-867 0 1 0 0 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 4 EI BABAPOE-DONOR-2817 0 1 0 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 <p>5 rows \u00d7 482 columns</p> In\u00a0[\u00a0]: Copied! <pre># Visualizar las 5 \u00faltimas filas\ndf_splice.tail(5)\n</pre> # Visualizar las 5 \u00faltimas filas df_splice.tail(5) Out[\u00a0]: class seq_name V1 V2 V3 V4 V5 V6 V7 V8 ... V471 V472 V473 V474 V475 V476 V477 V478 V479 V480 3185 N ORAHBPSBD-NEG-2881 0 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 3186 N ORAINVOL-NEG-2161 0 1 0 0 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 3187 N ORARGIT-NEG-241 0 0 1 0 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 3188 N TARHBB-NEG-541 1 0 0 0 0 0 0 0 ... 0 0 1 0 0 0 0 0 0 0 3189 N TARHBD-NEG-1981 1 0 0 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 <p>5 rows \u00d7 482 columns</p> In\u00a0[\u00a0]: Copied! <pre># Obtener el n\u00famero de secuencias \u00fanicas\nn_unique = df_splice['seq_name'].nunique()\nn_unique\n</pre> # Obtener el n\u00famero de secuencias \u00fanicas n_unique = df_splice['seq_name'].nunique() n_unique Out[\u00a0]: <pre>3178</pre> In\u00a0[\u00a0]: Copied! <pre># Calcular el n\u00famero de secuencias repetidas\nprint(f\"El dataset contiene {len(df_splice)-n_unique} secuencia repetidas\")\n</pre> # Calcular el n\u00famero de secuencias repetidas print(f\"El dataset contiene {len(df_splice)-n_unique} secuencia repetidas\") <pre>El dataset contiene 12 secuencia repetidas\n</pre> In\u00a0[\u00a0]: Copied! <pre># Alternativamente para calcular duplicados\ndf_splice.duplicated().sum()\n</pre> # Alternativamente para calcular duplicados df_splice.duplicated().sum() Out[\u00a0]: <pre>np.int64(12)</pre> In\u00a0[\u00a0]: Copied! <pre># Identificar las secuencias repetidas\ndf_splice_unique = df_splice['seq_name'].value_counts()\ndf_splice_unique.head(15)\n</pre> # Identificar las secuencias repetidas df_splice_unique = df_splice['seq_name'].value_counts() df_splice_unique.head(15) Out[\u00a0]: count seq_name HUMMYC3L-ACCEPTOR-4242 2 HUMALBGC-DONOR-17044 2 HUMMYLCA-DONOR-2559 2 HUMMYLCA-DONOR-2388 2 HUMMYLCA-DONOR-1975 2 HUMMYLCA-DONOR-952 2 HUMALBGC-ACCEPTOR-18496 2 HUMMYLCA-ACCEPTOR-924 2 HUMMYLCA-ACCEPTOR-1831 2 HUMMYLCA-ACCEPTOR-2214 2 HUMMYLCA-ACCEPTOR-2481 2 HUMMYLCA-DONOR-644 2 HUMGAPJR-NEG-961 1 HUMGBR-NEG-2521 1 HUMGALAB-NEG-901 1 dtype: int64 In\u00a0[\u00a0]: Copied! <pre># Visualizar un par de secuencias repetidas\ndf_splice[df_splice['seq_name'].str.contains('HUMMYC3L-ACCEPTOR-4242')]\n</pre> # Visualizar un par de secuencias repetidas df_splice[df_splice['seq_name'].str.contains('HUMMYC3L-ACCEPTOR-4242')] Out[\u00a0]: class seq_name V1 V2 V3 V4 V5 V6 V7 V8 ... V471 V472 V473 V474 V475 V476 V477 V478 V479 V480 1316 IE HUMMYC3L-ACCEPTOR-4242 0 0 1 0 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 1317 IE HUMMYC3L-ACCEPTOR-4242 0 0 1 0 0 0 0 0 ... 0 0 0 1 0 0 0 0 0 0 <p>2 rows \u00d7 482 columns</p> In\u00a0[\u00a0]: Copied! <pre># Eliminar duplicados\ndf_splice_n = df_splice.drop_duplicates()\nprint(f\"El dataset originalmente conten\u00eda {len(df_splice)} registros y despu\u00e9s de eliminar duplicados se tienen {len(df_splice_n)} registros \")\n</pre> # Eliminar duplicados df_splice_n = df_splice.drop_duplicates() print(f\"El dataset originalmente conten\u00eda {len(df_splice)} registros y despu\u00e9s de eliminar duplicados se tienen {len(df_splice_n)} registros \") <pre>El dataset originalmente conten\u00eda 3190 registros y despu\u00e9s de eliminar duplicados se tienen 3178 registros \n</pre> In\u00a0[\u00a0]: Copied! <pre>df_splice_label = df_splice_n['class'].value_counts()\ndf_splice_label\n</pre> df_splice_label = df_splice_n['class'].value_counts() df_splice_label Out[\u00a0]: count class N 1655 IE 762 EI 761 dtype: int64 <p>Tenemos un dataset desbalanceado donde poco m\u00e1s del 50% de la secuencias corresponden a la categor\u00eda no splicing. Esto se debe tomar en cuenta puesto que los modelos de clasificaci\u00f3n propuestos a continuaci\u00f3n podr\u00edan sesgarse a predecir \"N\".</p> In\u00a0[\u00a0]: Copied! <pre># Seleccionar desde V1 hasta V480\nseq_encoded = df_splice_n.iloc[:,2:]\n</pre> # Seleccionar desde V1 hasta V480 seq_encoded = df_splice_n.iloc[:,2:] In\u00a0[\u00a0]: Copied! <pre># Verificar el tipo de variable\ntype(seq_encoded)\n</pre> # Verificar el tipo de variable type(seq_encoded) Out[\u00a0]: <pre>pandas.core.frame.DataFramedef __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre>/usr/local/lib/python3.11/dist-packages/pandas/core/frame.pyTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n&gt;&gt;&gt; d = {'col1': [1, 2], 'col2': [3, 4]}\n&gt;&gt;&gt; df = pd.DataFrame(data=d)\n&gt;&gt;&gt; df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n&gt;&gt;&gt; df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n&gt;&gt;&gt; df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n&gt;&gt;&gt; d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n&gt;&gt;&gt; df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n&gt;&gt;&gt; df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n&gt;&gt;&gt; from dataclasses import make_dataclass\n&gt;&gt;&gt; Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df\n   0\na  1\nc  3\n\n&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df2\n   x\na  1\nc  3</pre> In\u00a0[\u00a0]: Copied! <pre># Convertir datos a numpy.array\nseq_encoded_array = seq_encoded.to_numpy()\nseq_encoded_array[:5]\n</pre> # Convertir datos a numpy.array seq_encoded_array = seq_encoded.to_numpy() seq_encoded_array[:5] Out[\u00a0]: <pre>array([[0, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0],\n       [0, 1, 0, ..., 0, 0, 0],\n       [0, 1, 0, ..., 0, 0, 0],\n       [0, 1, 0, ..., 0, 0, 0]])</pre> In\u00a0[\u00a0]: Copied! <pre>labels = df_splice_n['class'].map({'EI': 0, 'IE': 1, 'N': 2})\n</pre> labels = df_splice_n['class'].map({'EI': 0, 'IE': 1, 'N': 2}) In\u00a0[\u00a0]: Copied! <pre># Convertir datos a numpy.array\nlabels_array = labels.to_numpy()\nlabels_array[:5]\n</pre> # Convertir datos a numpy.array labels_array = labels.to_numpy() labels_array[:5] Out[\u00a0]: <pre>array([0, 0, 0, 0, 0])</pre> In\u00a0[\u00a0]: Copied! <pre># Comprobar que las variables y los labels para entrenar los modelos de clasificaci\u00f3n sean tipo array\nprint(f\"La secuencias codificadas por one-hot encoding son de tipo {type(seq_encoded_array)} con dimensi\u00f3n {seq_encoded_array.shape}\\n\"\n      f\"y la variable label es de tipo {type(labels_array)} con dimensi\u00f3n {labels_array.shape}\")\n</pre> # Comprobar que las variables y los labels para entrenar los modelos de clasificaci\u00f3n sean tipo array print(f\"La secuencias codificadas por one-hot encoding son de tipo {type(seq_encoded_array)} con dimensi\u00f3n {seq_encoded_array.shape}\\n\"       f\"y la variable label es de tipo {type(labels_array)} con dimensi\u00f3n {labels_array.shape}\") <pre>La secuencias codificadas por one-hot encoding son de tipo &lt;class 'numpy.ndarray'&gt; con dimensi\u00f3n (3178, 480)\ny la variable label es de tipo &lt;class 'numpy.ndarray'&gt; con dimensi\u00f3n (3178,)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Crear train y test datasets\nX_train, X_test, y_train, y_test = train_test_split(\n    seq_encoded_array,\n    labels_array,\n    test_size = 0.33, # 33% para el dataset test como lo indica el enunciado de la PEC4\n    random_state= 123 # Fijamos la semilla aleatoria en 123\n)\n</pre> # Crear train y test datasets X_train, X_test, y_train, y_test = train_test_split(     seq_encoded_array,     labels_array,     test_size = 0.33, # 33% para el dataset test como lo indica el enunciado de la PEC4     random_state= 123 # Fijamos la semilla aleatoria en 123 ) In\u00a0[\u00a0]: Copied! <pre># Visualizar la dimensiones de los 4 datases creados a partir de train_test_split()\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n</pre> # Visualizar la dimensiones de los 4 datases creados a partir de train_test_split() X_train.shape, y_train.shape, X_test.shape, y_test.shape Out[\u00a0]: <pre>((2129, 480), (2129,), (1049, 480), (1049,))</pre> In\u00a0[\u00a0]: Copied! <pre>Image(\"autoencoder_image.png\",width=600, height=400)\n</pre> Image(\"autoencoder_image.png\",width=600, height=400) Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre># Reajustar las dimensiones de X_train puesto que Conv1D requiere datos en formato 3D\nm, n   = X_train.shape # Dimensiones del dataset train\np, r   = X_test.shape  # Dimensiones del dataset test\n\nX_train_reshaped = X_train.reshape((m, 60, 8))\nX_test_reshaped = X_test.reshape((p, 60, 8))\n</pre> # Reajustar las dimensiones de X_train puesto que Conv1D requiere datos en formato 3D m, n   = X_train.shape # Dimensiones del dataset train p, r   = X_test.shape  # Dimensiones del dataset test  X_train_reshaped = X_train.reshape((m, 60, 8)) X_test_reshaped = X_test.reshape((p, 60, 8)) In\u00a0[\u00a0]: Copied! <pre># AUTOENCODER\n# Definir capa de entrada\ninput_layer = Input(shape=(60, 8))\n\n# Definir encoder\nx           = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(input_layer)\nbottleneck  = Flatten()(x)\n#bottleneck  = Dense(32, activation='relu')(x)\n\n# Definir decoder\n#x             = Dense(60 * 8, activation='relu')(bottleneck)\nx             = Reshape((60, 8))(bottleneck)\noutput_layer  = Conv1DTranspose(filters=8, kernel_size=3, activation='sigmoid', padding='same')(x)\n\n# Definir modelo\nautoencoder = Model(inputs=input_layer, outputs=output_layer)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Entrenar autoencoder\nhistory = autoencoder.fit(X_train_reshaped, X_train_reshaped, epochs=20, batch_size=32, validation_split=0.2)\n</pre> # AUTOENCODER # Definir capa de entrada input_layer = Input(shape=(60, 8))  # Definir encoder x           = Conv1D(filters=8, kernel_size=3, activation='relu', padding='same')(input_layer) bottleneck  = Flatten()(x) #bottleneck  = Dense(32, activation='relu')(x)  # Definir decoder #x             = Dense(60 * 8, activation='relu')(bottleneck) x             = Reshape((60, 8))(bottleneck) output_layer  = Conv1DTranspose(filters=8, kernel_size=3, activation='sigmoid', padding='same')(x)  # Definir modelo autoencoder = Model(inputs=input_layer, outputs=output_layer) autoencoder.compile(optimizer='adam', loss='binary_crossentropy')  # Entrenar autoencoder history = autoencoder.fit(X_train_reshaped, X_train_reshaped, epochs=20, batch_size=32, validation_split=0.2) <pre>Epoch 1/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 13ms/step - loss: 0.7044 - val_loss: 0.6261\nEpoch 2/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 10ms/step - loss: 0.5937 - val_loss: 0.4684\nEpoch 3/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.4215 - val_loss: 0.3057\nEpoch 4/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - loss: 0.2815 - val_loss: 0.2242\nEpoch 5/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.2105 - val_loss: 0.1758\nEpoch 6/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.1660 - val_loss: 0.1404\nEpoch 7/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - loss: 0.1325 - val_loss: 0.1122\nEpoch 8/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.1057 - val_loss: 0.0897\nEpoch 9/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0842 - val_loss: 0.0718\nEpoch 10/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0673 - val_loss: 0.0577\nEpoch 11/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0539 - val_loss: 0.0466\nEpoch 12/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0435 - val_loss: 0.0380\nEpoch 13/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0354 - val_loss: 0.0313\nEpoch 14/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - loss: 0.0291 - val_loss: 0.0261\nEpoch 15/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0241 - val_loss: 0.0220\nEpoch 16/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0202 - val_loss: 0.0187\nEpoch 17/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0172 - val_loss: 0.0162\nEpoch 18/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0147 - val_loss: 0.0141\nEpoch 19/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0127 - val_loss: 0.0124\nEpoch 20/20\n54/54 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 0.0111 - val_loss: 0.0110\n</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluar la p\u00e9rdida de reconstrucci\u00f3n del autoencoder\ntrain_loss  = autoencoder.evaluate(X_train_reshaped, X_train_reshaped)\ntest_loss   = autoencoder.evaluate(X_test_reshaped, X_test_reshaped)\n\nprint(f\"\\nTrain loss: {train_loss:.4f}\")\nprint(f\"Test loss: {test_loss:.4f}\")\n</pre> # Evaluar la p\u00e9rdida de reconstrucci\u00f3n del autoencoder train_loss  = autoencoder.evaluate(X_train_reshaped, X_train_reshaped) test_loss   = autoencoder.evaluate(X_test_reshaped, X_test_reshaped)  print(f\"\\nTrain loss: {train_loss:.4f}\") print(f\"Test loss: {test_loss:.4f}\") <pre>67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 0.0101\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 0.0101\n\nTrain loss: 0.0103\nTest loss: 0.0101\n</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar la curva de entrenamiento del autoencoder\nplt.plot(history.history['loss'], label='Train loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('P\u00e9rdida de entrenamiento vs validaci\u00f3n')\nplt.show()\n</pre> # Visualizar la curva de entrenamiento del autoencoder plt.plot(history.history['loss'], label='Train loss') plt.plot(history.history['val_loss'], label='Validation loss') plt.legend() plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('P\u00e9rdida de entrenamiento vs validaci\u00f3n') plt.show() <p>Observamos que la calidad del autoencoder es buena puesto que la p\u00e9rdida de reconstrucci\u00f3n es muy baja.</p> In\u00a0[\u00a0]: Copied! <pre># Extraer la parte codificadora \"encoder\" para usarla en otros modelos\nencoder = Model(inputs=input_layer, outputs=bottleneck)\nX_train_encoded = encoder.predict(X_train_reshaped)\n</pre> # Extraer la parte codificadora \"encoder\" para usarla en otros modelos encoder = Model(inputs=input_layer, outputs=bottleneck) X_train_encoded = encoder.predict(X_train_reshaped) <pre>67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step\n</pre> In\u00a0[\u00a0]: Copied! <pre># Verificar las dimensiones de las nuevas coordenadas de X_train_encoded\nX_train_encoded.shape, X_train.shape\n</pre> # Verificar las dimensiones de las nuevas coordenadas de X_train_encoded X_train_encoded.shape, X_train.shape Out[\u00a0]: <pre>((2129, 480), (2129, 480))</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar X_train_encoded\nprint(X_train_encoded)\n</pre> # Visualizar X_train_encoded print(X_train_encoded) <pre>[[0.41865867 1.1792793  0.         ... 1.0533024  1.2378863  1.4537132 ]\n [0.97194517 1.2243328  0.7337566  ... 1.6940844  0.6968196  2.2641408 ]\n [0.7622293  0.27394313 1.5169445  ... 1.1491497  1.7781353  0.6089502 ]\n ...\n [0.         0.19901627 1.6442008  ... 0.92114854 1.2606764  0.        ]\n [0.55828255 1.0124682  0.6177381  ... 1.4002684  0.5707153  1.0705616 ]\n [0.18931112 0.25097656 1.0731099  ... 2.1333911  0.10522419 1.4503322 ]]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener las nuevas coordenadas de X_test\nX_test_encoded = encoder.predict(X_test_reshaped)\n</pre> # Obtener las nuevas coordenadas de X_test X_test_encoded = encoder.predict(X_test_reshaped) <pre>33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step \n</pre> In\u00a0[\u00a0]: Copied! <pre># Verificar las dimensiones de las nuevas coordenadas de X_test_encoded\nX_test_encoded.shape, X_test.shape\n</pre> # Verificar las dimensiones de las nuevas coordenadas de X_test_encoded X_test_encoded.shape, X_test.shape Out[\u00a0]: <pre>((1049, 480), (1049, 480))</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar X_test_encoded\nprint(X_test_encoded)\n</pre> # Visualizar X_test_encoded print(X_test_encoded) <pre>[[0.18931112 0.25097656 1.0731099  ... 0.8388007  1.4651122  1.0704793 ]\n [0.55828255 1.0124682  0.6177381  ... 1.4002684  0.5707153  1.0705616 ]\n [0.55828255 1.0124682  0.6177381  ... 0.8388007  1.4651122  1.0704793 ]\n ...\n [0.34228757 1.3086706  0.33683705 ... 1.0533024  1.2378863  1.4537132 ]\n [1.4116294  0.32590342 0.94585365 ... 1.0568092  1.7025597  1.0396074 ]\n [0.16956863 0.1146785  2.0411205  ... 1.7764323  0.49238378 1.1906232 ]]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Crear modelo k-nn con k = 3\nk     = 3   # Valor del nearest neighbor k = [1, 3, 5, 7]\nmodel = KNeighborsClassifier(n_neighbors=k, metric='euclidean') # Distancia euclidiana\n\n# Entrenar modelo k-nn\nmodel.fit(X_train_encoded, y_train)\n\n# Hacer predicciones con el modelo k-nn entrenado usando X_test\ny_pred = model.predict(X_test_encoded)\n\n# Imprimir reporte de clasificaci\u00f3n\n# Recordar que {'EI': 0, 'IE': 1, 'N': 2}\nprint(classification_report(y_test, y_pred, target_names=['Clase EI', 'Clase IE', 'Clase N']))\n</pre> # Crear modelo k-nn con k = 3 k     = 3   # Valor del nearest neighbor k = [1, 3, 5, 7] model = KNeighborsClassifier(n_neighbors=k, metric='euclidean') # Distancia euclidiana  # Entrenar modelo k-nn model.fit(X_train_encoded, y_train)  # Hacer predicciones con el modelo k-nn entrenado usando X_test y_pred = model.predict(X_test_encoded)  # Imprimir reporte de clasificaci\u00f3n # Recordar que {'EI': 0, 'IE': 1, 'N': 2} print(classification_report(y_test, y_pred, target_names=['Clase EI', 'Clase IE', 'Clase N'])) <pre>              precision    recall  f1-score   support\n\n    Clase EI       0.61      0.93      0.74       243\n    Clase IE       0.78      0.87      0.82       247\n     Clase N       0.96      0.70      0.81       559\n\n    accuracy                           0.79      1049\n   macro avg       0.78      0.83      0.79      1049\nweighted avg       0.83      0.79      0.79      1049\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener un dataframe a partir de la matriz de confusion\nreport             = classification_report(y_test, y_pred, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report          = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\ndf_report['error'] = 1 - df_report['recall']\ndf_report['k']     = k\ndf_report['modelo'] = \"kNN\"\ndf_report.reset_index(inplace=True)\ndf_report.rename(columns={'index': 'clase'}, inplace=True)\ndf_report\n</pre> # Obtener un dataframe a partir de la matriz de confusion report             = classification_report(y_test, y_pred, output_dict=True, target_names=['EI', 'IE', 'N']) df_report          = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']] df_report['error'] = 1 - df_report['recall'] df_report['k']     = k df_report['modelo'] = \"kNN\" df_report.reset_index(inplace=True) df_report.rename(columns={'index': 'clase'}, inplace=True) df_report Out[\u00a0]: clase precision recall f1-score support error k modelo 0 EI 0.614754 0.925926 0.738916 243.0 0.074074 3 kNN 1 IE 0.775362 0.866397 0.818356 247.0 0.133603 3 kNN 2 N 0.955774 0.695886 0.805383 559.0 0.304114 3 kNN In\u00a0[\u00a0]: Copied! <pre># Definir funci\u00f3n evaluar_knn_por_k()\ndef evaluar_knn_por_k(k, X_train, y_train, X_test, y_test):\n    \"\"\"\n    Entrena y eval\u00faa un modelo k-NN con valor k dado.\n    Retorna un DataFrame con precision, recall, f1-score, error por clase y k.\n    \"\"\"\n    # Entrenar modelo\n    model = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n    model.fit(X_train, y_train)\n\n    # Predicci\u00f3n\n    y_pred = model.predict(X_test)\n\n    # Clasification report por clase\n    report = classification_report(\n        y_test,\n        y_pred,\n        output_dict=True,\n        target_names=['EI', 'IE', 'N']\n    )\n\n    # Convertir a DataFrame\n    df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error'] = 1 - df_report['recall']\n    df_report['modelo'] = 'kNN' + ' k=' + str(k)\n    df_report.reset_index(inplace=True)\n    df_report.rename(columns={'index': 'clase'}, inplace=True)\n\n    return df_report\n</pre> # Definir funci\u00f3n evaluar_knn_por_k() def evaluar_knn_por_k(k, X_train, y_train, X_test, y_test):     \"\"\"     Entrena y eval\u00faa un modelo k-NN con valor k dado.     Retorna un DataFrame con precision, recall, f1-score, error por clase y k.     \"\"\"     # Entrenar modelo     model = KNeighborsClassifier(n_neighbors=k, metric='euclidean')     model.fit(X_train, y_train)      # Predicci\u00f3n     y_pred = model.predict(X_test)      # Clasification report por clase     report = classification_report(         y_test,         y_pred,         output_dict=True,         target_names=['EI', 'IE', 'N']     )      # Convertir a DataFrame     df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error'] = 1 - df_report['recall']     df_report['modelo'] = 'kNN' + ' k=' + str(k)     df_report.reset_index(inplace=True)     df_report.rename(columns={'index': 'clase'}, inplace=True)      return df_report In\u00a0[\u00a0]: Copied! <pre># Probar con diferentes valores de k\nresultados = []\n\nfor k in [1, 3, 5, 7]:\n    df_k = evaluar_knn_por_k(k, X_train_encoded, y_train, X_test_encoded, y_test)\n    resultados.append(df_k)\n\n# Unir todos los resultados\ndf_knn_final = pd.concat(resultados, ignore_index=True)\ndf_knn_final\n</pre> # Probar con diferentes valores de k resultados = []  for k in [1, 3, 5, 7]:     df_k = evaluar_knn_por_k(k, X_train_encoded, y_train, X_test_encoded, y_test)     resultados.append(df_k)  # Unir todos los resultados df_knn_final = pd.concat(resultados, ignore_index=True) df_knn_final Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.638554 0.872428 0.737391 243.0 0.127572 kNN k=1 1 IE 0.667722 0.854251 0.749556 247.0 0.145749 kNN k=1 2 N 0.922693 0.661896 0.770833 559.0 0.338104 kNN k=1 3 EI 0.614754 0.925926 0.738916 243.0 0.074074 kNN k=3 4 IE 0.775362 0.866397 0.818356 247.0 0.133603 kNN k=3 5 N 0.955774 0.695886 0.805383 559.0 0.304114 kNN k=3 6 EI 0.684524 0.946502 0.794473 243.0 0.053498 kNN k=5 7 IE 0.751656 0.919028 0.826958 247.0 0.080972 kNN k=5 8 N 0.975669 0.717352 0.826804 559.0 0.282648 kNN k=5 9 EI 0.707317 0.954733 0.812609 243.0 0.045267 kNN k=7 10 IE 0.759868 0.935223 0.838475 247.0 0.064777 kNN k=7 11 N 0.980815 0.731664 0.838115 559.0 0.268336 kNN k=7 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_knn_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_knn_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_knn_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\n### 4.1 Implementar el modelo kNN para k = 1, 3, 5 y 7\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_knn_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() ### 4.1 Implementar el modelo kNN para k = 1, 3, 5 y 7 plt.show() In\u00a0[\u00a0]: Copied! <pre># Entrenar BernoulliNB sin suavizado (alpha=0) con datos one-hot encoded\nmodelNB_alpha0 = BernoulliNB(alpha=0.0)\nmodelNB_alpha0.fit(X_train, y_train)\n\n# Predecir\ny_pred_NB_alpha0 = modelNB_alpha0.predict(X_test)\nprint(\"Predicciones:\", y_pred_NB_alpha0)\n</pre> # Entrenar BernoulliNB sin suavizado (alpha=0) con datos one-hot encoded modelNB_alpha0 = BernoulliNB(alpha=0.0) modelNB_alpha0.fit(X_train, y_train)  # Predecir y_pred_NB_alpha0 = modelNB_alpha0.predict(X_test) print(\"Predicciones:\", y_pred_NB_alpha0) <pre>Predicciones: [0 0 0 ... 0 0 0]\n</pre> <pre>/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py:1209: RuntimeWarning: divide by zero encountered in log\n  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n  ret = a @ b\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener un dataframe a partir de la matriz de confusion\nreport_NB_alpha0              = classification_report(y_test, y_pred_NB_alpha0, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report_NB_alpha0           = pd.DataFrame(report_NB_alpha0 ).T.loc[['EI', 'IE', 'N']]\ndf_report_NB_alpha0 ['error'] = 1 - df_report_NB_alpha0 ['recall']\ndf_report_NB_alpha0 ['modelo']  = \"Bernoulli B alpha=0\"\ndf_report_NB_alpha0\n</pre> # Obtener un dataframe a partir de la matriz de confusion report_NB_alpha0              = classification_report(y_test, y_pred_NB_alpha0, output_dict=True, target_names=['EI', 'IE', 'N']) df_report_NB_alpha0           = pd.DataFrame(report_NB_alpha0 ).T.loc[['EI', 'IE', 'N']] df_report_NB_alpha0 ['error'] = 1 - df_report_NB_alpha0 ['recall'] df_report_NB_alpha0 ['modelo']  = \"Bernoulli B alpha=0\" df_report_NB_alpha0 <pre>/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n</pre> Out[\u00a0]: precision recall f1-score support error modelo EI 0.231649 1.0 0.376161 243.0 0.0 Bernoulli B alpha=0 IE 0.000000 0.0 0.000000 247.0 1.0 Bernoulli B alpha=0 N 0.000000 0.0 0.000000 559.0 1.0 Bernoulli B alpha=0 In\u00a0[\u00a0]: Copied! <pre># Entrenar BernoulliNB con suavizado (alpha=1) con datos one-hot encoded\nmodelNB_alpha1 = BernoulliNB(alpha=1.0)\nmodelNB_alpha1.fit(X_train, y_train)\n\n# Predecir\ny_pred_NB_alpha1 = modelNB_alpha0.predict(X_test)\nprint(\"Predicciones:\", y_pred_NB_alpha1)\n</pre> # Entrenar BernoulliNB con suavizado (alpha=1) con datos one-hot encoded modelNB_alpha1 = BernoulliNB(alpha=1.0) modelNB_alpha1.fit(X_train, y_train)  # Predecir y_pred_NB_alpha1 = modelNB_alpha0.predict(X_test) print(\"Predicciones:\", y_pred_NB_alpha1) <pre>Predicciones: [0 0 0 ... 0 0 0]\n</pre> <pre>/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n  ret = a @ b\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener un dataframe a partir de la matriz de confusion\nreport_NB_alpha1             = classification_report(y_test, y_pred_NB_alpha1, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report_NB_alpha1          = pd.DataFrame(report_NB_alpha1).T.loc[['EI', 'IE', 'N']]\ndf_report_NB_alpha1['error'] = 1 - df_report_NB_alpha1['recall']\ndf_report_NB_alpha1['modelo']  = \"BernoulliNB alpha=1\"\ndf_report_NB_alpha1\n</pre> # Obtener un dataframe a partir de la matriz de confusion report_NB_alpha1             = classification_report(y_test, y_pred_NB_alpha1, output_dict=True, target_names=['EI', 'IE', 'N']) df_report_NB_alpha1          = pd.DataFrame(report_NB_alpha1).T.loc[['EI', 'IE', 'N']] df_report_NB_alpha1['error'] = 1 - df_report_NB_alpha1['recall'] df_report_NB_alpha1['modelo']  = \"BernoulliNB alpha=1\" df_report_NB_alpha1 <pre>/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n</pre> Out[\u00a0]: precision recall f1-score support error modelo EI 0.231649 1.0 0.376161 243.0 0.0 BernoulliNB alpha=1 IE 0.000000 0.0 0.000000 247.0 1.0 BernoulliNB alpha=1 N 0.000000 0.0 0.000000 559.0 1.0 BernoulliNB alpha=1 In\u00a0[\u00a0]: Copied! <pre># Entrenar GaussianNB con datos del autoencoder\nmodelNB = GaussianNB()\nmodelNB.fit(X_train_encoded, y_train)\n\n# Predecir\ny_pred_NB = modelNB.predict(X_test_encoded)\nprint(\"Predicciones:\", y_pred_NB)\n</pre> # Entrenar GaussianNB con datos del autoencoder modelNB = GaussianNB() modelNB.fit(X_train_encoded, y_train)  # Predecir y_pred_NB = modelNB.predict(X_test_encoded) print(\"Predicciones:\", y_pred_NB) <pre>Predicciones: [1 1 2 ... 0 2 1]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener un dataframe a partir de la matriz de confusion\nreport_NB             = classification_report(y_test, y_pred_NB, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report_NB          = pd.DataFrame(report_NB).T.loc[['EI', 'IE', 'N']]\ndf_report_NB['error'] = 1 - df_report_NB['recall']\ndf_report_NB['modelo']  = \"GaussianNB\"\ndf_report_NB\n</pre> # Obtener un dataframe a partir de la matriz de confusion report_NB             = classification_report(y_test, y_pred_NB, output_dict=True, target_names=['EI', 'IE', 'N']) df_report_NB          = pd.DataFrame(report_NB).T.loc[['EI', 'IE', 'N']] df_report_NB['error'] = 1 - df_report_NB['recall'] df_report_NB['modelo']  = \"GaussianNB\" df_report_NB Out[\u00a0]: precision recall f1-score support error modelo EI 0.942623 0.946502 0.944559 243.0 0.053498 GaussianNB IE 0.917647 0.947368 0.932271 247.0 0.052632 GaussianNB N 0.983636 0.967800 0.975654 559.0 0.032200 GaussianNB In\u00a0[\u00a0]: Copied! <pre># Unir todos los resultados\ndf_naivebayes_final = pd.concat([df_report_NB_alpha0, df_report_NB_alpha1, df_report_NB]).reset_index().rename(columns={'index': 'clase'})\ndf_naivebayes_final\n</pre> # Unir todos los resultados df_naivebayes_final = pd.concat([df_report_NB_alpha0, df_report_NB_alpha1, df_report_NB]).reset_index().rename(columns={'index': 'clase'}) df_naivebayes_final Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.231649 1.000000 0.376161 243.0 0.000000 Bernoulli B alpha=0 1 IE 0.000000 0.000000 0.000000 247.0 1.000000 Bernoulli B alpha=0 2 N 0.000000 0.000000 0.000000 559.0 1.000000 Bernoulli B alpha=0 3 EI 0.231649 1.000000 0.376161 243.0 0.000000 BernoulliNB alpha=1 4 IE 0.000000 0.000000 0.000000 247.0 1.000000 BernoulliNB alpha=1 5 N 0.000000 0.000000 0.000000 559.0 1.000000 BernoulliNB alpha=1 6 EI 0.942623 0.946502 0.944559 243.0 0.053498 GaussianNB 7 IE 0.917647 0.947368 0.932271 247.0 0.052632 GaussianNB 8 N 0.983636 0.967800 0.975654 559.0 0.032200 GaussianNB In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_naivebayes_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_naivebayes_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_naivebayes_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_naivebayes_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Definir nodo\np = 20\n</pre> # Definir nodo p = 20 In\u00a0[\u00a0]: Copied! <pre># Definir la arquitectura de la ANN\nmodel = Sequential([\n    Input(shape=(X_train_encoded.shape[1],)),\n    Dense(100, activation='relu'),\n    Dense(p, activation='relu'),\n    Dense(3, activation='softmax') # Ya que se tienen m\u00e1s de 2 categor\u00edas\n    ])\n</pre> # Definir la arquitectura de la ANN model = Sequential([     Input(shape=(X_train_encoded.shape[1],)),     Dense(100, activation='relu'),     Dense(p, activation='relu'),     Dense(3, activation='softmax') # Ya que se tienen m\u00e1s de 2 categor\u00edas     ]) In\u00a0[\u00a0]: Copied! <pre># Ver detalles del modelo\nmodel.summary()\n</pre> # Ver detalles del modelo model.summary() <pre>Model: \"sequential_8\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 dense_24 (Dense)                \u2502 (None, 100)            \u2502        48,100 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_25 (Dense)                \u2502 (None, 20)             \u2502         2,020 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_26 (Dense)                \u2502 (None, 3)              \u2502            63 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 50,183 (196.03 KB)\n</pre> <pre> Trainable params: 50,183 (196.03 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Compilar modelo\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy', # Puesto que las categor\u00edas de y_train son enteras [0, 1, 2]\n              metrics=['accuracy'])\n</pre> # Compilar modelo model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy', # Puesto que las categor\u00edas de y_train son enteras [0, 1, 2]               metrics=['accuracy']) In\u00a0[\u00a0]: Copied! <pre># Definir \u00e9pocas, tama\u00f1o del batch y entrenar el modelo\nn_batch  = 32\nn_epochs = 20\n\nmfit = model.fit(X_train_encoded, y_train,\n          epochs=n_epochs,\n          batch_size=n_batch)\n</pre> # Definir \u00e9pocas, tama\u00f1o del batch y entrenar el modelo n_batch  = 32 n_epochs = 20  mfit = model.fit(X_train_encoded, y_train,           epochs=n_epochs,           batch_size=n_batch) <pre>Epoch 1/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 4ms/step - accuracy: 0.5665 - loss: 0.9898\nEpoch 2/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 4ms/step - accuracy: 0.8900 - loss: 0.3338\nEpoch 3/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - accuracy: 0.9415 - loss: 0.1934\nEpoch 4/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 4ms/step - accuracy: 0.9525 - loss: 0.1493\nEpoch 5/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step - accuracy: 0.9655 - loss: 0.1159\nEpoch 6/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9739 - loss: 0.0986\nEpoch 7/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9715 - loss: 0.0910\nEpoch 8/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9725 - loss: 0.0824\nEpoch 9/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9725 - loss: 0.0859\nEpoch 10/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9717 - loss: 0.0829\nEpoch 11/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9720 - loss: 0.0820\nEpoch 12/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9534 - loss: 0.1180\nEpoch 13/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9412 - loss: 0.1866\nEpoch 14/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9617 - loss: 0.1020\nEpoch 15/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9781 - loss: 0.0762\nEpoch 16/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9778 - loss: 0.0704\nEpoch 17/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9730 - loss: 0.0781\nEpoch 18/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9680 - loss: 0.0903\nEpoch 19/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9769 - loss: 0.0664\nEpoch 20/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9795 - loss: 0.0596\n</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluar modelo\nloss, acc     = model.evaluate(X_test_encoded, y_test)\ny_pred        = model.predict(X_test_encoded)\ny_pred_labels = y_pred.argmax(axis=1)\n</pre> # Evaluar modelo loss, acc     = model.evaluate(X_test_encoded, y_test) y_pred        = model.predict(X_test_encoded) y_pred_labels = y_pred.argmax(axis=1) <pre>33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8993 - loss: 0.2753\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step\n</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar las primeras predicciones\ny_pred_labels[:100]\n</pre> # Visualizar las primeras predicciones y_pred_labels[:100] Out[\u00a0]: <pre>array([1, 1, 2, 2, 2, 2, 2, 1, 0, 0, 1, 0, 0, 1, 2, 0, 2, 0, 2, 0, 2, 0,\n       1, 2, 2, 2, 0, 2, 2, 2, 0, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2,\n       2, 2, 2, 1, 2, 2, 0, 0, 2, 1, 2, 2, 2, 2, 1, 1, 2, 0, 1, 2, 0, 2,\n       2, 0, 2, 1, 2, 1, 1, 2, 1, 0, 2, 0, 1, 2, 2, 0, 2, 1, 2, 1, 1, 2,\n       2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 0, 1])</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizar los truth labels\ny_test[:100]\n</pre> # Visualizar los truth labels y_test[:100] Out[\u00a0]: <pre>array([1, 1, 2, 2, 2, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0,\n       1, 2, 2, 2, 1, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 2, 1, 2, 1,\n       2, 2, 2, 1, 2, 2, 0, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 1, 2, 0, 2,\n       1, 0, 2, 1, 2, 1, 1, 2, 1, 0, 1, 0, 1, 2, 2, 0, 2, 1, 2, 1, 1, 2,\n       1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 0, 1])</pre> In\u00a0[\u00a0]: Copied! <pre># Obtener reporte de clasificaci\u00f3n\nreport              = classification_report(y_test, y_pred_labels, output_dict=True, target_names=['EI', 'IE', 'N'])\ndf_report           = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\ndf_report['error']  = 1 - df_report['recall']\ndf_report['modelo'] = 'ANN'\ndf_report['p']      = p\ndf_report\n</pre> # Obtener reporte de clasificaci\u00f3n report              = classification_report(y_test, y_pred_labels, output_dict=True, target_names=['EI', 'IE', 'N']) df_report           = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']] df_report['error']  = 1 - df_report['recall'] df_report['modelo'] = 'ANN' df_report['p']      = p df_report Out[\u00a0]: precision recall f1-score support error modelo p EI 0.932773 0.913580 0.923077 243.0 0.086420 ANN 20 IE 0.982659 0.688259 0.809524 247.0 0.311741 ANN 20 N 0.873041 0.996422 0.930660 559.0 0.003578 ANN 20 In\u00a0[\u00a0]: Copied! <pre># Validar el modelo ANN con distintos valores de p = [5, 10, 20]\n\n# Lista para guardar resultados\nresultados_ann = []\n\nfor p in [5, 10, 20]:\n    print(f\"\\nEntrenando red con p = {p} nodos en segunda capa oculta...\")\n\n    # Definir modelo\n    model = Sequential([\n        Input(shape=(X_train_encoded.shape[1],)),\n        Dense(100, activation='relu'),\n        Dense(p, activation='relu'),\n        Dense(3, activation='softmax')  # 3 clases\n    ])\n\n    # Compilar\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    # Entrenar\n    model.fit(X_train_encoded, y_train, epochs=20, batch_size=32)\n\n    # Evaluar\n    loss, acc = model.evaluate(X_test_encoded, y_test)\n    y_pred = model.predict(X_test_encoded)\n    y_pred_labels = y_pred.argmax(axis=1)\n\n    # Reporte de clasificaci\u00f3n\n    report = classification_report(y_test, y_pred_labels, output_dict=True, target_names=['EI', 'IE', 'N'])\n    df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error'] = 1 - df_report['recall']\n    df_report['modelo'] = 'ANN' + ' p=' + str(p)\n    resultados_ann.append(df_report)\n</pre> # Validar el modelo ANN con distintos valores de p = [5, 10, 20]  # Lista para guardar resultados resultados_ann = []  for p in [5, 10, 20]:     print(f\"\\nEntrenando red con p = {p} nodos en segunda capa oculta...\")      # Definir modelo     model = Sequential([         Input(shape=(X_train_encoded.shape[1],)),         Dense(100, activation='relu'),         Dense(p, activation='relu'),         Dense(3, activation='softmax')  # 3 clases     ])      # Compilar     model.compile(optimizer='adam',                   loss='sparse_categorical_crossentropy',                   metrics=['accuracy'])      # Entrenar     model.fit(X_train_encoded, y_train, epochs=20, batch_size=32)      # Evaluar     loss, acc = model.evaluate(X_test_encoded, y_test)     y_pred = model.predict(X_test_encoded)     y_pred_labels = y_pred.argmax(axis=1)      # Reporte de clasificaci\u00f3n     report = classification_report(y_test, y_pred_labels, output_dict=True, target_names=['EI', 'IE', 'N'])     df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error'] = 1 - df_report['recall']     df_report['modelo'] = 'ANN' + ' p=' + str(p)     resultados_ann.append(df_report) <pre>\nEntrenando red con p = 5 nodos en segunda capa oculta...\nEpoch 1/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step - accuracy: 0.4817 - loss: 1.0251\nEpoch 2/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7174 - loss: 0.7122\nEpoch 3/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7311 - loss: 0.6508\nEpoch 4/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7394 - loss: 0.6104\nEpoch 5/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7355 - loss: 0.5964\nEpoch 6/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7436 - loss: 0.5715\nEpoch 7/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7389 - loss: 0.5576\nEpoch 8/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7482 - loss: 0.5414\nEpoch 9/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7461 - loss: 0.5322\nEpoch 10/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7450 - loss: 0.5276\nEpoch 11/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7442 - loss: 0.5433\nEpoch 12/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.7369 - loss: 0.5390\nEpoch 13/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.7341 - loss: 0.5265\nEpoch 14/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.7325 - loss: 0.5235\nEpoch 15/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.7423 - loss: 0.5057\nEpoch 16/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.7481 - loss: 0.4924\nEpoch 17/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.7528 - loss: 0.4844\nEpoch 18/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.7776 - loss: 0.3549\nEpoch 19/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.9420 - loss: 0.1865\nEpoch 20/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9622 - loss: 0.1296\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9115 - loss: 0.2400\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step\n\nEntrenando red con p = 10 nodos en segunda capa oculta...\nEpoch 1/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step - accuracy: 0.5541 - loss: 0.9439\nEpoch 2/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.8711 - loss: 0.3730\nEpoch 3/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9430 - loss: 0.1999\nEpoch 4/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9568 - loss: 0.1497\nEpoch 5/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9606 - loss: 0.1249\nEpoch 6/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9654 - loss: 0.1074\nEpoch 7/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9657 - loss: 0.1014\nEpoch 8/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9686 - loss: 0.0942\nEpoch 9/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9654 - loss: 0.0940\nEpoch 10/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9663 - loss: 0.0922\nEpoch 11/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9588 - loss: 0.1094\nEpoch 12/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9443 - loss: 0.1466\nEpoch 13/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9430 - loss: 0.1606\nEpoch 14/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9715 - loss: 0.0983\nEpoch 15/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9747 - loss: 0.0832\nEpoch 16/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9801 - loss: 0.0753\nEpoch 17/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9808 - loss: 0.0748\nEpoch 18/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9780 - loss: 0.0772\nEpoch 19/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9783 - loss: 0.0756\nEpoch 20/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9752 - loss: 0.0696\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9418 - loss: 0.1558\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step\n\nEntrenando red con p = 20 nodos en segunda capa oculta...\nEpoch 1/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 3ms/step - accuracy: 0.5639 - loss: 0.9263\nEpoch 2/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9006 - loss: 0.3122\nEpoch 3/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9286 - loss: 0.2126\nEpoch 4/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - accuracy: 0.9639 - loss: 0.1353\nEpoch 5/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 4ms/step - accuracy: 0.9669 - loss: 0.1155\nEpoch 6/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.9678 - loss: 0.1092\nEpoch 7/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 4ms/step - accuracy: 0.9654 - loss: 0.1084\nEpoch 8/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 5ms/step - accuracy: 0.9652 - loss: 0.1103\nEpoch 9/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9578 - loss: 0.1172\nEpoch 10/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9519 - loss: 0.1390\nEpoch 11/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9445 - loss: 0.1590\nEpoch 12/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9648 - loss: 0.1040\nEpoch 13/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9733 - loss: 0.0845\nEpoch 14/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9793 - loss: 0.0773\nEpoch 15/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9798 - loss: 0.0695\nEpoch 16/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9836 - loss: 0.0618\nEpoch 17/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9857 - loss: 0.0570\nEpoch 18/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9824 - loss: 0.0534\nEpoch 19/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9822 - loss: 0.0551\nEpoch 20/20\n67/67 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - accuracy: 0.9850 - loss: 0.0539\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - accuracy: 0.9065 - loss: 0.3325\n33/33 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step\n</pre> In\u00a0[\u00a0]: Copied! <pre># Concatenar resultados\ndf_ann_final = pd.concat(resultados_ann).reset_index().rename(columns={'index': 'clase'})\n\n# Mostrar resultados\nprint(\"Resultados por clase para cada valor de p:\")\ndf_ann_final\n</pre> # Concatenar resultados df_ann_final = pd.concat(resultados_ann).reset_index().rename(columns={'index': 'clase'})  # Mostrar resultados print(\"Resultados por clase para cada valor de p:\") df_ann_final <pre>Resultados por clase para cada valor de p:\n</pre> Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.954733 0.954733 0.954733 243.0 0.045267 ANN p=5 1 IE 0.877323 0.955466 0.914729 247.0 0.044534 ANN p=5 2 N 0.990689 0.951699 0.970803 559.0 0.048301 ANN p=5 3 EI 0.881481 0.979424 0.927875 243.0 0.020576 ANN p=10 4 IE 0.906504 0.902834 0.904665 247.0 0.097166 ANN p=10 5 N 0.984991 0.939177 0.961538 559.0 0.060823 ANN p=10 6 EI 0.960870 0.909465 0.934461 243.0 0.090535 ANN p=20 7 IE 0.925620 0.906883 0.916155 247.0 0.093117 ANN p=20 8 N 0.944541 0.974955 0.959507 559.0 0.025045 ANN p=20 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_ann_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_ann_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_ann_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_ann_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Crear modelo SVM\nmodel = SVC(kernel='linear')\n</pre> # Crear modelo SVM model = SVC(kernel='linear') In\u00a0[\u00a0]: Copied! <pre># Entrenar el modelo\nmodel.fit(X_train_encoded, y_train)\n</pre> # Entrenar el modelo model.fit(X_train_encoded, y_train) Out[\u00a0]: <pre>SVC(kernel='linear')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted<pre>SVC(kernel='linear')</pre> In\u00a0[\u00a0]: Copied! <pre># Hacer Predicciones\ny_pred = model.predict(X_test_encoded)\n</pre> # Hacer Predicciones y_pred = model.predict(X_test_encoded) In\u00a0[\u00a0]: Copied! <pre># Crear reporte de clasificaci\u00f3n\nreport = classification_report(\n    y_test,\n    y_pred,\n    output_dict=True,\n    target_names=['EI', 'IE', 'N']\n    )\n</pre> # Crear reporte de clasificaci\u00f3n report = classification_report(     y_test,     y_pred,     output_dict=True,     target_names=['EI', 'IE', 'N']     ) In\u00a0[\u00a0]: Copied! <pre># Crear DataFrame con m\u00e9tricas por clase\ndf_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\ndf_report['error'] = 1 - df_report['recall']\ndf_report['kernel'] = 'lineal'\ndf_report\n</pre> # Crear DataFrame con m\u00e9tricas por clase df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']] df_report['error'] = 1 - df_report['recall'] df_report['kernel'] = 'lineal' df_report Out[\u00a0]: precision recall f1-score support error kernel EI 0.882129 0.954733 0.916996 243.0 0.045267 lineal IE 0.905738 0.894737 0.900204 247.0 0.105263 lineal N 0.968635 0.939177 0.953678 559.0 0.060823 lineal In\u00a0[\u00a0]: Copied! <pre># Explorar las dos opciones de kernel lineal y rbf\n\n# Lista para almacenar resultados\nresultados_svm = []\n\n# Definir kernels a evaluar\nfor kernel in ['linear', 'rbf']:\n    print(f\"\\nEntrenando SVM con kernel = '{kernel}'...\")\n\n    # Crear modelo SVM\n    model = SVC(kernel=kernel)\n\n    # Entrenar el modelo\n    model.fit(X_train_encoded, y_train)\n\n    # Predicciones\n    y_pred = model.predict(X_test_encoded)\n\n    # Reporte de clasificaci\u00f3n\n    report = classification_report(\n        y_test,\n        y_pred,\n        output_dict=True,\n        target_names=['EI', 'IE', 'N']\n    )\n\n    # Crear DataFrame con m\u00e9tricas por clase\n    df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error'] = 1 - df_report['recall']\n    df_report['modelo'] = \"SVM \" + kernel\n    resultados_svm.append(df_report)\n</pre> # Explorar las dos opciones de kernel lineal y rbf  # Lista para almacenar resultados resultados_svm = []  # Definir kernels a evaluar for kernel in ['linear', 'rbf']:     print(f\"\\nEntrenando SVM con kernel = '{kernel}'...\")      # Crear modelo SVM     model = SVC(kernel=kernel)      # Entrenar el modelo     model.fit(X_train_encoded, y_train)      # Predicciones     y_pred = model.predict(X_test_encoded)      # Reporte de clasificaci\u00f3n     report = classification_report(         y_test,         y_pred,         output_dict=True,         target_names=['EI', 'IE', 'N']     )      # Crear DataFrame con m\u00e9tricas por clase     df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error'] = 1 - df_report['recall']     df_report['modelo'] = \"SVM \" + kernel     resultados_svm.append(df_report) <pre>\nEntrenando SVM con kernel = 'linear'...\n\nEntrenando SVM con kernel = 'rbf'...\n</pre> In\u00a0[\u00a0]: Copied! <pre># Concatenar resultados\ndf_svm_final = pd.concat(resultados_svm).reset_index().rename(columns={'index': 'clase'})\n\n# Mostrar tabla final\nprint(\"Resultados por clase para cada kernel:\")\ndf_svm_final\n</pre> # Concatenar resultados df_svm_final = pd.concat(resultados_svm).reset_index().rename(columns={'index': 'clase'})  # Mostrar tabla final print(\"Resultados por clase para cada kernel:\") df_svm_final <pre>Resultados por clase para cada kernel:\n</pre> Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.882129 0.954733 0.916996 243.0 0.045267 SVM linear 1 IE 0.905738 0.894737 0.900204 247.0 0.105263 SVM linear 2 N 0.968635 0.939177 0.953678 559.0 0.060823 SVM linear 3 EI 0.947791 0.971193 0.959350 243.0 0.028807 SVM rbf 4 IE 0.931727 0.939271 0.935484 247.0 0.060729 SVM rbf 5 N 0.985481 0.971377 0.978378 559.0 0.028623 SVM rbf In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_svm_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_svm_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_svm_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_svm_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Definir la funci\u00f3n evaluar_model_arbol()\n\ndef evaluar_modelo_arbol(boosting=False, max_depth=None):\n    # Seleccionar modelo\n    if boosting:\n        model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=max_depth, random_state=42)\n        nombre_modelo = '\u00c1rbol con Boosting'\n    else:\n        model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n        nombre_modelo = '\u00c1rbol de Decisi\u00f3n'\n\n    # Entrenar\n    model.fit(X_train_encoded, y_train)\n\n    # Predecir\n    y_pred = model.predict(X_test_encoded)\n\n    # Reporte de clasificaci\u00f3n\n    report = classification_report(\n        y_test, y_pred,\n        output_dict=True,\n        target_names=['EI', 'IE', 'N']\n    )\n\n    # Formatear DataFrame\n    df_report               = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error']      = 1 - df_report['recall']\n    df_report['modelo']     = nombre_modelo + ' depth=' + str(max_depth)\n\n    return df_report\n</pre> # Definir la funci\u00f3n evaluar_model_arbol()  def evaluar_modelo_arbol(boosting=False, max_depth=None):     # Seleccionar modelo     if boosting:         model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=max_depth, random_state=42)         nombre_modelo = '\u00c1rbol con Boosting'     else:         model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)         nombre_modelo = '\u00c1rbol de Decisi\u00f3n'      # Entrenar     model.fit(X_train_encoded, y_train)      # Predecir     y_pred = model.predict(X_test_encoded)      # Reporte de clasificaci\u00f3n     report = classification_report(         y_test, y_pred,         output_dict=True,         target_names=['EI', 'IE', 'N']     )      # Formatear DataFrame     df_report               = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error']      = 1 - df_report['recall']     df_report['modelo']     = nombre_modelo + ' depth=' + str(max_depth)      return df_report In\u00a0[\u00a0]: Copied! <pre># Evaluar ambos modelos\ndf_arbol = evaluar_modelo_arbol(boosting=False, max_depth=5)\ndf_boost = evaluar_modelo_arbol(boosting=True, max_depth=5)\n\n# Combinar resultados\ndf_arboles_final = pd.concat([df_arbol, df_boost]).reset_index().rename(columns={'index': 'clase'})\n\n# Mostrar\ndf_arboles_final\n</pre> # Evaluar ambos modelos df_arbol = evaluar_modelo_arbol(boosting=False, max_depth=5) df_boost = evaluar_modelo_arbol(boosting=True, max_depth=5)  # Combinar resultados df_arboles_final = pd.concat([df_arbol, df_boost]).reset_index().rename(columns={'index': 'clase'})  # Mostrar df_arboles_final Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.893130 0.962963 0.926733 243.0 0.037037 \u00c1rbol de Decisi\u00f3n depth=5 1 IE 0.946429 0.858300 0.900212 247.0 0.141700 \u00c1rbol de Decisi\u00f3n depth=5 2 N 0.959147 0.966011 0.962567 559.0 0.033989 \u00c1rbol de Decisi\u00f3n depth=5 3 EI 0.932540 0.967078 0.949495 243.0 0.032922 \u00c1rbol con Boosting depth=5 4 IE 0.942857 0.935223 0.939024 247.0 0.064777 \u00c1rbol con Boosting depth=5 5 N 0.983696 0.971377 0.977498 559.0 0.028623 \u00c1rbol con Boosting depth=5 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_arboles_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_arboles_final.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_arboles_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_arboles_final.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Definir la funci\u00f3n evaluar_random_forest()\ndef evaluar_random_forest(n_estimators):\n    # Inicializar modelo\n    model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n\n    # Entrenar\n    model.fit(X_train_encoded, y_train)\n\n    # Predecir\n    y_pred = model.predict(X_test_encoded)\n\n    # Obtener reporte por clase\n    report = classification_report(\n        y_test,\n        y_pred,\n        output_dict=True,\n        target_names=['EI', 'IE', 'N']\n    )\n\n    # Formatear resultado en DataFrame\n    df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]\n    df_report['error'] = 1 - df_report['recall']\n    df_report['modelo'] = 'Random Forest' + ' n=' + str(n_estimators)\n\n    return df_report\n</pre> # Definir la funci\u00f3n evaluar_random_forest() def evaluar_random_forest(n_estimators):     # Inicializar modelo     model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)      # Entrenar     model.fit(X_train_encoded, y_train)      # Predecir     y_pred = model.predict(X_test_encoded)      # Obtener reporte por clase     report = classification_report(         y_test,         y_pred,         output_dict=True,         target_names=['EI', 'IE', 'N']     )      # Formatear resultado en DataFrame     df_report = pd.DataFrame(report).T.loc[['EI', 'IE', 'N']]     df_report['error'] = 1 - df_report['recall']     df_report['modelo'] = 'Random Forest' + ' n=' + str(n_estimators)      return df_report In\u00a0[\u00a0]: Copied! <pre># Evaluar Random Forest con 50 y 100 \u00e1rboles\ndf_rf_50  = evaluar_random_forest(n_estimators=50)\ndf_rf_100 = evaluar_random_forest(n_estimators=100)\n\n# Combinar resultados\ndf_rf_total = pd.concat([df_rf_50, df_rf_100]).reset_index().rename(columns={'index': 'clase'})\n\n# Mostrar resultados\ndf_rf_total\n</pre> # Evaluar Random Forest con 50 y 100 \u00e1rboles df_rf_50  = evaluar_random_forest(n_estimators=50) df_rf_100 = evaluar_random_forest(n_estimators=100)  # Combinar resultados df_rf_total = pd.concat([df_rf_50, df_rf_100]).reset_index().rename(columns={'index': 'clase'})  # Mostrar resultados df_rf_total Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.925197 0.967078 0.945674 243.0 0.032922 Random Forest n=50 1 IE 0.941909 0.919028 0.930328 247.0 0.080972 Random Forest n=50 2 N 0.983755 0.974955 0.979335 559.0 0.025045 Random Forest n=50 3 EI 0.928854 0.967078 0.947581 243.0 0.032922 Random Forest n=100 4 IE 0.937759 0.914980 0.926230 247.0 0.085020 Random Forest n=100 5 N 0.980180 0.973166 0.976661 559.0 0.026834 Random Forest n=100 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_rf_total.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_rf_total.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar la error por modelo y clase\npivot_error = df_rf_total.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar la error por modelo y clase pivot_error = df_rf_total.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Crear tabla comparativa de todos los modelos implementados\n# Combinar resultados\ndf_resultado_modelos = pd.concat([df_knn_final, df_naivebayes_final, df_ann_final, df_svm_final, df_arboles_final, df_rf_total]).reset_index(drop=True)\n\n# Mostrar resultados\ndf_resultado_modelos\n</pre> # Crear tabla comparativa de todos los modelos implementados # Combinar resultados df_resultado_modelos = pd.concat([df_knn_final, df_naivebayes_final, df_ann_final, df_svm_final, df_arboles_final, df_rf_total]).reset_index(drop=True)  # Mostrar resultados df_resultado_modelos Out[\u00a0]: clase precision recall f1-score support error modelo 0 EI 0.633929 0.876543 0.735751 243.0 0.123457 kNN k=1 1 IE 0.662420 0.842105 0.741533 247.0 0.157895 kNN k=1 2 N 0.919799 0.656530 0.766180 559.0 0.343470 kNN k=1 3 EI 0.630986 0.921811 0.749164 243.0 0.078189 kNN k=3 4 IE 0.763699 0.902834 0.827458 247.0 0.097166 kNN k=3 5 N 0.965174 0.694097 0.807492 559.0 0.305903 kNN k=3 6 EI 0.665698 0.942387 0.780239 243.0 0.057613 kNN k=5 7 IE 0.719136 0.943320 0.816112 247.0 0.056680 kNN k=5 8 N 0.989501 0.674419 0.802128 559.0 0.325581 kNN k=5 9 EI 0.728707 0.950617 0.825000 243.0 0.049383 kNN k=7 10 IE 0.750000 0.959514 0.841918 247.0 0.040486 kNN k=7 11 N 0.990385 0.737030 0.845128 559.0 0.262970 kNN k=7 12 EI 0.231649 1.000000 0.376161 243.0 0.000000 Bernoulli B alpha=0 13 IE 0.000000 0.000000 0.000000 247.0 1.000000 Bernoulli B alpha=0 14 N 0.000000 0.000000 0.000000 559.0 1.000000 Bernoulli B alpha=0 15 EI 0.231649 1.000000 0.376161 243.0 0.000000 BernoulliNB alpha=1 16 IE 0.000000 0.000000 0.000000 247.0 1.000000 BernoulliNB alpha=1 17 N 0.000000 0.000000 0.000000 559.0 1.000000 BernoulliNB alpha=1 18 EI 0.958506 0.950617 0.954545 243.0 0.049383 GaussianNB 19 IE 0.905882 0.935223 0.920319 247.0 0.064777 GaussianNB 20 N 0.978300 0.967800 0.973022 559.0 0.032200 GaussianNB 21 EI 0.954733 0.954733 0.954733 243.0 0.045267 ANN p=5 22 IE 0.877323 0.955466 0.914729 247.0 0.044534 ANN p=5 23 N 0.990689 0.951699 0.970803 559.0 0.048301 ANN p=5 24 EI 0.881481 0.979424 0.927875 243.0 0.020576 ANN p=10 25 IE 0.906504 0.902834 0.904665 247.0 0.097166 ANN p=10 26 N 0.984991 0.939177 0.961538 559.0 0.060823 ANN p=10 27 EI 0.960870 0.909465 0.934461 243.0 0.090535 ANN p=20 28 IE 0.925620 0.906883 0.916155 247.0 0.093117 ANN p=20 29 N 0.944541 0.974955 0.959507 559.0 0.025045 ANN p=20 30 EI 0.882129 0.954733 0.916996 243.0 0.045267 SVM linear 31 IE 0.905738 0.894737 0.900204 247.0 0.105263 SVM linear 32 N 0.968635 0.939177 0.953678 559.0 0.060823 SVM linear 33 EI 0.947791 0.971193 0.959350 243.0 0.028807 SVM rbf 34 IE 0.931727 0.939271 0.935484 247.0 0.060729 SVM rbf 35 N 0.985481 0.971377 0.978378 559.0 0.028623 SVM rbf 36 EI 0.893130 0.962963 0.926733 243.0 0.037037 \u00c1rbol de Decisi\u00f3n depth=5 37 IE 0.946429 0.858300 0.900212 247.0 0.141700 \u00c1rbol de Decisi\u00f3n depth=5 38 N 0.959147 0.966011 0.962567 559.0 0.033989 \u00c1rbol de Decisi\u00f3n depth=5 39 EI 0.932540 0.967078 0.949495 243.0 0.032922 \u00c1rbol con Boosting depth=5 40 IE 0.942857 0.935223 0.939024 247.0 0.064777 \u00c1rbol con Boosting depth=5 41 N 0.983696 0.971377 0.977498 559.0 0.028623 \u00c1rbol con Boosting depth=5 42 EI 0.925197 0.967078 0.945674 243.0 0.032922 Random Forest n=50 43 IE 0.941909 0.919028 0.930328 247.0 0.080972 Random Forest n=50 44 N 0.983755 0.974955 0.979335 559.0 0.025045 Random Forest n=50 45 EI 0.928854 0.967078 0.947581 243.0 0.032922 Random Forest n=100 46 IE 0.937759 0.914980 0.926230 247.0 0.085020 Random Forest n=100 47 N 0.980180 0.973166 0.976661 559.0 0.026834 Random Forest n=100 In\u00a0[\u00a0]: Copied! <pre># Graficar el f1-score por modelo y clase\npivot_f1_score = df_resultado_modelos.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('f1-score por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el f1-score por modelo y clase pivot_f1_score = df_resultado_modelos.pivot_table(index='modelo', columns='clase', values='f1-score', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_f1_score, annot=True, cmap='inferno', fmt=\".2f\") plt.title('f1-score por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Graficar el error por modelo y clase\npivot_error = df_resultado_modelos.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean')\nplt.figure(figsize=(8, 4))\nsns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\")\nplt.title('Error por Modelo y Clase')\nplt.ylabel('Modelo')\nplt.xlabel('Clase')\nplt.tight_layout()\nplt.show()\n</pre> # Graficar el error por modelo y clase pivot_error = df_resultado_modelos.pivot_table(index='modelo', columns='clase', values='error', aggfunc='mean') plt.figure(figsize=(8, 4)) sns.heatmap(pivot_error, annot=True, cmap='inferno', fmt=\".2f\") plt.title('Error por Modelo y Clase') plt.ylabel('Modelo') plt.xlabel('Clase') plt.tight_layout() plt.show()"},{"location":"machine-learning/classification-splice-junction_spanish/#classification-of-dna-splice-junctions","title":"Classification of DNA Splice Junctions\u00b6","text":""},{"location":"machine-learning/classification-splice-junction_spanish/#1-instalacion-de-dependencias-y-configuracion-del-directorio-de-trabajo","title":"1 Instalaci\u00f3n de dependencias y configuraci\u00f3n del directorio de trabajo\u00b6","text":""},{"location":"machine-learning/classification-splice-junction_spanish/#2-lectura-y-preparacion-de-datos","title":"2 Lectura y preparaci\u00f3n de datos\u00b6","text":"<p>En esta secci\u00f3n responderemos algunas preguntas generales sobre el dataset contenido en el archivo <code>splice.csv</code></p>"},{"location":"machine-learning/classification-splice-junction_spanish/#21-leer-datos","title":"2.1 Leer datos\u00b6","text":""},{"location":"machine-learning/classification-splice-junction_spanish/#22-cuantas-seq_name-unicas-tiene-el-dataset","title":"2.2 \u00bfCu\u00e1ntas <code>seq_name</code> \u00fanicas tiene el dataset?\u00b6","text":""},{"location":"machine-learning/classification-splice-junction_spanish/#23-cuantos-tipos-de-etiquetas-existen","title":"2.3 \u00bfCu\u00e1ntos tipos de etiquetas existen?\u00b6","text":""},{"location":"machine-learning/classification-splice-junction_spanish/#24-preparar-las-secuencias-codificadas-del-dataset","title":"2.4 Preparar las secuencias codificadas del dataset\u00b6","text":"<p>En esta secci\u00f3n, definiremos la variable <code>seq_encoded</code> que contendr\u00e1 los 3,178 registros y sus 480 caracter\u00edsticas.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#25-preparar-los-labels-del-dataset","title":"2.5 Preparar los labels del dataset\u00b6","text":"<p>La variable <code>Class</code> del dataset splicing posse tres categor\u00edas EI, IE y N. Aqu\u00ed no se define una categor\u00eda positiva en particular. A continuaci\u00f3n, es necesario convertir esta variable en tipo num\u00e9rica para poder usarla en nuestros modelos de clasificaci\u00f3n posteriores.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#26-crear-train-y-test-datasets","title":"2.6 Crear train y test datasets\u00b6","text":""},{"location":"machine-learning/classification-splice-junction_spanish/#3-implementacion-del-autoencoder-convolucional","title":"3 Implementacion del autoencoder convolucional\u00b6","text":"<p>En esta secci\u00f3n construiremos un autoencoder con la arquitectura que se muestra en la figura y tomando en cuenta el prop\u00f3sito de cada componente como se describe a continuaci\u00f3n:</p> <p>The encoder is a regular CNN composed of convolutional layers and pooling layers. It typically reduces the spatial dimensionality of the inputs (i.e., height and width) while increasing the depth (i.e., the number of feature maps). The decoder must do the reverse (upscale the image and reduce its depth back to the original dimensions), and for this you can use transpose convolutional layers (alternatively, you could combine upsampling layers with convolutional layers). Texto extra\u00eddo de Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition secci\u00f3n Convolutional Autoencoders.</p> <p>Como las secuencias de ADN son lineales, es decir, una cadena ordenada de nucle\u00f3tidos entonces utilizaremos <code>Conv1D</code> con el fin de detectar motivos locales (como patrones de splicing).</p> <p>Recordemos tambi\u00e9n que la \u00fanica forma de entrenar al componente encoder para que genere representaciones \u00fatiles es forzarlo a reconstruir la entrada original. Y es ah\u00ed donde interviene el decoder. Sin decoder, no hay aprendizaje del encoder, puesto que no se tendr\u00eda una m\u00e9trica para saber si la compresi\u00f3n conserva informaci\u00f3n \u00fatil. Una vez entrenado el decoder, ya podemos usar solo el encoder en los modelos de clasificaci\u00f3n posteriores.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#4-aplicacion-de-algoritmos","title":"4 Aplicaci\u00f3n de Algoritmos\u00b6","text":""},{"location":"machine-learning/classification-splice-junction_spanish/#41-implementar-el-modelo-knn-para-k-1-3-5-y-7","title":"4.1 Implementar el modelo kNN para k = 1, 3, 5 y 7\u00b6","text":"<p>Para generar las predicciones del modelo kNN en funci\u00f3n del valor de k, creamos una funci\u00f3n que implemente las siguientes tareas principales:</p> <ol> <li>Crear el modelo</li> <li>Entrenar el modelo</li> <li>Obtener predicciones con el modelo</li> <li>Calcular m\u00e9tricas del modelo</li> </ol> <p>La funci\u00f3n tomar\u00e1 como argumentos de entrada el valor de k, <code>X_train_encoded</code>, <code>y_train</code>, <code>X_test_encoded</code> y <code>y_test</code> y como salida proporcionar\u00e1 un dataframe de m\u00e9tricas. Comenzamos la creaci\u00f3n del modelo kNN para k=3.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#comentario-sobre-el-desempeno-del-modelo-knn","title":"Comentario sobre el desempe\u00f1o del modelo kNN\u00b6","text":"<p>La estrategia modelo que se seguir\u00e1 para evaluar los modelos es la siguiente:</p> <ul> <li>Observar f1-score ya que es la m\u00e9trica m\u00e1s equilibrada si hay un trate-off entre precision y recall especialmente en tareas de clasificaci\u00f3n multicateg\u00f3rica.</li> <li>Estudiar el error</li> <li>Evaluar el rendimiento del modelo por clase para asegurarnos de no optimizar la predicci\u00f3n de una clase en part\u00edcular, como ser\u00eda el caso de N.</li> </ul> <p>El modelo k-Nearest Neighbors (kNN) muestra una mejora progresiva del f1-score conforme aumenta el valor de k, lo que indica mayor robustez a medida que se consideran m\u00e1s vecinos. De forma inversa, el error disminuye, lo que respalda esta mejora en desempe\u00f1o.</p> <p>Al analizar el f1-score por clase, se nota que las clases IE y N tienen un rendimiento m\u00e1s consistente, mientras que el modelo tiene m\u00e1s dificultad para identificar correctamente la categor\u00eda EI. Finalmente, llama la atenci\u00f3n que la clase N presente el error m\u00e1s alto, lo que indica que el modelo a\u00fan puede seguir mejorando.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#42-implementar-el-modelo-naive-bayes","title":"4.2 Implementar el modelo Naive Bayes\u00b6","text":"<p>Para evaluar el rendimiento del modelo Naive Bayes seg\u00fan el tipo de datos en Python, se plantean dos enfoques. Primero, se emplea el clasificador BernoulliNB con suavizado de Laplace (alpha = 1, en Python) sobre datos binarios obtenidos mediante codificaci\u00f3n one-hot. A continuaci\u00f3n, se utiliza GaussianNB sobre datos continuos derivados del autoencoder convolucional, dado que este modelo asume una distribuci\u00f3n normal de las variables. Esta estrategia permitir\u00e1 comparar el impacto de distintas transformaciones de entrada en el desempe\u00f1o del clasificador Naive Bayes.</p> <p>Para mayor informaci\u00f3n sobre la implementaci\u00f3n del modelo Naive Bayes consultar BernoulliNB y GausssianNB.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#comentario-sobre-el-desempeno-del-modelo-naive-bayes","title":"Comentario sobre el desempe\u00f1o del modelo Naive Bayes\u00b6","text":"<p>Como se mencion\u00f3 al inicio de esta secci\u00f3n, se probaron dos variantes de Naive Bayes: BernoulliNB (para evaluar el suavizado de Laplace) y GaussianNB.</p> <p>El modelo BernoulliNB, que utiliz\u00f3 datos codificados mediante one-hot encoding, no logr\u00f3 resultados \u00fatiles. Esto se debe a que los datos binarios derivados de secuencias de ADN tienden a ser dispersos y altamente correlacionados, lo cual dificulta que el modelo capte patrones relevantes, especialmente considerando que BernoulliNB asume independencia entre caracter\u00edsticas.</p> <p>En contraste, el modelo GaussianNB que se aliment\u00f3 de los datos transformados por el encoder, mostr\u00f3 un rendimiento muy s\u00f3lido. Alcanz\u00f3 valores de f1-score altos y consistentes entre las tres clases, as\u00ed como errores muy bajos.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#43-implementar-el-modelo-artificial-neural-network","title":"4.3 Implementar el modelo Artificial Neural Network\u00b6","text":"<p>En esta secci\u00f3n se construye una red neuronal artificial cuya arquitectura contiene dos capas ocultas de 100 y p nodos, donde se explorar\u00e1 el n\u00famero de nodos p = 5, 10 y 20. Comenzamos el modelo para el valor de p = 20 y posteriormente escalaremos el n\u00famero de nodos.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#comentario-sobre-el-desempeno-del-modelo-ann","title":"Comentario sobre el desempe\u00f1o del modelo ANN\u00b6","text":"<p>El modelo de red neuronal artificial (ANN) presenta un rendimiento s\u00f3lido en general, con valores de f1-score altos y consistentes en todas las clases. A medida que se incrementa el n\u00famero de nodos de la 2da. capa oculta (p), el f1-score se sostiene aunque sin mejoras tan claras como las observadas en el modelo kNN respecto al valor de k.</p> <p>En relaci\u00f3n a las clases, la clase N tiene el mejor desempe\u00f1o con f1-scores superiores al 0.96 y errores m\u00ednimos. Y en desempe\u00f1o le sigue la clase EI, sin embargo, la clase IE presenta un aumento del error cuando p sube de 5 a 10, lo cual sugiere un posible sobreajuste.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#44-implementar-el-modelo-support-vector-machine","title":"4.4 Implementar el modelo Support Vector Machine\u00b6","text":"<p>En esta secci\u00f3n se construye un modelo de M\u00e1quina de Vector de Soporte con dos tipos de configuraciones: <code>kernel lineal</code> y <code>rbf</code>. Comenzamos con la implementaci\u00f3n del primero de ellos.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#comentario-sobre-el-desempeno-del-modelo-svm-linear-y-rbf","title":"Comentario sobre el desempe\u00f1o del modelo SVM (Linear y RBF)\u00b6","text":"<p>El modelo SVM con kernel lineal ofrece un rendimiento aceptable, con f1-scores por encima de 0.90 en todas las clases. El mejor desempe\u00f1o lo tiene la clase N, mientras que la clase IE es la que presenta m\u00e1s error.</p> <p>Por otro lado, el modelo SVM con kernel RBF presenta mejores f1-scores (&gt; 0.94), con errores considerablemente m\u00e1s bajos. El salto de calidad se nota especialmente en la clase EI e IE, mientras que N tambi\u00e9n mejora, con un rendimiento m\u00e1s equilibrado y consistente.</p> <p>En resumen, el modelo SVM con kernel RBF supera al lineal en todas las m\u00e9tricas relevantes, mostrando mejor generalizaci\u00f3n.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#45-implementar-el-modelo-arbol-de-clasificacion","title":"4.5 Implementar el modelo \u00c1rbol de Clasificaci\u00f3n\u00b6","text":"<p>En esta secci\u00f3n se construye un modelo de \u00e1rbol de clasificaci\u00f3n con dos tipos de configuraciones: <code>boosting</code> y <code>no boosting</code>.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#comentario-sobre-el-modelo-arbol-de-clasificacion","title":"Comentario sobre el modelo \u00c1rbol de Clasificaci\u00f3n\u00b6","text":"<p>El modelo \u00e1rbol de decisi\u00f3n con profundidad 5 presenta un rendimiento s\u00f3lido, con f1-scores superiores al 0.90 en todas las clases y errores bajos. Destaca especialmente la clase N, que mantiene un f1-score de 0.96, mientras que EI e IE muestran un comportamiento estable, aunque IE presenta un error mayor.</p> <p>Al aplicar Boosting sobre el \u00e1rbol, se observa una mejora clara en todas las m\u00e9tricas. El f1-score se incrementa ligeramente en todas las clases, alcanzando valores por encima de 0.94. Los errores tambi\u00e9n disminuyen, en especial para la clase IE.</p> <p>En resumen, el \u00c1rbol con Boosting ofrece un rendimiento m\u00e1s alto y m\u00e1s consistente. La combinaci\u00f3n de Boosting y \u00e1rboles poco profundos parece capturar mejor la estructura del problema sin sobreajustar.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#46-implementar-el-modelo-random-forest-con-n-50-y-100","title":"4.6 Implementar el modelo Random Forest con n = 50 y 100\u00b6","text":"<p>En esta secci\u00f3n se construye un modelo de Random Forest con dos n\u00fameros de \u00e1rboles: <code>n=50</code> y <code>n=100</code>.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#comentario-sobre-el-modelo-random-forest","title":"Comentario sobre el modelo Random Forest\u00b6","text":"<p>El modelo Random Forest con 50 \u00e1rboles muestra un rendimiento muy competitivo. Los f1-scores superan el 0.93 en todas las clases, destacando especialmente la clase N. Los errores son bajos y bastante equilibrados entre las clases, con excepci\u00f3n de la clase IE, lo que indica una buena capacidad de generalizaci\u00f3n.</p> <p>Al incrementar el n\u00famero de \u00e1rboles a 100, no se observan  mejoras. El f1-score se mantiene en todas las clases, y el error se mantiene bajo. Este comportamiento nos sugiere que podr\u00edamos aumentar la cantidad de \u00e1rboles para ayudar a estabilizar y afinar la predicci\u00f3n, aunque el desempe\u00f1o actual ya parece suficiente.</p>"},{"location":"machine-learning/classification-splice-junction_spanish/#5-comparacion-del-desempeno-de-los-modelos-propuestos","title":"5 Comparaci\u00f3n del desempe\u00f1o de los modelos propuestos\u00b6","text":""},{"location":"machine-learning/classification-splice-junction_spanish/#comentario-global-sobre-la-eleccion-de-modelos","title":"Comentario global sobre la elecci\u00f3n de modelos\u00b6","text":"<p>En el contexto de este proyecto, donde el objetivo es predecir sitios de splicing (clases EI e IE), es fundamental elegir modelos que no solo alcancen buenos f1-scores globales, sino que sean precisos en estas clases, sin confundir regiones de no splicing (clase N).</p> <p>Entre los modelos con mejor desempe\u00f1o para esta tarea encontramos SVM RBF, Random Forest (n&gt;50) y \u00c1rbol con Boosting, ya que ofrecen el mejor equilibrio entre las clases EI/IE sin sacrificar especificidad frente a la clase N.</p> <p>Los modelos ANN muestran tambi\u00e9n muy buenos resultados a  destacando por su f1-score alto y balanceado. No obstante, el modelo GaussianNB ofrece casi el mismo desempe\u00f1o, con una opci\u00f3n m\u00e1s liviana computacionalmente.</p> <p>Finalmente, los modelos kNN, aunque mejoran con valores de k mayores, los f1-score son menores a los de los otros modelos. Cabe mencionar que el modelo BernoulliNB se descart\u00f3 del todo ya que no pudo captura la complejidad de los datos.</p>"},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/","title":"Used Car Price Predictions Solution for the KaggleX Skill Assessment Challenge","text":"<p>by Salomon Marquez</p> <p>25/06/2024</p> <p>This notebook showcases my solution for the KaggleX Skill Assessment Challenge, a prerequisite to apply for the KaggleX Fellowship Program. In this competition, I ranked 146th out of 1,846 participants.</p> <p>My approach emphasized extensive feature engineering on the training dataset and augmenting it with additional data. For the model, I implemented a simple Multi-Layer Perceptron (MLP) to perform inference on the test dataset.</p> In\u00a0[59]: Copied! <pre># This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n</pre> # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load  import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory  import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename))  # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\"  # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session <pre>/kaggle/input/used-car-price-prediction-dataset-for-kagglex/sample_submission.csv\n/kaggle/input/used-car-price-prediction-dataset-for-kagglex/used_cars.csv\n/kaggle/input/used-car-price-prediction-dataset-for-kagglex/train.csv\n/kaggle/input/used-car-price-prediction-dataset-for-kagglex/test.csv\n</pre> In\u00a0[60]: Copied! <pre># Library Definition\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport re\nfrom pandas.plotting import scatter_matrix\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score\nimport category_encoders as ce\n\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\n</pre> # Library Definition import numpy as np  import pandas as pd import seaborn as sns import matplotlib.pyplot as plt  import re from pandas.plotting import scatter_matrix  from sklearn.preprocessing import OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, RobustScaler from sklearn.pipeline import make_pipeline from sklearn.compose import ColumnTransformer from sklearn.neural_network import MLPRegressor from sklearn.metrics import mean_squared_error from sklearn.model_selection import KFold, cross_val_score import category_encoders as ce  from scipy.stats import skew from scipy.special import boxcox1p In\u00a0[61]: Copied! <pre>train_df = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset-for-kagglex/train.csv')\ntest_df = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset-for-kagglex/test.csv')\ndev_df = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset-for-kagglex/used_cars.csv')\n</pre> train_df = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset-for-kagglex/train.csv') test_df = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset-for-kagglex/test.csv') dev_df = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset-for-kagglex/used_cars.csv') In\u00a0[62]: Copied! <pre># Function to extract numeric values from the mileage string\ndef extract_numeric_value(text):\n    # Define a pattern to match numeric values\n    pattern = r'[\\d]+'\n    # Use findall() function from the re package to extract all numeric values from the text and \n    # join them into a single string\n    return ''.join(re.findall(pattern, text))\n</pre> # Function to extract numeric values from the mileage string def extract_numeric_value(text):     # Define a pattern to match numeric values     pattern = r'[\\d]+'     # Use findall() function from the re package to extract all numeric values from the text and      # join them into a single string     return ''.join(re.findall(pattern, text)) In\u00a0[63]: Copied! <pre># Apply the function to the DataFrame column\ndev_df['milage'] = dev_df['milage'].apply(extract_numeric_value)\n\n# Convert the mileage column to integer type\ndev_df['milage'] = dev_df['milage'].astype(int)\n\n# Apply the function to the DataFrame column\ndev_df['price'] = dev_df['price'].apply(extract_numeric_value)\n\n# Convert the mileage column to integer type\ndev_df['price'] = dev_df['price'].astype(int)\n\n# Define new index\nstart =  len(train_df)\nend = len(train_df) + len(dev_df)\n\n# Insert ID column in the beginning of the dev dataframe\ndev_df.insert(0, 'id', range(start, end))\n\n# Vertical concatenation \ntrain2_df = pd.concat([train_df, dev_df])\n\n# Reset Indexes\ntrain2_df.reset_index(drop=True, inplace=True)\n</pre> # Apply the function to the DataFrame column dev_df['milage'] = dev_df['milage'].apply(extract_numeric_value)  # Convert the mileage column to integer type dev_df['milage'] = dev_df['milage'].astype(int)  # Apply the function to the DataFrame column dev_df['price'] = dev_df['price'].apply(extract_numeric_value)  # Convert the mileage column to integer type dev_df['price'] = dev_df['price'].astype(int)  # Define new index start =  len(train_df) end = len(train_df) + len(dev_df)  # Insert ID column in the beginning of the dev dataframe dev_df.insert(0, 'id', range(start, end))  # Vertical concatenation  train2_df = pd.concat([train_df, dev_df])  # Reset Indexes train2_df.reset_index(drop=True, inplace=True) In\u00a0[64]: Copied! <pre># Create brandmodel feature\ntrain2_df[\"brandmodel\"] = train2_df['brand']+ ' ' + train2_df['model']\ntrain2_df[['brand','model','brandmodel']].head(3)\n</pre> # Create brandmodel feature train2_df[\"brandmodel\"] = train2_df['brand']+ ' ' + train2_df['model'] train2_df[['brand','model','brandmodel']].head(3) Out[64]: brand model brandmodel 0 Ford F-150 Lariat Ford F-150 Lariat 1 BMW 335 i BMW 335 i 2 Jaguar XF Luxury Jaguar XF Luxury In\u00a0[65]: Copied! <pre># Extract  HoursePower, Engine Displacements, and Number of Cylinders\ndef get_engine_features(text):\n    \n    #Regular expresion pattern to extract engine values\n    hp_pattern = r'(\\d+\\.\\d+)HP' # HorsePower\n    ed_pattern = r'(\\d+\\.\\d+)L'  # Engine Displacement\n    cylinders_pattern = r'I(\\d+)|V(\\d+)|(\\d+) Cylinder' # Number of Cylinders\n    valves_pattern = r'(\\d+)V' # Number of Valves\n    \n    # Search for the patterns in the string\n    hp_match = re.search(hp_pattern, text)\n    ed_match = re.search(ed_pattern, text)\n    cylinders_match = re.search(cylinders_pattern, text)\n    valves_match = re.search(valves_pattern, text)\n    \n    # Extract and convert the matched values\n    if hp_match:\n        hp = float(hp_match.group(1))\n    else:\n        hp = np.nan\n\n    if ed_match:\n        ed = float(ed_match.group(1))\n    else:\n        ed = np.nan\n\n    if cylinders_match:\n        cylinders = int(cylinders_match.group(cylinders_match.lastindex))\n    else:\n        cylinders = np.nan\n    \n    if valves_match:\n        valves = int(valves_match.group(1))\n    else:\n        valves = np.nan\n        \n    return pd.Series({\n        'hp': hp,\n        'ed': ed,\n        'num_cylinders': cylinders,\n        'num_valves': valves\n    })    \n</pre> # Extract  HoursePower, Engine Displacements, and Number of Cylinders def get_engine_features(text):          #Regular expresion pattern to extract engine values     hp_pattern = r'(\\d+\\.\\d+)HP' # HorsePower     ed_pattern = r'(\\d+\\.\\d+)L'  # Engine Displacement     cylinders_pattern = r'I(\\d+)|V(\\d+)|(\\d+) Cylinder' # Number of Cylinders     valves_pattern = r'(\\d+)V' # Number of Valves          # Search for the patterns in the string     hp_match = re.search(hp_pattern, text)     ed_match = re.search(ed_pattern, text)     cylinders_match = re.search(cylinders_pattern, text)     valves_match = re.search(valves_pattern, text)          # Extract and convert the matched values     if hp_match:         hp = float(hp_match.group(1))     else:         hp = np.nan      if ed_match:         ed = float(ed_match.group(1))     else:         ed = np.nan      if cylinders_match:         cylinders = int(cylinders_match.group(cylinders_match.lastindex))     else:         cylinders = np.nan          if valves_match:         valves = int(valves_match.group(1))     else:         valves = np.nan              return pd.Series({         'hp': hp,         'ed': ed,         'num_cylinders': cylinders,         'num_valves': valves     })     In\u00a0[66]: Copied! <pre># Apply extraction function\ntrain2_df[['hp', 'ed', 'num_cylinders', 'num_valves']] = train2_df['engine'].apply(get_engine_features)\n\n# Plot new features\ntrain2_df[[\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].hist(bins=50, figsize=(6,4))\nplt.show()\n</pre> # Apply extraction function train2_df[['hp', 'ed', 'num_cylinders', 'num_valves']] = train2_df['engine'].apply(get_engine_features)  # Plot new features train2_df[[\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].hist(bins=50, figsize=(6,4)) plt.show() In\u00a0[67]: Copied! <pre># Function to transform transmission feature\ndef simple_transmission(text):\n    \n    # Patterns to identify\n    patterns = {\n        r'\\b(automatic|a/t|at)\\b':'automatic',\n        r'\\b(manual|m/t|mt)\\b':'manual',\n        r'\\b(variable|cvt)\\b':'cvt',\n        r'\\b(with|w/|at/mt)\\b':'manumatic'\n    }\n      \n    for pattern, replacement in patterns.items():\n        if re.search(pattern, text, flags=re.IGNORECASE):\n            return replacement\n    \n    return 'others'\n</pre> # Function to transform transmission feature def simple_transmission(text):          # Patterns to identify     patterns = {         r'\\b(automatic|a/t|at)\\b':'automatic',         r'\\b(manual|m/t|mt)\\b':'manual',         r'\\b(variable|cvt)\\b':'cvt',         r'\\b(with|w/|at/mt)\\b':'manumatic'     }            for pattern, replacement in patterns.items():         if re.search(pattern, text, flags=re.IGNORECASE):             return replacement          return 'others' In\u00a0[68]: Copied! <pre># Apply function \ntrain2_df['transmission'] = train2_df['transmission'].apply(simple_transmission)\n</pre> # Apply function  train2_df['transmission'] = train2_df['transmission'].apply(simple_transmission) In\u00a0[69]: Copied! <pre>train2_df.info()\n</pre> train2_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58282 entries, 0 to 58281\nData columns (total 18 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   id             58282 non-null  int64  \n 1   brand          58282 non-null  object \n 2   model          58282 non-null  object \n 3   model_year     58282 non-null  int64  \n 4   milage         58282 non-null  int64  \n 5   fuel_type      58112 non-null  object \n 6   engine         58282 non-null  object \n 7   transmission   58282 non-null  object \n 8   ext_col        58282 non-null  object \n 9   int_col        58282 non-null  object \n 10  accident       58169 non-null  object \n 11  clean_title    57686 non-null  object \n 12  price          58282 non-null  int64  \n 13  brandmodel     58282 non-null  object \n 14  hp             53415 non-null  float64\n 15  ed             57280 non-null  float64\n 16  num_cylinders  57081 non-null  float64\n 17  num_valves     4056 non-null   float64\ndtypes: float64(4), int64(4), object(10)\nmemory usage: 8.0+ MB\n</pre> In\u00a0[70]: Copied! <pre># Plot numerical features\ntrain2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].hist(bins=50, figsize=(8,8))\nplt.show()\n</pre> # Plot numerical features train2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].hist(bins=50, figsize=(8,8)) plt.show() In\u00a0[71]: Copied! <pre># Plot scatter matrix\nscatter_matrix(train2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]], figsize=(8,8))\nplt.show()\n</pre> # Plot scatter matrix scatter_matrix(train2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]], figsize=(8,8)) plt.show() In\u00a0[72]: Copied! <pre># Check the skew of all numerical features\nskewed_feats = train2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].apply(lambda x: skew(x.dropna()))\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n</pre> # Check the skew of all numerical features skewed_feats = train2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].apply(lambda x: skew(x.dropna())) print(\"\\nSkew in numerical features: \\n\") skewness = pd.DataFrame({'Skew' :skewed_feats}) skewness.head(10) <pre>\nSkew in numerical features: \n\n</pre> Out[72]: Skew model_year -0.951024 milage 0.874905 hp 0.661737 ed 0.525690 num_cylinders 0.195700 num_valves 36.235544 In\u00a0[73]: Copied! <pre># Reduce skeweness \ntrain2_df['milage']= train2_df['milage'].apply(lambda x: np.sqrt(x))\n\nlambda_ = 0.15\ntrain2_df['hp']= boxcox1p(train2_df['hp'], lambda_)\ntrain2_df['ed']= boxcox1p(train2_df['ed'], lambda_)\n</pre> # Reduce skeweness  train2_df['milage']= train2_df['milage'].apply(lambda x: np.sqrt(x))  lambda_ = 0.15 train2_df['hp']= boxcox1p(train2_df['hp'], lambda_) train2_df['ed']= boxcox1p(train2_df['ed'], lambda_) In\u00a0[74]: Copied! <pre># percentile \n#model_year_quantile = train2_df[\"model_year\"].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n#model_year_quantile\n</pre> # percentile  #model_year_quantile = train2_df[\"model_year\"].quantile([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) #model_year_quantile In\u00a0[75]: Copied! <pre>train2_df['model_year'] = pd.cut(train2_df['model_year'],\n                                        bins=[1974, 2007, 2011, 2013, 2015, 2016, 2018, 2019, 2020, 2021, 2024],\n                                        labels=[2007, 2011, 2013, 2015, 2016, 2018, 2019, 2020, 2021, 2024])\n</pre> train2_df['model_year'] = pd.cut(train2_df['model_year'],                                         bins=[1974, 2007, 2011, 2013, 2015, 2016, 2018, 2019, 2020, 2021, 2024],                                         labels=[2007, 2011, 2013, 2015, 2016, 2018, 2019, 2020, 2021, 2024]) In\u00a0[76]: Copied! <pre># percentile \n#model_year_quantile = train2_df[\"model_year\"].quantile([0.0, 0.25, 0.5, 0.75, 1.0])\n\n#train2_df['model_year'] = pd.cut(train2_df['model_year'],\n#                                        bins=[1974, 2012, 2016, 2019, 2024],\n#                                        labels=[2012, 2016, 2019, 2024])\n\n#train2_df[\"model_year_skew\"].hist(bins=10, figsize=(6,2))\n#plt.show()\n</pre> # percentile  #model_year_quantile = train2_df[\"model_year\"].quantile([0.0, 0.25, 0.5, 0.75, 1.0])  #train2_df['model_year'] = pd.cut(train2_df['model_year'], #                                        bins=[1974, 2012, 2016, 2019, 2024], #                                        labels=[2012, 2016, 2019, 2024])  #train2_df[\"model_year_skew\"].hist(bins=10, figsize=(6,2)) #plt.show() In\u00a0[77]: Copied! <pre># Check the skew of all numerical features\nskewed_box = train2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].apply(lambda x: skew(x.dropna()))\nprint(\"\\nSkew in numerical features: \\n\")\nskewness_box = pd.DataFrame({'Skew' :skewed_box})\nskewness_box.head(10)\n</pre> # Check the skew of all numerical features skewed_box = train2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].apply(lambda x: skew(x.dropna())) print(\"\\nSkew in numerical features: \\n\") skewness_box = pd.DataFrame({'Skew' :skewed_box}) skewness_box.head(10) <pre>\nSkew in numerical features: \n\n</pre> Out[77]: Skew model_year -0.292276 milage -0.080750 hp -0.140137 ed 0.094202 num_cylinders 0.195700 num_valves 36.235544 In\u00a0[78]: Copied! <pre># Plot numerical features\ntrain2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].hist(bins=50, figsize=(8,8))\nplt.show()\n</pre> # Plot numerical features train2_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].hist(bins=50, figsize=(8,8)) plt.show() In\u00a0[79]: Copied! <pre>train2_df.info()\n</pre> train2_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58282 entries, 0 to 58281\nData columns (total 18 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   id             58282 non-null  int64   \n 1   brand          58282 non-null  object  \n 2   model          58282 non-null  object  \n 3   model_year     58276 non-null  category\n 4   milage         58282 non-null  float64 \n 5   fuel_type      58112 non-null  object  \n 6   engine         58282 non-null  object  \n 7   transmission   58282 non-null  object  \n 8   ext_col        58282 non-null  object  \n 9   int_col        58282 non-null  object  \n 10  accident       58169 non-null  object  \n 11  clean_title    57686 non-null  object  \n 12  price          58282 non-null  int64   \n 13  brandmodel     58282 non-null  object  \n 14  hp             53415 non-null  float64 \n 15  ed             57280 non-null  float64 \n 16  num_cylinders  57081 non-null  float64 \n 17  num_valves     4056 non-null   float64 \ndtypes: category(1), float64(5), int64(2), object(10)\nmemory usage: 7.6+ MB\n</pre> In\u00a0[80]: Copied! <pre>from sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor \n</pre> from sklearn.decomposition import PCA from sklearn.feature_selection import mutual_info_regression from sklearn.model_selection import cross_val_score from xgboost import XGBRegressor  In\u00a0[81]: Copied! <pre>def apply_pca(X, standardize=True):\n    # Standardize \n    if standardize:\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    \n    # Create principal components \n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    \n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns = component_names)\n    \n    # Creating loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,\n        columns=component_names,\n        index=X.columns    \n    )\n    return pca, X_pca, loadings\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n+1)\n    \n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)    \n    )\n    \n    # Cumulative Variance \n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)    \n    )\n    # Set up figure \n    fig.set(figwidth=width, dpi=dpi)\n    return axs \n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score       \n</pre> def apply_pca(X, standardize=True):     # Standardize      if standardize:         X = (X - X.mean(axis=0)) / X.std(axis=0)          # Create principal components      pca = PCA()     X_pca = pca.fit_transform(X)          # Convert to dataframe     component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]     X_pca = pd.DataFrame(X_pca, columns = component_names)          # Creating loadings     loadings = pd.DataFrame(         pca.components_.T,         columns=component_names,         index=X.columns         )     return pca, X_pca, loadings  def plot_variance(pca, width=8, dpi=100):     # Create figure     fig, axs = plt.subplots(1, 2)     n = pca.n_components_     grid = np.arange(1, n+1)          # Explained variance     evr = pca.explained_variance_ratio_     axs[0].bar(grid, evr)     axs[0].set(         xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)         )          # Cumulative Variance      cv = np.cumsum(evr)     axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")     axs[1].set(         xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)         )     # Set up figure      fig.set(figwidth=width, dpi=dpi)     return axs   def make_mi_scores(X, y):     X = X.copy()     for colname in X.select_dtypes([\"object\", \"category\"]):         X[colname], _ = X[colname].factorize()     # All discrete features should now have integer dtypes     discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)     mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)     mi_scores = mi_scores.sort_values(ascending=False)     return mi_scores   def score_dataset(X, y, model=XGBRegressor()):     # Label encoding for categoricals     for colname in X.select_dtypes([\"category\", \"object\"]):         X[colname], _ = X[colname].factorize()     # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)     score = cross_val_score(         model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",     )     score = -1 * score.mean()     score = np.sqrt(score)     return score        In\u00a0[82]: Copied! <pre>features = [\n    \"milage\",\n    \"model_year\",\n    \"hp\",\n    \"ed\",\n    \"num_cylinders\"\n]\n\nprint(\"Correlation with Price:\\n\")\nprint(train2_df[features].corrwith(train2_df.price))\n</pre> features = [     \"milage\",     \"model_year\",     \"hp\",     \"ed\",     \"num_cylinders\" ]  print(\"Correlation with Price:\\n\") print(train2_df[features].corrwith(train2_df.price)) <pre>Correlation with Price:\n\nmilage          -0.285471\nmodel_year       0.230884\nhp               0.256449\ned               0.111935\nnum_cylinders    0.143659\ndtype: float64\n</pre> In\u00a0[83]: Copied! <pre>X = train2_df.copy()\ny = X.pop(\"price\")\nX = X.loc[:, features]\nX['model_year']= X['model_year'].astype('float64')\n</pre> X = train2_df.copy() y = X.pop(\"price\") X = X.loc[:, features] X['model_year']= X['model_year'].astype('float64') In\u00a0[84]: Copied! <pre>X.info()\n</pre> X.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58282 entries, 0 to 58281\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   milage         58282 non-null  float64\n 1   model_year     58276 non-null  float64\n 2   hp             53415 non-null  float64\n 3   ed             57280 non-null  float64\n 4   num_cylinders  57081 non-null  float64\ndtypes: float64(5)\nmemory usage: 2.2 MB\n</pre> In\u00a0[85]: Copied! <pre>X.head(3)\n</pre> X.head(3) Out[85]: milage model_year hp ed num_cylinders 0 272.670130 2018.0 9.558416 1.687259 6.0 1 282.842712 2007.0 9.025890 1.540963 6.0 2 302.474792 2011.0 9.025890 1.870411 8.0 In\u00a0[86]: Copied! <pre># Create a pipeline for doing imputation and standard scaling \nnum_pipeline  = make_pipeline(SimpleImputer(strategy=\"median\"), RobustScaler())\npreprocessing = ColumnTransformer([(\"num\", num_pipeline, features)])\n</pre> # Create a pipeline for doing imputation and standard scaling  num_pipeline  = make_pipeline(SimpleImputer(strategy=\"median\"), RobustScaler()) preprocessing = ColumnTransformer([(\"num\", num_pipeline, features)]) In\u00a0[87]: Copied! <pre># Fit and transform the data\nX_scaled = preprocessing.fit_transform(X)\n\n# Create a new dataframe for scaled data\nX_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n</pre> # Fit and transform the data X_scaled = preprocessing.fit_transform(X)  # Create a new dataframe for scaled data X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns) In\u00a0[88]: Copied! <pre>X_scaled_df.head(3)\n</pre> X_scaled_df.head(3) Out[88]: milage model_year hp ed num_cylinders 0 0.124850 0.333333 0.487727 0.000000 0.0 1 0.196527 -1.500000 -0.082593 -0.344326 0.0 2 0.334855 -0.833333 -0.082593 0.431070 1.0 In\u00a0[89]: Copied! <pre># Apply pca\npca, X_pca, loadings = apply_pca(X_scaled_df)\nprint(loadings)\n</pre> # Apply pca pca, X_pca, loadings = apply_pca(X_scaled_df) print(loadings) <pre>                    PC1       PC2       PC3       PC4       PC5\nmilage        -0.263490  0.614072 -0.742920  0.004903  0.039187\nmodel_year     0.223742 -0.644155 -0.610470 -0.395939  0.074582\nhp             0.551986 -0.065634 -0.251091  0.783558 -0.118297\ned             0.528445  0.327686  0.046116 -0.428746 -0.653772\nnum_cylinders  0.544582  0.310313  0.101114 -0.213125  0.742624\n</pre> In\u00a0[90]: Copied! <pre># Explained variance \nplot_variance(pca);\n</pre> # Explained variance  plot_variance(pca); In\u00a0[91]: Copied! <pre>sns.catplot(\n    y=\"value\",\n    col=\"variable\",\n    data=X_pca.melt(),\n    kind='boxen',\n    sharey=False,\n    col_wrap=3\n);\n</pre> sns.catplot(     y=\"value\",     col=\"variable\",     data=X_pca.melt(),     kind='boxen',     sharey=False,     col_wrap=3 );  <pre>/opt/conda/lib/python3.10/site-packages/seaborn/categorical.py:1794: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/categorical.py:1794: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/categorical.py:1794: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/categorical.py:1794: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/categorical.py:1794: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n</pre> In\u00a0[92]: Copied! <pre>train2_df.info()\n</pre> train2_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58282 entries, 0 to 58281\nData columns (total 18 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   id             58282 non-null  int64   \n 1   brand          58282 non-null  object  \n 2   model          58282 non-null  object  \n 3   model_year     58276 non-null  category\n 4   milage         58282 non-null  float64 \n 5   fuel_type      58112 non-null  object  \n 6   engine         58282 non-null  object  \n 7   transmission   58282 non-null  object  \n 8   ext_col        58282 non-null  object  \n 9   int_col        58282 non-null  object  \n 10  accident       58169 non-null  object  \n 11  clean_title    57686 non-null  object  \n 12  price          58282 non-null  int64   \n 13  brandmodel     58282 non-null  object  \n 14  hp             53415 non-null  float64 \n 15  ed             57280 non-null  float64 \n 16  num_cylinders  57081 non-null  float64 \n 17  num_valves     4056 non-null   float64 \ndtypes: category(1), float64(5), int64(2), object(10)\nmemory usage: 7.6+ MB\n</pre> In\u00a0[93]: Copied! <pre># Choose a component PC1, PC2, PC3, or PC4\ncomponent = \"PC5\"\n\nidx = X_pca[component].sort_values(ascending=False).index\ntrain2_df.loc[idx, [\"price\",\"brandmodel\",\"fuel_type\",\"transmission\",\"ext_col\",\"int_col\",\"accident\",\"clean_title\"] + features]\n</pre> # Choose a component PC1, PC2, PC3, or PC4 component = \"PC5\"  idx = X_pca[component].sort_values(ascending=False).index train2_df.loc[idx, [\"price\",\"brandmodel\",\"fuel_type\",\"transmission\",\"ext_col\",\"int_col\",\"accident\",\"clean_title\"] + features] Out[93]: price brandmodel fuel_type transmission ext_col int_col accident clean_title milage model_year hp ed num_cylinders 56486 289991 Bentley Continental GT GT Speed Gasoline automatic Granite Porpoise None reported NaN 43.806392 2024 NaN NaN 12.0 19906 16500 BMW i3 94 Ah Gasoline automatic Gray Black None reported Yes 288.097206 2016 7.749778 0.520063 NaN 57741 16500 BMW i3 94 Ah NaN automatic Black \u2013 None reported Yes 207.364414 2018 7.749778 0.520063 NaN 57363 26990 BMW i3 120Ah w/Range Extender NaN automatic Black Black None reported Yes 130.288142 2019 7.749778 0.520063 NaN 11599 11499 BMW i3 Base w/Range Extender Gasoline automatic White Black None reported Yes 272.946881 2015 7.749778 0.520063 NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... 42944 6699 RAM 2500 Big Horn Diesel automatic White Gray None reported Yes 311.448230 2007 9.391827 2.388206 6.0 48383 25500 Dodge Ram 2500 Laramie Quad Cab Diesel automatic Blue Beige None reported Yes 193.963914 2007 9.391827 2.388206 6.0 52980 69995 RAM 3500 Tradesman Diesel automatic Blue Gray None reported Yes 54.680892 2007 9.715842 2.388206 6.0 48311 1950995 Bugatti Veyron 16.4 Grand Sport Gasoline automatic White Black None reported Yes 79.561297 2011 NaN 2.602594 NaN 54502 1950995 Bugatti Veyron 16.4 Grand Sport Gasoline automatic White White None reported Yes 79.561297 2011 NaN 2.602594 NaN <p>58282 rows \u00d7 13 columns</p> In\u00a0[94]: Copied! <pre>train2_df.info()\n</pre> train2_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58282 entries, 0 to 58281\nData columns (total 18 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   id             58282 non-null  int64   \n 1   brand          58282 non-null  object  \n 2   model          58282 non-null  object  \n 3   model_year     58276 non-null  category\n 4   milage         58282 non-null  float64 \n 5   fuel_type      58112 non-null  object  \n 6   engine         58282 non-null  object  \n 7   transmission   58282 non-null  object  \n 8   ext_col        58282 non-null  object  \n 9   int_col        58282 non-null  object  \n 10  accident       58169 non-null  object  \n 11  clean_title    57686 non-null  object  \n 12  price          58282 non-null  int64   \n 13  brandmodel     58282 non-null  object  \n 14  hp             53415 non-null  float64 \n 15  ed             57280 non-null  float64 \n 16  num_cylinders  57081 non-null  float64 \n 17  num_valves     4056 non-null   float64 \ndtypes: category(1), float64(5), int64(2), object(10)\nmemory usage: 7.6+ MB\n</pre> In\u00a0[95]: Copied! <pre># Drop undesired columns \ncar_labels = train2_df['price']\ncar_labels = car_labels.to_frame()\n\n# Drop undesired features\ntrain2_df.drop(['price','id','brand','model','engine','num_valves'], axis=1, inplace=True)\n#train2_df.drop(['price','id','brand','model','engine','hp','ed','num_valves','num_cylinders'], axis=1, inplace=True)\n</pre> # Drop undesired columns  car_labels = train2_df['price'] car_labels = car_labels.to_frame()  # Drop undesired features train2_df.drop(['price','id','brand','model','engine','num_valves'], axis=1, inplace=True) #train2_df.drop(['price','id','brand','model','engine','hp','ed','num_valves','num_cylinders'], axis=1, inplace=True) In\u00a0[96]: Copied! <pre>train2_df.info()\n</pre> train2_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58282 entries, 0 to 58281\nData columns (total 12 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   model_year     58276 non-null  category\n 1   milage         58282 non-null  float64 \n 2   fuel_type      58112 non-null  object  \n 3   transmission   58282 non-null  object  \n 4   ext_col        58282 non-null  object  \n 5   int_col        58282 non-null  object  \n 6   accident       58169 non-null  object  \n 7   clean_title    57686 non-null  object  \n 8   brandmodel     58282 non-null  object  \n 9   hp             53415 non-null  float64 \n 10  ed             57280 non-null  float64 \n 11  num_cylinders  57081 non-null  float64 \ndtypes: category(1), float64(4), object(7)\nmemory usage: 4.9+ MB\n</pre> In\u00a0[97]: Copied! <pre># Total imputation \ntrain2_df = train2_df.replace('\u2013', np.nan)\n</pre> # Total imputation  train2_df = train2_df.replace('\u2013', np.nan) In\u00a0[98]: Copied! <pre># Run this cell for joining the additional features from the PCA analysis\n#train2_df = train2_df.join(X_pca)\n</pre> # Run this cell for joining the additional features from the PCA analysis #train2_df = train2_df.join(X_pca) In\u00a0[99]: Copied! <pre>train2_df.info()\n</pre> train2_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58282 entries, 0 to 58281\nData columns (total 12 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   model_year     58276 non-null  category\n 1   milage         58282 non-null  float64 \n 2   fuel_type      57773 non-null  object  \n 3   transmission   58282 non-null  object  \n 4   ext_col        58226 non-null  object  \n 5   int_col        57104 non-null  object  \n 6   accident       58169 non-null  object  \n 7   clean_title    57686 non-null  object  \n 8   brandmodel     58282 non-null  object  \n 9   hp             53415 non-null  float64 \n 10  ed             57280 non-null  float64 \n 11  num_cylinders  57081 non-null  float64 \ndtypes: category(1), float64(4), object(7)\nmemory usage: 4.9+ MB\n</pre> In\u00a0[103]: Copied! <pre># Processing Pipeline\n#num_features = ['model_year', 'milage','hp','ed','num_cylinders','PC1','PC2','PC3','PC4']\nnum_features = ['model_year', 'milage','hp','ed','num_cylinders']\ncat_features = ['brandmodel','fuel_type','transmission','ext_col', 'int_col','accident','clean_title']\n\nnum_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), RobustScaler())\ncat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder(handle_unknown='ignore'))\n\npreprocessing = ColumnTransformer([\n    (\"num\", num_pipeline, num_features),\n    (\"cat\", cat_pipeline, cat_features)])\n</pre> # Processing Pipeline #num_features = ['model_year', 'milage','hp','ed','num_cylinders','PC1','PC2','PC3','PC4'] num_features = ['model_year', 'milage','hp','ed','num_cylinders'] cat_features = ['brandmodel','fuel_type','transmission','ext_col', 'int_col','accident','clean_title']  num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), RobustScaler()) cat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder(handle_unknown='ignore'))  preprocessing = ColumnTransformer([     (\"num\", num_pipeline, num_features),     (\"cat\", cat_pipeline, cat_features)]) In\u00a0[104]: Copied! <pre># MLP Model\nmlp_reg = MLPRegressor(\n    hidden_layer_sizes = [8,32,8],   # [8,32,8], [8,32,64,32,8],[8,64,8]\n    activation ='relu', \n    solver = 'adam',\n    alpha = 0.001,\n    batch_size = 64,  # 32,64,128\n    learning_rate = 'constant',\n    max_iter = 400,\n    shuffle = True,\n    early_stopping = True, \n    validation_fraction = 0.1,\n    random_state=42)\npipeline = make_pipeline(preprocessing, mlp_reg)\n</pre> # MLP Model mlp_reg = MLPRegressor(     hidden_layer_sizes = [8,32,8],   # [8,32,8], [8,32,64,32,8],[8,64,8]     activation ='relu',      solver = 'adam',     alpha = 0.001,     batch_size = 64,  # 32,64,128     learning_rate = 'constant',     max_iter = 400,     shuffle = True,     early_stopping = True,      validation_fraction = 0.1,     random_state=42) pipeline = make_pipeline(preprocessing, mlp_reg) In\u00a0[105]: Copied! <pre>pipeline.fit(train2_df, car_labels)\n</pre> pipeline.fit(train2_df, car_labels) <pre>/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n</pre> Out[105]: <pre>Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('robustscaler',\n                                                                   RobustScaler())]),\n                                                  ['model_year', 'milage', 'hp',\n                                                   'ed', 'num_cylinders']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['brandmodel', 'fuel_type',\n                                                   'transmission', 'ext_col',\n                                                   'int_col', 'accident',\n                                                   'clean_title'])])),\n                ('mlpregressor',\n                 MLPRegressor(alpha=0.001, batch_size=64, early_stopping=True,\n                              hidden_layer_sizes=[8, 32, 8], max_iter=400,\n                              random_state=42))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('robustscaler',\n                                                                   RobustScaler())]),\n                                                  ['model_year', 'milage', 'hp',\n                                                   'ed', 'num_cylinders']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['brandmodel', 'fuel_type',\n                                                   'transmission', 'ext_col',\n                                                   'int_col', 'accident',\n                                                   'clean_title'])])),\n                ('mlpregressor',\n                 MLPRegressor(alpha=0.001, batch_size=64, early_stopping=True,\n                              hidden_layer_sizes=[8, 32, 8], max_iter=400,\n                              random_state=42))])</pre>columntransformer: ColumnTransformer<pre>ColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('robustscaler',\n                                                  RobustScaler())]),\n                                 ['model_year', 'milage', 'hp', 'ed',\n                                  'num_cylinders']),\n                                ('cat',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['brandmodel', 'fuel_type', 'transmission',\n                                  'ext_col', 'int_col', 'accident',\n                                  'clean_title'])])</pre>num<pre>['model_year', 'milage', 'hp', 'ed', 'num_cylinders']</pre>SimpleImputer<pre>SimpleImputer(strategy='median')</pre>RobustScaler<pre>RobustScaler()</pre>cat<pre>['brandmodel', 'fuel_type', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']</pre>SimpleImputer<pre>SimpleImputer(strategy='most_frequent')</pre>OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore')</pre>MLPRegressor<pre>MLPRegressor(alpha=0.001, batch_size=64, early_stopping=True,\n             hidden_layer_sizes=[8, 32, 8], max_iter=400, random_state=42)</pre> In\u00a0[106]: Copied! <pre># Define K-Fold Cross Validator\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Perform cross-validation\nbaseline_scores = -cross_val_score(pipeline, train2_df, car_labels, scoring='neg_root_mean_squared_error', cv=kf)\nprint(\"Cross-Validation RMSE scores:\", baseline_scores)\nprint(\"Average RMSE:\", np.mean(baseline_scores))\n</pre> # Define K-Fold Cross Validator kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Perform cross-validation baseline_scores = -cross_val_score(pipeline, train2_df, car_labels, scoring='neg_root_mean_squared_error', cv=kf) print(\"Cross-Validation RMSE scores:\", baseline_scores) print(\"Average RMSE:\", np.mean(baseline_scores)) <pre>/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n</pre> <pre>Cross-Validation RMSE scores: [60193.71277415 78966.61767177 77424.15910457 48134.62743974\n 72502.42776201]\nAverage RMSE: 67444.30895044885\n</pre> In\u00a0[107]: Copied! <pre>pd.Series(baseline_scores).describe()\n</pre> pd.Series(baseline_scores).describe() Out[107]: <pre>count        5.000000\nmean     67444.308950\nstd      13070.773870\nmin      48134.627440\n25%      60193.712774\n50%      72502.427762\n75%      77424.159105\nmax      78966.617672\ndtype: float64</pre> In\u00a0[108]: Copied! <pre>pipeline.get_feature_names_out\n</pre> pipeline.get_feature_names_out Out[108]: <pre>&lt;bound method Pipeline.get_feature_names_out of Pipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('robustscaler',\n                                                                   RobustScaler())]),\n                                                  ['model_year', 'milage', 'hp',\n                                                   'ed', 'num_cylinders']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['brandmodel', 'fuel_type',\n                                                   'transmission', 'ext_col',\n                                                   'int_col', 'accident',\n                                                   'clean_title'])])),\n                ('mlpregressor',\n                 MLPRegressor(alpha=0.001, batch_size=64, early_stopping=True,\n                              hidden_layer_sizes=[8, 32, 8], max_iter=400,\n                              random_state=42))])&gt;</pre> <p>count        5.000000 mean     67444.308950 std      13070.773870 min      48134.627440 25%      60193.712774 50%      72502.427762 75%      77424.159105 max      78966.617672 dtype: float64</p> In\u00a0[109]: Copied! <pre># Predict train2_df prices 67277.4197309128 | 67308.44019387408 | 67231.14714140317\ncar_price_prediction = pipeline.predict(train2_df)\n\n# Model evaluation\nlm_rmse = mean_squared_error(car_labels, car_price_prediction, squared=False)\nlm_rmse\n</pre> # Predict train2_df prices 67277.4197309128 | 67308.44019387408 | 67231.14714140317 car_price_prediction = pipeline.predict(train2_df)  # Model evaluation lm_rmse = mean_squared_error(car_labels, car_price_prediction, squared=False) lm_rmse Out[109]: <pre>67454.88485800906</pre> In\u00a0[121]: Copied! <pre>test_df = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset-for-kagglex/test.csv')\n</pre> test_df = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset-for-kagglex/test.csv') In\u00a0[122]: Copied! <pre># Create brandmodel feature\ntest_df[\"brandmodel\"] = test_df['brand']+ ' ' + test_df['model']\n\n# Create engine additional features\ntest_df[['hp', 'ed', 'num_cylinders', 'num_valves']] = test_df['engine'].apply(get_engine_features)\n\n# Create transmission features \ntest_df['transmission'] = test_df['transmission'].apply(simple_transmission)\n</pre> # Create brandmodel feature test_df[\"brandmodel\"] = test_df['brand']+ ' ' + test_df['model']  # Create engine additional features test_df[['hp', 'ed', 'num_cylinders', 'num_valves']] = test_df['engine'].apply(get_engine_features)  # Create transmission features  test_df['transmission'] = test_df['transmission'].apply(simple_transmission) In\u00a0[123]: Copied! <pre># Replace milage with square root\ntest_df['milage']= test_df['milage'].apply(lambda x: np.sqrt(x))\n\nlambda_ = 0.15\ntest_df['hp']= boxcox1p(test_df['hp'], lambda_)\ntest_df['ed']= boxcox1p(test_df['ed'], lambda_)\n\ntest_df['model_year'] = pd.cut(test_df['model_year'],\n                                        bins=[1974, 2007, 2011, 2013, 2015, 2016, 2018, 2019, 2020, 2021, 2024],\n                                        labels=[2007, 2011, 2013, 2015, 2016, 2018, 2019, 2020, 2021, 2024])\n</pre> # Replace milage with square root test_df['milage']= test_df['milage'].apply(lambda x: np.sqrt(x))  lambda_ = 0.15 test_df['hp']= boxcox1p(test_df['hp'], lambda_) test_df['ed']= boxcox1p(test_df['ed'], lambda_)  test_df['model_year'] = pd.cut(test_df['model_year'],                                         bins=[1974, 2007, 2011, 2013, 2015, 2016, 2018, 2019, 2020, 2021, 2024],                                         labels=[2007, 2011, 2013, 2015, 2016, 2018, 2019, 2020, 2021, 2024]) In\u00a0[\u00a0]: Copied! <pre># Replacing model_year with quantiles\n#test_df['model_year'] = pd.cut(test_df['model_year'],\n#                                        bins=[1974, 2012, 2016, 2019, 2024],\n#                                        labels=[2012, 2016, 2019, 2024])\n</pre> # Replacing model_year with quantiles #test_df['model_year'] = pd.cut(test_df['model_year'], #                                        bins=[1974, 2012, 2016, 2019, 2024], #                                        labels=[2012, 2016, 2019, 2024]) In\u00a0[124]: Copied! <pre># Plot model_year with quantiles\ntest_df[\"model_year\"].hist(bins=10, figsize=(6,2))\nplt.show()\n</pre> # Plot model_year with quantiles test_df[\"model_year\"].hist(bins=10, figsize=(6,2)) plt.show() In\u00a0[125]: Copied! <pre># Plot numerical features\ntest_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].hist(bins=50, figsize=(8,8))\nplt.show()\n</pre> # Plot numerical features test_df[[\"model_year\", \"milage\",\"hp\", \"ed\", \"num_cylinders\", \"num_valves\"]].hist(bins=50, figsize=(8,8)) plt.show() In\u00a0[126]: Copied! <pre># Remove undesired features\nindex_test = test_df['id']\nindex_test = index_test.to_frame()\ntest_df.drop(['id','engine','brand','model','num_valves'], axis=1, inplace=True)\n#test_df.drop(['id','brand','model','engine','hp','ed','num_valves','num_cylinders'], axis=1, inplace=True)\n</pre> # Remove undesired features index_test = test_df['id'] index_test = index_test.to_frame() test_df.drop(['id','engine','brand','model','num_valves'], axis=1, inplace=True) #test_df.drop(['id','brand','model','engine','hp','ed','num_valves','num_cylinders'], axis=1, inplace=True)  In\u00a0[127]: Copied! <pre># Total imputation \ntest_df = test_df.replace('\u2013', np.nan)\n</pre> # Total imputation  test_df = test_df.replace('\u2013', np.nan) In\u00a0[128]: Copied! <pre>test_df.info()\n</pre> test_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 36183 entries, 0 to 36182\nData columns (total 12 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   model_year     36182 non-null  category\n 1   milage         36183 non-null  float64 \n 2   fuel_type      35986 non-null  object  \n 3   transmission   36183 non-null  object  \n 4   ext_col        36156 non-null  object  \n 5   int_col        35479 non-null  object  \n 6   accident       36183 non-null  object  \n 7   clean_title    36183 non-null  object  \n 8   brandmodel     36183 non-null  object  \n 9   hp             33577 non-null  float64 \n 10  ed             35778 non-null  float64 \n 11  num_cylinders  35679 non-null  float64 \ndtypes: category(1), float64(4), object(7)\nmemory usage: 3.1+ MB\n</pre> In\u00a0[\u00a0]: Copied! <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58282 entries, 0 to 58281\nData columns (total 12 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   model_year     58276 non-null  category\n 1   milage         58282 non-null  float64 \n 2   fuel_type      57773 non-null  object  \n 3   transmission   58282 non-null  object  \n 4   ext_col        58226 non-null  object  \n 5   int_col        57104 non-null  object  \n 6   accident       58169 non-null  object  \n 7   clean_title    57686 non-null  object  \n 8   brandmodel     58282 non-null  object  \n 9   hp             53415 non-null  float64 \n 10  ed             57280 non-null  float64 \n 11  num_cylinders  57081 non-null  float64 \ndtypes: category(1), float64(4), object(7)\nmemory usage: 4.9+ MB\n</pre>  RangeIndex: 58282 entries, 0 to 58281 Data columns (total 12 columns):  #   Column         Non-Null Count  Dtype    ---  ------         --------------  -----     0   model_year     58276 non-null  category  1   milage         58282 non-null  float64   2   fuel_type      57773 non-null  object    3   transmission   58282 non-null  object    4   ext_col        58226 non-null  object    5   int_col        57104 non-null  object    6   accident       58169 non-null  object    7   clean_title    57686 non-null  object    8   brandmodel     58282 non-null  object    9   hp             53415 non-null  float64   10  ed             57280 non-null  float64   11  num_cylinders  57081 non-null  float64  dtypes: category(1), float64(4), object(7) memory usage: 4.9+ MB In\u00a0[134]: Copied! <pre># Make predictions\n#car_price_test_prediction = np.exp(pipeline.predict(test_df))\ncar_price_test_prediction = pipeline.predict(test_df)\ncar_price_test_prediction_df = pd.DataFrame(car_price_test_prediction, columns=['price'])\nresults = pd.concat([index_test, car_price_test_prediction_df], axis=1)\n</pre> # Make predictions #car_price_test_prediction = np.exp(pipeline.predict(test_df)) car_price_test_prediction = pipeline.predict(test_df) car_price_test_prediction_df = pd.DataFrame(car_price_test_prediction, columns=['price']) results = pd.concat([index_test, car_price_test_prediction_df], axis=1) In\u00a0[135]: Copied! <pre>results.head(3)\n</pre> results.head(3) Out[135]: id price 0 54273 30081.982962 1 54274 19885.575096 2 54275 27483.291429 In\u00a0[136]: Copied! <pre># Save results to a csv file\nresults.to_csv('submission39-REPOFsubmission22.csv', index=False)\n</pre> # Save results to a csv file results.to_csv('submission39-REPOFsubmission22.csv', index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#used-car-price-predictions-solution-for-the-kagglex-skill-assessment-challenge","title":"Used Car Price Predictions Solution for the KaggleX Skill Assessment Challenge\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#loading-datasets","title":"Loading Datasets\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#joining-train-and-dev-datasets","title":"Joining Train and Dev Datasets\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#feature-engineering","title":"Feature Engineering\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#categorical-features","title":"Categorical Features\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#numerical-features","title":"Numerical Features\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#principal-component-analysis","title":"Principal Component Analysis\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#dropping-undesired-features","title":"Dropping undesired features\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#builiding-a-pipeline","title":"Builiding a Pipeline\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#model-implementation","title":"Model Implementation\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#score-a-baseline","title":"Score a Baseline\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#evaluate-model-on-rmse","title":"Evaluate Model on RMSE\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#test-dataset","title":"Test Dataset\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#feature-engineering-of-categorical-features","title":"Feature Engineering of Categorical Features\u00b6","text":""},{"location":"machine-learning/used-car-predictions-for-kagglex-competition/#feature-engineering-of-numerical-features","title":"Feature Engineering of Numerical Features\u00b6","text":""},{"location":"natural-language-processing/a-journey-through-text-data-competitions/","title":"A Journey Through Kaggle Text Data Competitions From 2021 to 2023","text":"Authors: Liliana Badillo and Salomon Marquez <p>01/07/2023</p> \u201cIt is estimated that 90% of the world\u2019s data was generated in the last two years alone\u201d by Amount of Data Created Daily (2023) <p>Natural Language Processing (NLP), the branch of artificial intelligence that enables computers to understand human language, has witnessed remarkable growth and transformative advancements in recent years. This surge has been driven by the increasing availability of vast amounts of text data and the rapid development of sophisticated machine learning techniques.</p> <p>Kaggle is an online platform that hosts machine learning and data science competitions. Among these competitions, text data competitions specifically focus on tasks related to NLP and text analysis. Participants are provided with text datasets and are tasked with developing models and algorithms to solve language-related challenges such as sentiment analysis, text classification, and machine translation. These competitions are instrumental in advancing NLP techniques and provide participants with valuable experience and knowledge exchange opportunities.</p> <p>Accessing and learning about the latest techniques employed in text data Kaggle competitions can be challenging due to their dispersion within discussions and winning solution write-ups. Valuable information about these cutting-edge approaches tends to get buried, making it difficult to stay updated on the evolving landscape of text data techniques.</p> <p>In this essay, our objective is to offer valuable insights into key lessons learned by the Kaggle community during their engagement in text data competitions spanning from 2021 to 2023. We will focus on four pivotal topics that have emerged as the foundation for successful solutions in recent years: model architectures, pseudo labeling, adversarial weight perturbation, and mask language modeling. By examining these four key areas, we aim to provide a comprehensive understanding of the essential elements driving winning strategies in text data competitions.</p> In\u00a0[66]: Copied! <pre>import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%config InlineBackend.figure_format = 'retina'\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n</pre> import numpy as np  import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  %config InlineBackend.figure_format = 'retina'  import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename)) <pre>/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\n/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json\n/kaggle/input/2023-kaggle-ai-report/kaggle_writeups_20230510.csv\n/kaggle/input/top-3-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport - Text Data Write-ups 27.csv\n/kaggle/input/top-3-kaggle-text-data-competitions-2021-2023/Summary_27write-ups.csv\n/kaggle/input/top-3-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport .csv\n/kaggle/input/faq-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport - FAQ.csv\n</pre> In\u00a0[67]: Copied! <pre>import matplotlib.dates as mdates\nfrom datetime import datetime\n\nnames = ['U.S. Patent Phrase to Phrase Matching',\n         'Google AI4Code \u2013 Understand Code in Python Notebooks', \n         'Feedback Prize \u2014 Predicting Effective Arguments', \n         'NBME \u2014 Score Clinical Patient Notes', \n         'Feedback Prize \u2014 Evaluating Student Writing', \n         'Jigsaw Rate Severity of Toxic Comments',\n         'chaii \u2014 Hindi and Tamil Question Answering', \n         'CommonLit Readability Prize', \n         'Coleridge Initiative \u2014 Show US the Data']\n\ndates = ['2022-03-21', \n         '2022-05-11', \n         '2022-05-24', \n         '2022-02-01',\n         '2021-12-14', \n         '2021-11-08', \n         '2021-08-11', \n         '2021-05-03',\n         '2021-03-23']\n\n# Convert date strings (e.g. 2014-10-18) to datetime\ndates = [datetime.strptime(d, \"%Y-%m-%d\") for d in dates]\n</pre> import matplotlib.dates as mdates from datetime import datetime  names = ['U.S. Patent Phrase to Phrase Matching',          'Google AI4Code \u2013 Understand Code in Python Notebooks',           'Feedback Prize \u2014 Predicting Effective Arguments',           'NBME \u2014 Score Clinical Patient Notes',           'Feedback Prize \u2014 Evaluating Student Writing',           'Jigsaw Rate Severity of Toxic Comments',          'chaii \u2014 Hindi and Tamil Question Answering',           'CommonLit Readability Prize',           'Coleridge Initiative \u2014 Show US the Data']  dates = ['2022-03-21',           '2022-05-11',           '2022-05-24',           '2022-02-01',          '2021-12-14',           '2021-11-08',           '2021-08-11',           '2021-05-03',          '2021-03-23']  # Convert date strings (e.g. 2014-10-18) to datetime dates = [datetime.strptime(d, \"%Y-%m-%d\") for d in dates] In\u00a0[68]: Copied! <pre># Choose some nice levels\nlevels = np.tile([-5, 5, -3, 3, -1, 1],\n                 int(np.ceil(len(dates)/6)))[:len(dates)]\n\n# Create figure and plot a stem plot with the date\nfig, ax = plt.subplots(figsize=(10, 6), layout=\"constrained\")\n#ax.set(title=\"Kaggle text data competitions from 2021 to 2023\")\n\nax.vlines(dates, 0, levels, color=\"tab:blue\")  # The vertical stems.\nax.plot(dates, np.zeros_like(dates), \"-o\",\n        color=\"k\", markerfacecolor=\"w\")  # Baseline and markers on it.\n\n# annotate lines\nfor d, l, r in zip(dates, levels, names):\n    ax.annotate(r, xy=(d, l),\n                xytext=(-3, np.sign(l)*3), textcoords=\"offset points\",\n                horizontalalignment=\"right\",\n                verticalalignment=\"bottom\" if l &gt; 0 else \"top\")\n\n# format x-axis with 2-month intervals\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\nplt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")\n\n# remove y-axis and spines\nax.yaxis.set_visible(False)\nax.spines[[\"left\", \"top\", \"right\"]].set_visible(False)\n\nax.margins(y=0.1)\n%config InlineBackend.figure_format = 'retina'\nplt.show()\n</pre> # Choose some nice levels levels = np.tile([-5, 5, -3, 3, -1, 1],                  int(np.ceil(len(dates)/6)))[:len(dates)]  # Create figure and plot a stem plot with the date fig, ax = plt.subplots(figsize=(10, 6), layout=\"constrained\") #ax.set(title=\"Kaggle text data competitions from 2021 to 2023\")  ax.vlines(dates, 0, levels, color=\"tab:blue\")  # The vertical stems. ax.plot(dates, np.zeros_like(dates), \"-o\",         color=\"k\", markerfacecolor=\"w\")  # Baseline and markers on it.  # annotate lines for d, l, r in zip(dates, levels, names):     ax.annotate(r, xy=(d, l),                 xytext=(-3, np.sign(l)*3), textcoords=\"offset points\",                 horizontalalignment=\"right\",                 verticalalignment=\"bottom\" if l &gt; 0 else \"top\")  # format x-axis with 2-month intervals ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1)) ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\")) plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\")  # remove y-axis and spines ax.yaxis.set_visible(False) ax.spines[[\"left\", \"top\", \"right\"]].set_visible(False)  ax.margins(y=0.1) %config InlineBackend.figure_format = 'retina' plt.show() Figure 1. Kaggle text data competitions held from 2021 to 2023 In\u00a0[69]: Copied! <pre>import re\n\ndef clean_text(df, col_to_clean):\n\n    # Remove html tags\n    df['cleaned_text'] = df[col_to_clean].apply(lambda x: re.sub('&lt;[^&lt;]+?&gt;', '', x))\n\n    # Remove special characters\n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"\\n+\", \"\", regex=True).str.replace(\"-\", \" \", regex=False).str.replace(\"[^a-zA-Z0-9 ]\", \"\", regex=True)\n\n    # Lowercase text\n    df['cleaned_text'] = df['cleaned_text'].str.lower()\n    \n    return df\n\ndef count_occurrences_in_dataframe(df, column_name, strings_list):\n    # Convert the list of strings to a set for faster membership checking\n    strings_set = set(strings_list)\n    \n    # Filter the dataframe to include only rows where the text column contains any of the strings\n    filtered_df = df[df[column_name].str.contains('|'.join(strings_set))]\n    \n    # Use value_counts() to count the occurrences of each string\n    counts = filtered_df[column_name].value_counts()\n    \n    # Create a dictionary to store the results\n    results_dict = {'String': [], 'Occurrences': [], 'Actual Occurrences': []}\n    \n    # Iterate over the strings list\n    for string in strings_list:\n        # Add the string and its corresponding count to the dictionary\n        results_dict['String'].append(string)\n        results_dict['Occurrences'].append(counts.get(string, 0))\n        \n        # Count the actual occurrences in the filtered dataframe\n        actual_occurrences = filtered_df[column_name].str.count(string).sum()\n        results_dict['Actual Occurrences'].append(actual_occurrences)\n    \n    # Convert the dictionary to a dataframe\n    counts_df = pd.DataFrame(results_dict)\n    \n    return counts_df\n\ndf_arxiv = pd.read_json(\"/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json\", \n                     lines=True,\n                     convert_dates=True,\n                     chunksize=100000,\n                    )\n\nnlp_related_categories_arxiv = [\n    \"cs.IR\", \"cs.CL\", \"cs.LG\", \"cs.NE\",\n    \"cs.AI\", \"cs.DL\", \"cs.HC\", \"cs.SI\",\n    \"cs.SD\", \"cs.WEB\", \"stat.ML\", \"math.IT\"\n]\n\nnlp_related_categories = pd.DataFrame()\n\nfor chunk in df_arxiv:\n    chunk_nlp_papers = chunk[chunk.categories.isin(nlp_related_categories_arxiv)]\n    nlp_related_categories = pd.concat([nlp_related_categories, chunk_nlp_papers], ignore_index=True)\n      \nnlp_related_categories['update_date']= pd.to_datetime(nlp_related_categories['update_date'])\n\nnlp_related_categories = nlp_related_categories[(nlp_related_categories['update_date'].dt.year &gt; 2020) &amp; (nlp_related_categories['update_date'].dt.month &gt; 4)].copy()\n\nnlp_related_categories = nlp_related_categories.reset_index(drop=True)\n\ntext_data_keywords = [\n     \"text mining\",\n     \"text analytics\",\n     \"text preprocessing\",\n     \"text classification\",\n     \"text clustering\",\n     \"text matching\",\n     \"sentiment analysis\",\n     \"named entity recognition\",\n     \"topic modeling\",\n     \"information retrieval\",\n     \"text summarization\",\n     \"text generation\",\n     \"text similarity\",\n     \"word embeddings\",\n     \"document classification\",\n     \"text feature extraction\",\n     \"text segmentation\",\n     \"text corpora\",\n     \"textual data analysis\",\n     \"question answering\",\n     \"sentiment analysis\",\n     \"language modeling\"\n]\n\nnlp_related_categories = clean_text(nlp_related_categories, 'abstract')\n\nresult = count_occurrences_in_dataframe(nlp_related_categories, 'cleaned_text', text_data_keywords)\n\nsorted_result = result.sort_values('Actual Occurrences', ascending=True)\n</pre> import re  def clean_text(df, col_to_clean):      # Remove html tags     df['cleaned_text'] = df[col_to_clean].apply(lambda x: re.sub('&lt;[^&lt;]+?&gt;', '', x))      # Remove special characters     df['cleaned_text'] = df['cleaned_text'].str.replace(\"\\n+\", \"\", regex=True).str.replace(\"-\", \" \", regex=False).str.replace(\"[^a-zA-Z0-9 ]\", \"\", regex=True)      # Lowercase text     df['cleaned_text'] = df['cleaned_text'].str.lower()          return df  def count_occurrences_in_dataframe(df, column_name, strings_list):     # Convert the list of strings to a set for faster membership checking     strings_set = set(strings_list)          # Filter the dataframe to include only rows where the text column contains any of the strings     filtered_df = df[df[column_name].str.contains('|'.join(strings_set))]          # Use value_counts() to count the occurrences of each string     counts = filtered_df[column_name].value_counts()          # Create a dictionary to store the results     results_dict = {'String': [], 'Occurrences': [], 'Actual Occurrences': []}          # Iterate over the strings list     for string in strings_list:         # Add the string and its corresponding count to the dictionary         results_dict['String'].append(string)         results_dict['Occurrences'].append(counts.get(string, 0))                  # Count the actual occurrences in the filtered dataframe         actual_occurrences = filtered_df[column_name].str.count(string).sum()         results_dict['Actual Occurrences'].append(actual_occurrences)          # Convert the dictionary to a dataframe     counts_df = pd.DataFrame(results_dict)          return counts_df  df_arxiv = pd.read_json(\"/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json\",                       lines=True,                      convert_dates=True,                      chunksize=100000,                     )  nlp_related_categories_arxiv = [     \"cs.IR\", \"cs.CL\", \"cs.LG\", \"cs.NE\",     \"cs.AI\", \"cs.DL\", \"cs.HC\", \"cs.SI\",     \"cs.SD\", \"cs.WEB\", \"stat.ML\", \"math.IT\" ]  nlp_related_categories = pd.DataFrame()  for chunk in df_arxiv:     chunk_nlp_papers = chunk[chunk.categories.isin(nlp_related_categories_arxiv)]     nlp_related_categories = pd.concat([nlp_related_categories, chunk_nlp_papers], ignore_index=True)        nlp_related_categories['update_date']= pd.to_datetime(nlp_related_categories['update_date'])  nlp_related_categories = nlp_related_categories[(nlp_related_categories['update_date'].dt.year &gt; 2020) &amp; (nlp_related_categories['update_date'].dt.month &gt; 4)].copy()  nlp_related_categories = nlp_related_categories.reset_index(drop=True)  text_data_keywords = [      \"text mining\",      \"text analytics\",      \"text preprocessing\",      \"text classification\",      \"text clustering\",      \"text matching\",      \"sentiment analysis\",      \"named entity recognition\",      \"topic modeling\",      \"information retrieval\",      \"text summarization\",      \"text generation\",      \"text similarity\",      \"word embeddings\",      \"document classification\",      \"text feature extraction\",      \"text segmentation\",      \"text corpora\",      \"textual data analysis\",      \"question answering\",      \"sentiment analysis\",      \"language modeling\" ]  nlp_related_categories = clean_text(nlp_related_categories, 'abstract')  result = count_occurrences_in_dataframe(nlp_related_categories, 'cleaned_text', text_data_keywords)  sorted_result = result.sort_values('Actual Occurrences', ascending=True) In\u00a0[71]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\n\nlabels = ['text classification', 'information extraction', 'question and answer', 'text matching']\nsizes = [5, 2, 1, 1]\n\n# Sort the labels and sizes in reverse order\nlabels_sorted, sizes_sorted = zip(*sorted(zip(labels, sizes), key=lambda x: x[1], reverse=False))\n\nfig, ax = plt.subplots(figsize=(8, 2),layout=\"constrained\")\nax.barh(labels_sorted, sizes_sorted)\nax.set_xlabel('Occurrences',fontweight='bold');\nax.set_ylabel('Domain', fontweight='bold');\n#ax.set_title('a) Domains in Kaggle text data competitions');\n</pre> %config InlineBackend.figure_format = 'retina'  labels = ['text classification', 'information extraction', 'question and answer', 'text matching'] sizes = [5, 2, 1, 1]  # Sort the labels and sizes in reverse order labels_sorted, sizes_sorted = zip(*sorted(zip(labels, sizes), key=lambda x: x[1], reverse=False))  fig, ax = plt.subplots(figsize=(8, 2),layout=\"constrained\") ax.barh(labels_sorted, sizes_sorted) ax.set_xlabel('Occurrences',fontweight='bold'); ax.set_ylabel('Domain', fontweight='bold'); #ax.set_title('a) Domains in Kaggle text data competitions');  Figure 2. Main text related domains found in Kaggle text data competitions from 2021 to 2023 In\u00a0[73]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\n\nfig, ax = plt.subplots(figsize=(8, 5),layout=\"constrained\")\n\nax.barh(sorted_result[\"String\"], sorted_result['Actual Occurrences'])\nax.set_xlabel('Occurrences',fontweight='bold');\nax.set_ylabel('Domain', fontweight='bold');\n#ax.set_title('Text data keywords mentioned in Arxiv papers');\n</pre> %config InlineBackend.figure_format = 'retina'  fig, ax = plt.subplots(figsize=(8, 5),layout=\"constrained\")  ax.barh(sorted_result[\"String\"], sorted_result['Actual Occurrences']) ax.set_xlabel('Occurrences',fontweight='bold'); ax.set_ylabel('Domain', fontweight='bold'); #ax.set_title('Text data keywords mentioned in Arxiv papers'); Figure 3. Main text related domains found in Arxiv papers from 2021 to 2023 <p>In the four following sections, we focus our discussion on these topics:</p> <ul> <li>Model Architecture</li> <li>Adversarial Weight Perturbation (AWP)</li> <li>Pseudo Labeling</li> <li>Masked Language Modeling (MLM)</li> </ul> <p>Our motivation for discussing these four topics is based on two aspects. Firstly, we believe that they were the most impactful strategies used to improve the accuracy of the proposed models among the winning solutions. We identified that the ML community showed great interest in these strategies, as evidenced by the numerous questions found in the comments section of the write-ups regarding their implementation. Secondly, we identified that Kagglers relied on various scientific articles related to these four strategies, which we believe could be of interest to the ML community for a better understanding and implementation of these strategies while developing their solutions.</p> In\u00a0[74]: Copied! <pre>kw_text_data_architectures = [\n    \"fasttext\", \"roberta\", \"bert\",\n    \"gpt\", \"rnn\", \"cnn\", \"gru\",\n    \"t5\", \"electra\", \"xlnet\",\n    \"lstm\", \"deberta\", \"codebert\"\n]\n\ntop_text_data_architectures = [\n    \"bert\", \"gpt\", \"xlnet\",\n    \"roberta\", \"albert\", \"t5\",\n    \"electra\", \"deberta\", \"palm\",\n    \"structbert\"\n]\n\narxiv_text_data_architectures = list(set(top_text_data_architectures).union(set(kw_text_data_architectures)))\n\nresult = count_occurrences_in_dataframe(nlp_related_categories, 'cleaned_text', arxiv_text_data_architectures)\nsorted_result = result.sort_values('Actual Occurrences', ascending=True)\n</pre> kw_text_data_architectures = [     \"fasttext\", \"roberta\", \"bert\",     \"gpt\", \"rnn\", \"cnn\", \"gru\",     \"t5\", \"electra\", \"xlnet\",     \"lstm\", \"deberta\", \"codebert\" ]  top_text_data_architectures = [     \"bert\", \"gpt\", \"xlnet\",     \"roberta\", \"albert\", \"t5\",     \"electra\", \"deberta\", \"palm\",     \"structbert\" ]  arxiv_text_data_architectures = list(set(top_text_data_architectures).union(set(kw_text_data_architectures)))  result = count_occurrences_in_dataframe(nlp_related_categories, 'cleaned_text', arxiv_text_data_architectures) sorted_result = result.sort_values('Actual Occurrences', ascending=True) In\u00a0[75]: Copied! <pre>model_arch = [\n    'deberta-v3-large', 'deberta-large', 'roberta-base', 'roberta-large',\n    'deberta-v2-xlarge', 'deberta-xlarge', 'muril-large-cased', 'anferico/bert-for-patents',\n    'albert-xxlarge-v2', 'bert-base-uncased', 'rembert', 'funnel-transformer/large-base',\n    'longformer', 'funnel-transformer/large', 'longformer-large-4096', 'deberta-v2-xxlarge',\n    'electra-large-discriminator', 't5-large', 'xlm-roberta-large-squad2',\n    'AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru', 'microsoft/infoxlm-large',\n    'gpt2', 'electra-base-discriminator', 'muppet-roberta-large', 'xlm-roberta-large',\n    'sshleifer/distilbart-cnn-12-6', 'microsoft/mpnet-base', 'gpt2-medium', 'bart-large',\n    'bigbird-roberta-base', 'xlm-roberta-base', 'distilroberta-base', 'Yanhao/simcse-bert-for-patent',\n    'electra-large', 'deberta-v1', 'deberta-v1-xlarge', 'codebert-base',\n    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2', 'luke', 'deberta-base',\n    'deberta-v2-large', 'distilbart-mnli-12-9', 'bart-large-finetuned-squadv1',\n    'ccdv-ai/convert_checkpoint_to_lsg', 'ahotrod/electra_large_discriminator_squad2_512',\n    'uw-madison/yoso-4096'\n]\n\noccurrences = [\n    12, 11, 7, 5, 5, 4, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n]\n\ndata = {\n    'Model architecture': model_arch,\n    'Occurrence': occurrences\n}\n\narch_df = pd.DataFrame(data)\nsorted_df = arch_df.sort_values('Occurrence', ascending=True)\n</pre> model_arch = [     'deberta-v3-large', 'deberta-large', 'roberta-base', 'roberta-large',     'deberta-v2-xlarge', 'deberta-xlarge', 'muril-large-cased', 'anferico/bert-for-patents',     'albert-xxlarge-v2', 'bert-base-uncased', 'rembert', 'funnel-transformer/large-base',     'longformer', 'funnel-transformer/large', 'longformer-large-4096', 'deberta-v2-xxlarge',     'electra-large-discriminator', 't5-large', 'xlm-roberta-large-squad2',     'AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru', 'microsoft/infoxlm-large',     'gpt2', 'electra-base-discriminator', 'muppet-roberta-large', 'xlm-roberta-large',     'sshleifer/distilbart-cnn-12-6', 'microsoft/mpnet-base', 'gpt2-medium', 'bart-large',     'bigbird-roberta-base', 'xlm-roberta-base', 'distilroberta-base', 'Yanhao/simcse-bert-for-patent',     'electra-large', 'deberta-v1', 'deberta-v1-xlarge', 'codebert-base',     'sentence-transformers/paraphrase-multilingual-mpnet-base-v2', 'luke', 'deberta-base',     'deberta-v2-large', 'distilbart-mnli-12-9', 'bart-large-finetuned-squadv1',     'ccdv-ai/convert_checkpoint_to_lsg', 'ahotrod/electra_large_discriminator_squad2_512',     'uw-madison/yoso-4096' ]  occurrences = [     12, 11, 7, 5, 5, 4, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ]  data = {     'Model architecture': model_arch,     'Occurrence': occurrences }  arch_df = pd.DataFrame(data) sorted_df = arch_df.sort_values('Occurrence', ascending=True) In\u00a0[76]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\n\nfig, ax = plt.subplots(figsize=(8, 10),layout=\"constrained\")\n\n# Plot 1 - Set labels and title\nax.barh(sorted_df[\"Model architecture\"], sorted_df['Occurrence'])\nax.set_xlabel('Occurrences',fontweight='bold');\nax.set_ylabel('Model architectures', fontweight='bold');\n#ax.set_title('Model architectures used in Kaggle text data competitions');\n</pre> %config InlineBackend.figure_format = 'retina'  fig, ax = plt.subplots(figsize=(8, 10),layout=\"constrained\")  # Plot 1 - Set labels and title ax.barh(sorted_df[\"Model architecture\"], sorted_df['Occurrence']) ax.set_xlabel('Occurrences',fontweight='bold'); ax.set_ylabel('Model architectures', fontweight='bold'); #ax.set_title('Model architectures used in Kaggle text data competitions');  Figure 3. Main model architectures used in Kaggle text data competitions from 2021 to 2023 In\u00a0[77]: Copied! <pre>%config InlineBackend.figure_format = 'retina'\n\nfig, ax = plt.subplots(figsize=(8, 5),layout=\"constrained\")\n\n# Plot 2 - Set labels and title\nax.barh(sorted_result[\"String\"], sorted_result['Actual Occurrences'])\nax.set_xlabel('Occurrences',fontweight='bold');\nax.set_ylabel('Model architectures', fontweight='bold');\n#ax.set_title('Model architectures mentioned in Arxiv papers')\n\n# Rotate the x-axis labels if needed\nplt.xticks(rotation=90);\n</pre> %config InlineBackend.figure_format = 'retina'  fig, ax = plt.subplots(figsize=(8, 5),layout=\"constrained\")  # Plot 2 - Set labels and title ax.barh(sorted_result[\"String\"], sorted_result['Actual Occurrences']) ax.set_xlabel('Occurrences',fontweight='bold'); ax.set_ylabel('Model architectures', fontweight='bold'); #ax.set_title('Model architectures mentioned in Arxiv papers')  # Rotate the x-axis labels if needed plt.xticks(rotation=90); Figure 4. Main model architectures mentioned in arXiv papers from 2021 to 2023 <p>Regarding traditional methods based on recurrent neural networks (RNN) such as LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit), the ML community has continued to use them due to their recognized efficiency in retaining important information for longer periods and their ability to discard irrelevant information. Remarkably, their utilization has not been isolated but as an integral part of transformer-based model architectures through intermediate learning layers or heads. For instance, some architectures have included a single bidirectional RNN layer with LSTM or GRU between the backbone and three heads of a model [4]. Alternatively, LSTM has been simply added on top of BERT [5], [6] and with linear attention before the pooling stage of the Fully Connected (FC) layer [7]. Other architectures have benefited from integrating a two-layer GRU model before the predictions of each token [8], or incorporating a single attention head with its own learning rate instead of a GRU multi-attention head [9]. Overall, it has been demonstrated that adding LSTM or GRU after the pooling stage can stabilize training [10], or improve model performance by 1.5% when LSTM is used for code encoding [11]. As Figure 4 indicates, whereas traditional approaches like CNN and LSTM still hold their relevance and are actively used for research, on Kaggle, these approaches are often combined with transformer-based architectures.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\ndf_writeups = pd.read_csv('/kaggle/input/top-3-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport - Text Data Write-ups 27.csv')\ndisplay(df_writeups)\n</pre> import pandas as pd df_writeups = pd.read_csv('/kaggle/input/top-3-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport - Text Data Write-ups 27.csv') display(df_writeups) In\u00a0[\u00a0]: Copied! <pre>df_faq = pd.read_csv('/kaggle/input/faq-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport - FAQ.csv')\ndisplay(df_faq)\n</pre> df_faq = pd.read_csv('/kaggle/input/faq-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport - FAQ.csv') display(df_faq) In\u00a0[\u00a0]: Copied! <pre>submission = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\")\nsubmission.loc[0]['value'] = 'Kaggle competitions'\nsubmission.loc[1]['value'] = 'https://www.kaggle.com/code/sophieb/ai-report-text-data-competitions'\nsubmission.loc[2]['value'] = 'https://www.kaggle.com/code/kashnitsky/essay-competition-cards-recent-nlp-competitions/comments#2333491'\nsubmission.loc[3]['value'] = 'https://www.kaggle.com/code/nibeditasahu/kaggle-ai-report-2023/comments#2334882'\nsubmission.loc[4]['value'] = 'https://www.kaggle.com/code/datamafia7/a-text-odyssey-the-past-present-and-future-of-nlp/comments#2336611'\nsubmission.head()\n</pre> submission = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/sample_submission.csv\") submission.loc[0]['value'] = 'Kaggle competitions' submission.loc[1]['value'] = 'https://www.kaggle.com/code/sophieb/ai-report-text-data-competitions' submission.loc[2]['value'] = 'https://www.kaggle.com/code/kashnitsky/essay-competition-cards-recent-nlp-competitions/comments#2333491' submission.loc[3]['value'] = 'https://www.kaggle.com/code/nibeditasahu/kaggle-ai-report-2023/comments#2334882' submission.loc[4]['value'] = 'https://www.kaggle.com/code/datamafia7/a-text-odyssey-the-past-present-and-future-of-nlp/comments#2336611' submission.head() In\u00a0[\u00a0]: Copied! <pre>submission.to_csv('submission.csv', index=False)\n</pre> submission.to_csv('submission.csv', index=False)"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#a-journey-through-kaggle-text-data-competitions-from-2021-to-2023","title":"A Journey Through Kaggle Text Data Competitions From 2021 to 2023\u00b6","text":""},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#methodology","title":"Methodology\u00b6","text":"<p>To gain a better understanding into the text data landscape, we carried out the following tasks:</p> <ul> <li><p>Analize 27 write-ups from nine Kaggle competitions specifically related to text data. We dove deeper into the top three solutions for text data competitions held from 2021 to 2023, see Figure 1. Also, we explore a range of topics, including the prevalent techniques and methodologies employed, the most effective model architectures utilized, and the challenges encountered and overcame. This analysis enabled us to gain deeper insights into the problem-solving approaches and thought processes employed by Kagglers in developing their successful solutions.</p> </li> <li><p>Perform a general but not exhaustive analysis to the arXiv dataset. We filter out categories with the potential to include NLP papers published within the past two years. By leveraging a predefined list of keywords associated with text data, we directed our efforts towards uncovering the specific problem domains and top leading model architectures [1][2] that have garnered attention within the research community.</p> </li> </ul>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#key-discoveries-of-text-data-competitions","title":"Key Discoveries of Text Data Competitions\u00b6","text":"<p>The result of the analysis of the 27 write-ups and their most notable comments can be seen in Table I of Appendix A and Table II of Appendix B, respectively. Table I provides information about the top three winning solutions in each Kaggle competition related to text, highlighting aspects such as model architectures, parameters like PB and LB, optimizers, ensembles, and the most innovative proposed ideas, among others. On the other hand, Table II presents a summary of comments from the write-ups in a question-like format. Additionally, the analysis of the arXiv dataset has allowed us to unveil, on a larger scale, how the use of certain model architectures and their application to specific text domains compares to the solutions presented in text-oriented Kaggle competitions over the past two years. For more details, please refer to EDA Kaggle and Arxiv datasets notebook.</p> <p>As Figure 2 shows, out of the nine competitions, five focused on text classification tasks, two aimed at information extraction tasks, and the remaining targeted text matching and question-and-answer tasks. Interestingly, Figure 3 shows that with a frequency of the term \"question answering\" exceeding 400 times, it can be inferred that the scientific community has placed greater importance on domains related to this field. While Kaggle competitions have only hosted one question-and-answer competition, there seems to be a stronger relationship between Kaggle competitions and research topics related to text classification (over 200 ocurrences in arVix dataset). Especifically, text generation has become a highly prominent topic within the research community over the past two years. Meanwhile, tasks like sentiment analysis, text classification, information retrieval, and text summarization continue to be trending topics.</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#model-architecture","title":"Model Architecture\u00b6","text":"<p>Despite the proliferation of GPT models, it is evident that the ML community has opted to rely on the performance of BERT-based models. As Figure 3 shows, the top three Kaggle text data solutions from 2021 to 2023 predominantly utilize five architectures: deberta-v3-large, deberta-large, roberta-base, roberta-large, and deberta-v2-xlarge. Out of the 27 analyzed solutions, only one approach [3] from the Coleridge Initiative \u2014 Show US the Data competition employed causal Language Models (CLM) like GPT-2 to ensure that their model comprehends the context rather than just the target text. The underlying motivation behind this decision is that contextual models such as BERT can sometimes disregard the context itself in their quest to locate and match the exact target words, which can lead to overfitting. Similarly, Figure 4 indicates that the term BERT has gained significant attention within the scientific community, with over 1,750 mentions in arXiv papers, surpassing the over 300 mentions of the term GPT.</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#model-ensembling","title":"Model Ensembling\u00b6","text":"<p>Model ensembling has been one of the most diversified and in doubt topics within the ML community according to the Table II of Appendix B. This is because its implementation depends not only on the Kaggler\u2019s level of expertise and creativity but also on the time available before the competition deadline. In this regard, some solutions have relied on their best model without generating any ensembles [[3], [5], [12]\u2013[14]], while others have implemented ensembles of up to 38 models [15]. Weighted average has been the default ensemble technique as shown in solutions [7]\u2013[9], [16], [17]. Other approaches have used weighted average in combination with other strategies such as ridge regression [15], character-level predictions [17], probability distributions [11], and token type probabilities [4]. In second place, the strategy of out-of-fold predictions is found [6], [19], [20]. The third strategy is blending weights [21] either using Optuna to determine weights [22] or by applying word-level majority voting across backbones for the final submission [23]. Finally, the fourth option is a stacked generalization ensemble using linear regression [24] or a gradually stacking approach that involves assembling models by assigning the maximum logit of tokens for each character, applying the softmax function at the character level, and finally averaging all models [25].</p> <p>On the other hand, other solutions have explored alternative ways of determining the weights of their models, pushing the boundaries of creativity. An example of this is a solution [26], which promotes diversity through a 19-model ensemble and generates model weights using the Nelder-Mead simplex algorithm. Interestingly, they removed the weight restriction of 1 and considered negative weights. Alternatively, other approaches utilized genetic algorithms [16] or the <code>gp_minimize</code> package to generate optimal weights [10]. An innovative solution [27] proposed a Weighted Box Fusion (WBF) method to create a 10-model ensemble. For models with the same architecture but different folds, their strategy involved averaging token probabilities, whereas for models with different architectures, they employed the WBF method.</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#computational-efficiency","title":"Computational Efficiency\u00b6","text":"<p>Finally, it is important to discuss the computational efficiency of models used in Kaggle competitions over the past two years. In the pursuit of highly accurate models, considerations such as prediction time and computational capabilities often get overlooked. For instance, in the Google AI4Code - Understand Code in Python Notebooks competition, one solution managed to improve its LB score through 40 days of MLM training on a deberta-v3-large model [11]. Recognizing the need for practical solutions, Kaggle introduced a second stage of the Feedback Prize - Predicting Effective Arguments competition to assess the computational efficiency of models by tracking a combination of accuracy and prediction speed metrics. Kagglers enthusiastically embraced this challenge. In this regard, one solution showcased how a single deberta-v3-large model achieved a private LB score of 0.557 in just 5 minutes and 40 seconds by utilizing out-of-fold pseudo-labels [28]. Similarly, a solution presented in the Coleridge Initiative - Show US the data competition demonstrated that data preprocessing reduced inference time to 10 minutes compared to a lengthy end-to-end transformer-based analysis lasting an hour [12]. These initiatives illustrate that the ML community is becoming more mindful of developing models that can minimize carbon footprint and be practical for real-world applications where computational resources may be limited.</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#adversarial-weight-perturbation-awp","title":"Adversarial Weight Perturbation (AWP)\u00b6","text":"<p>In 2017, Miyato et al. [29] introduced adversarial and virtual adversarial training to the text domain. Instead of perturbing the original input itself, they applied perturbations to the word embeddings in a Recurrent Neural Network (RNN). This approach resulted in reduced overfitting and improved the quality of predicted samples. Building upon this work, in 2020, Wu et al. [30] developed an adversarial training framework called Adversarial Weight Perturbation (AWP). AWP introduced a double-perturbation mechanism to both the original input and weights. As a result, their framework contributed to improve model generalization and robust adversarial training. The promising advantages of AWP led Kagglers to eagerly test it in competitions.</p> <p>Early use of AWP is reported during the Tweet Sentiment Extraction competition [31] and over the past two years, at least five of the top three text-oriented competitions have used it, demonstrating its effectiveness in improving the generalization of text classification tasks [6], [32] and information extraction tasks [7], [22], [24]. Specifically, AWP has made a significant impact on cross-validation (CV), with observed improvements ranging from 0.001 [24], 0.002 [22], and up to an order of magnitude of 0.01 [6], [32]. Furthermore, it has been reported in solution [6] that AWP can assist in stabilizing and extending model training, in addition to optimizing other parameters such as batch size, learning rate, weight decay, and employing a cosine learning rate schedule, among others. Similarly, another solution suggests that conducting a 5-epoch training with AWP starting at the second epoch yielded the best results [7].</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#pseudo-labeling-pl","title":"Pseudo Labeling (PL)\u00b6","text":"<p>Introduced in 2013 as a simple and efficient semi-supervised learning method for deep neural networks [33], Pseudo Labeling (PL) has been a widely spread method among the ML community. During the Santander Customer Transaction competition, PL enabled solution [34] to win the competition boosting the private LB score by 0.0002. Similar results were showed for the Google QUEST Q&amp;A Labeling competition [35]. In words of a Kaggler [36]: \u201cPseudo labeling is the process of adding confident predicted test data to your training data.\u201d Briefly, PL consists of a five-step process:</p> <ol> <li>Train a model using labeled data</li> <li>Predict labels (pseudo-labels) with the trained model using unlabeled data</li> <li>Retrain the model using both labeled and pseudo-labeled datasets</li> <li>Iterate between steps 2 and 3 as necessarily</li> <li>Use the final trained model to predict test data.</li> </ol> <p>The key aspect of PL lies not in the utilization of unknown data, but rather in feeding the model with its own previously generated predictions to increase accuracy.</p> <p>Over the past two years, around 26% of the top three Kaggle solutions related to text data implemented PL. Particularly, all the top 3 solutions of the NBME competition used PL to either predict annotations of unlabeled data [4], create a dataset including 90% of pseudo-labeled data with 10% of training data [22], or predict text spans following a teacher-student approach [18]. The latter solution is rather innovative because they designed a multi-label token classification model to predict text spans using Knowledge Distillation (KD) and Meta Pseudo Labeling (MPL) [37], [38]. Unlike pseudo labeling, teachers in meta pseudo labelling consider the student\u2019s performance on labeled data to improve on their generation of pseudo-labels to better teach their students. Inspired by [39], [40], another solution [15] showed good improvement in CV and LB by finetuning a roberta-base model on original training data to label scraped data [41] and then training different models on pseudo-labeled data.</p> <p>On the other hand, several solutions have proven effective performing multiple rounds of PL. For example, solution [8] carried out a 2-round PL on external data to train a deberta-large model with a low learning rate, three epochs, and a learning scheduler with a three-group decaying learning rate (1, 0.5, 0.25) for each epoch. Meanwhile, solution [21] performed a 3-round PL to train a new single model without any original labels. This strategy allowed to distill the knowledge from a large ensemble into a single model. Finally, solution [10] applied a 5-round PL training that yielded to their best model by using 4k essays plus 11k essays from a previous competition [42].</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#masked-language-models-mlm","title":"Masked Language Models (MLM)\u00b6","text":"<p>Masked Language Model (MLM) has been another technique widely exploited by Kagglers. Because BERT-based models such as deberta-v3-large, deberta-large, roberta-base, roberta-large, and deberta-v2-xlarge are pretrained using MLM, this technique can be extended to pretrain new data to boost performance in other specialized areas that require in-domain learning, as show in solution [43]. For example, a solution [23] for the chaii \u2014 Hindi and Tamil Question Answering competition applied an MLM image-like technique known as cutout to mask 0 to 10% of tokens. Likewise, for the Feedback Prize \u2014 Evaluating Student Writing competition, another solution [44] applied MLM with a masking probability of 15% during training along with a Cutmix approach to cut a portion of a sequence and put it into another sequence of the same batch. In [22], utilizing MLM only on patients' notes boosted CV by 0.002 for the NBME \u2014 Score Clinical Patient Notes competition. Interestingly, a solution for the Google AI4Code \u2013 Understand Code in Python Notebooks competition noticed that a 40-day MLM pretraining on deberta-v3-large improved the LB score concluding that the longer the MLM pretraining, the better the scores [11].</p> <p>On the other hand, Wettig et al. [45] challenged whether a token masking probability of 15% is universally optimal, and thus they conducted a study on using alternatively masking rates to gain a better understanding of MLM pretraining. Inspired by this study, solution [6] used a MLM masking probability of 40-50% to boost CV and LB, and solution [18] implemented a task adaptation with MLM pretraining on patient notes using a token masking probability of 20%.</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#conclusions","title":"Conclusions\u00b6","text":"<p>This essay delves into the learnings of the ML community over the last two years in working with text data. We thoroughly examined 27 write-ups from the top three Kaggle competitions centered around text data and conducted a general meta-analysis of the arXiv dataset. Specifically, we discussed four strategies that have been commonly adopted by Kaggle text data winning solutions of the past two years. We understand that this discussion could have been taken a step further by including more write-ups, datasets, recent algorithms and techniques in NLP, and even considering tricks uniquely adopted by Kagglers to provide a broader perspective on the topic. However, we believe that the information distilled in Table I of Appendix A can be helpful for the ML community to examine specific topics of their interest in more detail.</p> <p>We present the following key insights and shed light on future directions based on our analysis:</p> <ul> <li><p>Although decoder-only models such as the GPT family have experienced rapid growth, both the ML and research community still leans towards utilizing BERT-based models.</p> </li> <li><p>Traditional approaches such as LSTM and GRU remain relevant and are frequently integrated into transformer-based models conforming hybrid architectures.</p> </li> <li><p>The ML community has embraced and matured four strategies across various competitions: model ensembling, pseudo labeling, AWP, and MLM. However, specific questions still remain unanswered as Table II of Appendix B shows. These include properly assembling of models, investigating the impact of multiple rounds of pseudo labeling on CV and LB scores, understanding how AWP can stabilize and prolong model training, and providing further evidence regarding the use of a masking rate higher than 15% during MLM pre-training.</p> </li> <li><p>Current text data solutions are too dependent on labeled data. This opens up opportunities for alternative learning approaches such as zero-shot and few-shot learning, specifically designed to classify text having no or few labeled data. An example of such a solution is [46], which leverages prompt learning\u2014a strategy that utilizes the model's existing knowledge for reasoning and excels in few-shot learning scenarios.</p> </li> <li><p>In text data solutions, there is a concern that improvements are primarily measured based on CV and LB scores. While these metrics are crucial, other factors like resource usage, prediction time, configuration settings, and language variety also hold significance. It is essential to establish comprehensive evaluation criteria and standards to ensure more reliable method comparisons. Including a small benchmark for assessing hyperparameter tuning, learning rate, dropout, weight decay, and early stopping would be desirable.</p> </li> <li><p>As the constant development of more sophisticated LLMs continues, the amount of data and parameters increases, resulting in a greater need for computational resources. To address this, Feedback Prize - Predicting Effective Arguments competition launched a second evaluation stage to assess model performance and execution. This helped Kagglers gain awareness of real-world implementation considerations, including computational resource consumption and environmental impact. For instance, some text data solutions compressed their models using Knowledge Distillation and fine-tuning techniques to create lightweight models to meet computation restrictions. However, it is crucial, as recommended by [47], to focus on the development of more efficient model training techniques. This can involve strategies such as higher data efficiency through sampling training samples, arithmetic acceleration, and hardware and algorithm co-design.</p> </li> <li><p>The solutions proposed for the chaii \u2014 Hindi and Tamil Question Answering competition also demonstrated the necessity of developing resources for languages other than English.</p> </li> </ul> <p>To enhance the transfer of knowledge and promote learning within the ML community, we propose a valuable solution: improving the quality of write-ups for winning solutions. In support of this proposal, we introduce Table I of Appendix A as a comprehensive guide for crafting winning solution write-ups. By following this guide, participants can effectively convey their methodologies, results, and insights, enabling others to better understand and learn from their successful approaches.</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#appendix-a","title":"Appendix A\u00b6","text":"<p>Table 1 Summary of the top 3 Kaggle text data competitions from 2021 to 2023</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#appendix-b","title":"Appendix B\u00b6","text":"<p>Table 2 Summary of Frequently Asked Questions (FAQ) from write-ups of the top 3 Kaggle text data competitions from 2021 to 2023</p>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#references","title":"References\u00b6","text":"<ul> <li>[1]\t\u201cTop 10 Leading Language Models for NLP in 2023.\u201d https://www.analyticsinsight.net/top-10-leading-language-models-for-nlp-in-2023/</li> <li>[2]\t\u201cCompilation of all the Text (NLP) Competitions Hosted on Kaggle.\u201d https://medium.com/@ebrahimhaqbhatti516/compilation-of-all-the-text-nlp-competitions-hosted-on-kaggle-17301835f225</li> <li>[3]\t\u201cTop 1 solution - Coleridge Initiative \u2014 Show US the Data.\u201d https://www.kaggle.com/competitions/coleridgeinitiative-show-us-the-data/discussion/248251</li> <li>[4]\t\u201cTop 2 solution - NBME \u2014 Score Clinical Patient Notes.\u201d https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/323085</li> <li>[5]\t\u201cTop 1 solution - Google AI4Code \u2013 Understand Code in Python Notebooks.\u201d https://www.kaggle.com/competitions/AI4Code/discussion/360501</li> <li>[6]\t\u201cTop 3 solution - Feedback Prize \u2014 Predicting Effective Arguments.\u201d https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347433</li> <li>[7]\t\u201cTop 1 solution - U.S. Patent Phrase to Phrase Matching.\u201d https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/332243</li> <li>[8]\t\u201cTop 3 solution - CommonLit Readability Prize.\u201d https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/258095</li> <li>[9]\t\u201cTop 2 solution - Jigsaw Rate Severity of Toxic Comments.\u201d https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating/discussion/308938</li> <li>[10]\t\u201cTop 2 solution - Feedback Prize \u2014 Predicting Effective Arguments.\u201d https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347359</li> <li>[11]\t\u201cTop 3 solution - Google AI4Code \u2013 Understand Code in Python Notebooks.\u201d https://www.kaggle.com/competitions/AI4Code/discussion/367600</li> <li>[12]\t\u201cTop 2 solution - Coleridge Initiative \u2014 Show US the Data.\u201d https://www.kaggle.com/competitions/coleridgeinitiative-show-us-the-data/discussion/248296</li> <li>[13]\t\u201cTop 4 solution - Coleridge Initiative \u2014 Show US the Data.\u201d https://www.kaggle.com/competitions/coleridgeinitiative-show-us-the-data/discussion/251457</li> <li>[14]\t\u201cTop 3 solution - chaii \u2014 Hindi and Tamil Question Answering.\u201d https://www.kaggle.com/competitions/chaii-hindi-and-tamil-question-answering/discussion/287929</li> <li>[15]\t\u201cTop 1 solution - CommonLit Readability Prize.\u201d https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/257844</li> <li>[16]\t\u201cTop 3 solution - Jigsaw Rate Severity of Toxic Comments.\u201d https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating/discussion/306235</li> <li>[17]\t\u201cTop 1 solution - Jigsaw Rate Severity of Toxic Comments.\u201d https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating/discussion/306274</li> <li>[18]\t\u201cTop 3 solution - NBME \u2014 Score Clinical Patient Notes.\u201d https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/322832</li> <li>[19]\t\u201cTop 2 solution - Google AI4Code \u2013 Understand Code in Python Notebooks.\u201d https://www.kaggle.com/competitions/AI4Code/discussion/343659</li> <li>[20]\t\u201cTop 2 solution - U.S. Patent Phrase to Phrase Matching.\u201d https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/332234</li> <li>[21]\t\u201cTop 1 solution - Feedback Prize \u2014 Predicting Effective Arguments.\u201d https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347536</li> <li>[22]\t\u201cTop 1 solution - NBME \u2014 Score Clinical Patient Notes.\u201d https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/323095</li> <li>[23]\t\u201cTop 1 solution - chaii \u2014 Hindi and Tamil Question Answering.\u201d https://www.kaggle.com/competitions/chaii-hindi-and-tamil-question-answering/discussion/287923</li> <li>[24]\t\u201cTop 3 solution - U.S. Patent Phrase to Phrase Matching.\u201d https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/332420</li> <li>[25]\t\u201cTop 2 solution - chaii \u2014 Hindi and Tamil Question Answering.\u201d https://www.kaggle.com/competitions/chaii-hindi-and-tamil-question-answering/discussion/287917</li> <li>[26]\t\u201cTop 2 solution - CommonLit Readability Prize.\u201d https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/258328</li> <li>[27]\t\u201cTop 2 solution - Feedback Prize \u2014 Evaluating Student Writing.\u201d https://www.kaggle.com/competitions/feedback-prize-2021/discussion/313389</li> <li>[28]\t\u201cTop 1 - Efficiency - Feedback Prize \u2014 Predicting Effective Arguments.\u201d https://www.kaggle.com/competitions/feedback-prize-effectiveness/discussion/347537</li> <li>[29]\tT. Miyato, A. M. Dai, and I. Goodfellow, \u201cAdversarial Training Methods for Semi-Supervised Text Classification.\u201d arXiv, Nov. 16, 2021. doi: 10.48550/arXiv.1605.07725.</li> <li>[30]\tD. Wu, S. Xia, and Y. Wang, \u201cAdversarial Weight Perturbation Helps Robust Generalization.\u201d arXiv, Oct. 13, 2020. doi: 10.48550/arXiv.2004.05884.</li> <li>[31]\t\u201c272 solution - Tweet Sentiment Extraction.\u201d https://www.kaggle.com/competitions/tweet-sentiment-extraction/discussion/143764</li> <li>[32]\t\u201cTop 1 solution - Feedback Prize \u2014 Evaluating Student Writing.\u201d https://www.kaggle.com/competitions/feedback-prize-2021/discussion/313177</li> <li>[33]\tD.-H. Lee, \u201cPseudo-Label\u202f: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks.\u201d 2013.</li> <li>[34]\t\u201cTop 1 solution - Santander Customer Transaction Prediction.\u201d https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003#513626</li> <li>[35]\t\u201cTop 1 solution - Google QUEST Q&amp;A Labeling.\u201d https://www.kaggle.com/c/google-quest-challenge/discussion/129840</li> <li>[36]\t\u201cPseudo Labeling-QDA.\u201d https://www.kaggle.com/code/cdeotte/pseudo-labeling-qda-0-969</li> <li>[37]\tH. Pham, Z. Dai, Q. Xie, M.-T. Luong, and Q. V. Le, \u201cMeta Pseudo Labels.\u201d arXiv, Mar. 01, 2021. doi: 10.48550/arXiv.2003.10580.</li> <li>[38]\t\u201cPlayground for meta-pseudo-label.\u201d https://www.kaggle.com/code/hengck23/playground-for-meta-pseudo-label</li> <li>[39]\tJ. Du et al., \u201cSelf-training Improves Pre-training for Natural Language Understanding.\u201d arXiv, Oct. 05, 2020. doi: 10.48550/arXiv.2010.02194.</li> <li>[40]\tN. Thakur, N. Reimers, J. Daxenberger, and I. Gurevych, \u201cAugmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks.\u201d arXiv, Apr. 12, 2021. doi: 10.48550/arXiv.2010.08240.</li> <li>[41]\t\u201cReadability URL Scrape.\u201d https://www.kaggle.com/code/teeyee314/readability-url-scrape/notebook</li> <li>[42]\t\u201cFeedback Prize - Evaluating Student Writing.\u201d https://www.kaggle.com/competitions/feedback-prize-2021/overview</li> <li>[43]\tS. Gururangan et al., \u201cDon\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks.\u201d arXiv, May 05, 2020. doi: 10.48550/arXiv.2004.10964.</li> <li>[44]\t\u201cTop 3 solution - Feedback Prize \u2014 Evaluating Student Writing.\u201d https://www.kaggle.com/competitions/feedback-prize-2021/discussion/313235</li> <li>[45]\tA. Wettig, T. Gao, Z. Zhong, and D. Chen, \u201cShould You Mask 15% in Masked Language Modeling?\u201d arXiv, Feb. 10, 2023. doi: 10.48550/arXiv.2202.08005.</li> <li>[46]\t\u201cTop 5 - U.S. Patent Phrase to Phrase Matching.\u201d https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/332418</li> <li>[47]\tB. Zhuang, J. Liu, Z. Pan, H. He, Y. Weng, and C. Shen, \u201cA Survey on Efficient Training of Transformers.\u201d arXiv, May 03, 2023. doi: 10.48550/arXiv.2302.01107.</li> </ul>"},{"location":"natural-language-processing/a-journey-through-text-data-competitions/#submission","title":"Submission\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/","title":"Analyzing Movie Genre Predictions through the Lens of Hugging Face Transformers and a Training Loop Approach","text":"<p>This notebook proposes a solution to the Movie Genre Prediction competition from Hugging Face using a <code>BERT-based model</code> to classify movie genres based on their title and synopses. It implements a training loop manually instead of using the trainer API from Hugging Face. This choice aims to improve the fine-tuning phase by manually setting and optimizing selected hyperparameters.</p> <p>The resulted predicted scores obtained by the fine-tuned model are as follows:</p> <ul> <li>Public Score:  0.4260611</li> <li>Private Score: 0.4184444</li> </ul> <p>The fine-tuning stage required 1.5 compute units of type T4 GPU. The execution time for this task took 23 min, utilizing the provided movie dataset. Subsequently, the prediction stage required approximately 2.5 min.</p> <p>This notebook was inspired by Anubhav's solution and the concepts acquired from the Hugging Face NLP course.</p> In\u00a0[\u00a0]: Copied! <pre># View the infrastructure provided by Colab\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') &gt;= 0:\n  print('Not connected to a GPU')\nelse:\n  print(gpu_info)\n</pre> # View the infrastructure provided by Colab gpu_info = !nvidia-smi gpu_info = '\\n'.join(gpu_info) if gpu_info.find('failed') &gt;= 0:   print('Not connected to a GPU') else:   print(gpu_info) <pre>Thu Jan 11 11:08:08 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   38C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> In\u00a0[\u00a0]: Copied! <pre># View the assigned RAM memory\nfrom psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\nif ram_gb &lt; 20:\n  print('Not using a high-RAM runtime')\nelse:\n  print('You are using a high-RAM runtime!')\n</pre> # View the assigned RAM memory from psutil import virtual_memory ram_gb = virtual_memory().total / 1e9 print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))  if ram_gb &lt; 20:   print('Not using a high-RAM runtime') else:   print('You are using a high-RAM runtime!') <pre>Your runtime has 13.6 gigabytes of available RAM\n\nNot using a high-RAM runtime\n</pre> In\u00a0[\u00a0]: Copied! <pre># Install libraries and modules\n!pip install evaluate datasets transformers[sentencepiece]\n!pip install accelerate -U\n!pip install huggingface_hub\n</pre> # Install libraries and modules !pip install evaluate datasets transformers[sentencepiece] !pip install accelerate -U !pip install huggingface_hub <pre>Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\nRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.35.2)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]&gt;=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\nRequirement already satisfied: responses&lt;0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\nRequirement already satisfied: pyarrow&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\nRequirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.0)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.1)\nRequirement already satisfied: sentencepiece!=0.1.92,&gt;=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (23.2.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (6.0.4)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.9.4)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.4.1)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.3.1)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (4.0.3)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.7.0-&gt;evaluate) (4.5.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;evaluate) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;evaluate) (2023.3.post1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;evaluate) (1.16.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.0)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch&gt;=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.2)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (4.5.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (2023.6.0)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (2.1.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-&gt;accelerate) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-&gt;accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.10.0-&gt;accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2023.11.17)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.10.0-&gt;accelerate) (1.3.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\nRequirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (2023.11.17)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Check transformers and accelerate modules version\nimport transformers\nimport accelerate\n\ntransformers.__version__, accelerate.__version__\n</pre> # Check transformers and accelerate modules version import transformers import accelerate  transformers.__version__, accelerate.__version__ Out[\u00a0]: <pre>('4.35.2', '0.26.0')</pre> In\u00a0[\u00a0]: Copied! <pre># Login to Huggingface_hub to access movie dataset\nfrom huggingface_hub import notebook_login\nnotebook_login()\n</pre> # Login to Huggingface_hub to access movie dataset from huggingface_hub import notebook_login notebook_login() <pre>VBox(children=(HTML(value='&lt;center&gt; &lt;img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026</pre> In\u00a0[\u00a0]: Copied! <pre># Import libraries and packages\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\nfrom datasets import load_dataset, Dataset\nfrom collections import Counter\nimport evaluate\n\nimport numpy as np\nimport pandas as pd\nfrom rich import print\n</pre> # Import libraries and packages from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding from datasets import load_dataset, Dataset from collections import Counter import evaluate  import numpy as np import pandas as pd from rich import print In\u00a0[\u00a0]: Copied! <pre># Load competition datasets\nraw_datasets = load_dataset(\"datadrivenscience/movie-genre-prediction\")\nraw_datasets\n</pre> # Load competition datasets raw_datasets = load_dataset(\"datadrivenscience/movie-genre-prediction\") raw_datasets <pre>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n</pre> <pre>Downloading readme:   0%|          | 0.00/1.19k [00:00&lt;?, ?B/s]</pre> <pre>Downloading data:   0%|          | 0.00/7.16M [00:00&lt;?, ?B/s]</pre> <pre>Downloading data:   0%|          | 0.00/4.74M [00:00&lt;?, ?B/s]</pre> <pre>Generating train split:   0%|          | 0/54000 [00:00&lt;?, ? examples/s]</pre> <pre>Generating test split:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'genre'],\n        num_rows: 54000\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'genre'],\n        num_rows: 36000\n    })\n})</pre> In\u00a0[\u00a0]: Copied! <pre># Explore train dataset\nraw_datasets[\"train\"].features\n</pre> # Explore train dataset raw_datasets[\"train\"].features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'genre': Value(dtype='string', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Explore train dataset\nraw_datasets[\"train\"][:5]\n</pre> # Explore train dataset raw_datasets[\"train\"][:5] Out[\u00a0]: <pre>{'id': [44978, 50185, 34131, 78522, 2206],\n 'movie_name': ['Super Me',\n  'Entity Project',\n  'Behavioral Family Therapy for Serious Psychiatric Disorders',\n  'Blood Glacier',\n  'Apat na anino'],\n 'synopsis': ['A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. Selling them makes him rich.',\n  'A director and her friends renting a haunted house to capture paranormal events in order to prove it and become popular.',\n  'This is an educational video for families and family therapists that describes the Behavioral Family Therapy approach to dealing with serious psychiatric illnesses.',\n  'Scientists working in the Austrian Alps discover that a glacier is leaking a liquid that appears to be affecting local wildlife.',\n  'Buy Day - Four Men Widely - Apart in Life - By Night Shadows United in One Fight Venting the Fire of their Fury Against the Hated Oppressors.'],\n 'genre': ['fantasy', 'horror', 'family', 'scifi', 'action']}</pre> In\u00a0[\u00a0]: Copied! <pre># Explore test dataset\nraw_datasets[\"test\"].features\n</pre> # Explore test dataset raw_datasets[\"test\"].features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'genre': Value(dtype='string', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Explore test dataset\nraw_datasets[\"test\"][:5]\n</pre> # Explore test dataset raw_datasets[\"test\"][:5] Out[\u00a0]: <pre>{'id': [16863, 48456, 41383, 84007, 40269],\n 'movie_name': ['A Death Sentence',\n  'Intermedio',\n  '30 Chua Phai Tet',\n  'Paranoiac',\n  'Ordinary Happiness'],\n 'synopsis': [\"12 y.o. Ida's dad'll die without a DKK1,500,000 operation. Ida plans to steal the money from the bank, her mom installed alarm systems in. She'll need her climbing skills, her 2 friends and 3 go-karts.\",\n  'A group of four teenage friends become trapped in a Mexican border tunnel where they fall prey, one-by one, to tortured ghosts who haunt it.',\n  \"A guy left his home for 12 years till he came back to claim what's his from his father, the vast Land, just to uncover that he had to live that day, year-end Lunar day, for another 12 years.\",\n  'A man long believed dead returns to the family estate to claim his inheritance.',\n  'After a deadly accident, Paolo comes back on Earth just 92 minutes more, thanks to a calculation error made in a paradise office.'],\n 'genre': ['action', 'action', 'action', 'action', 'action']}</pre> In\u00a0[\u00a0]: Copied! <pre># Identifying the existing genres in the train dataset\nlabels = set(raw_datasets[\"train\"][\"genre\"])\nnum_labels = len(labels)\nnum_labels, labels\n</pre> # Identifying the existing genres in the train dataset labels = set(raw_datasets[\"train\"][\"genre\"]) num_labels = len(labels) num_labels, labels Out[\u00a0]: <pre>(10,\n {'action',\n  'adventure',\n  'crime',\n  'family',\n  'fantasy',\n  'horror',\n  'mystery',\n  'romance',\n  'scifi',\n  'thriller'})</pre> In\u00a0[\u00a0]: Copied! <pre># Counting the number of movies per genre in the train dataset\nlabels_count = Counter(raw_datasets['train']['genre'])\nprint(labels_count)\n</pre> # Counting the number of movies per genre in the train dataset labels_count = Counter(raw_datasets['train']['genre']) print(labels_count) <pre>Counter({\n    'fantasy': 5400,\n    'horror': 5400,\n    'family': 5400,\n    'scifi': 5400,\n    'action': 5400,\n    'crime': 5400,\n    'adventure': 5400,\n    'mystery': 5400,\n    'romance': 5400,\n    'thriller': 5400\n})\n</pre> In\u00a0[\u00a0]: Copied! <pre># Counting the number of movies per genre in the test dataset\nlabels_count_test = Counter(raw_datasets['test']['genre'])\nprint(labels_count_test)\n</pre> # Counting the number of movies per genre in the test dataset labels_count_test = Counter(raw_datasets['test']['genre']) print(labels_count_test) <pre>Counter({'action': 36000})\n</pre> In\u00a0[\u00a0]: Copied! <pre># Rename \"genre\" column as \"labels\" in the train dataset and turn into a ClassLabel type\nraw_datasets = raw_datasets.rename_column('genre','labels')\nraw_datasets = raw_datasets.class_encode_column('labels')\nraw_datasets['train'].features\n</pre> # Rename \"genre\" column as \"labels\" in the train dataset and turn into a ClassLabel type raw_datasets = raw_datasets.rename_column('genre','labels') raw_datasets = raw_datasets.class_encode_column('labels') raw_datasets['train'].features <pre>Casting to class labels:   0%|          | 0/54000 [00:00&lt;?, ? examples/s]</pre> <pre>Casting to class labels:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['action', 'adventure', 'crime', 'family', 'fantasy', 'horror', 'mystery', 'romance', 'scifi', 'thriller'], id=None)}</pre> <p>Answer: The train dataset contains 10 genres that seemed to be evenly distributed accross the dataset. Meanwhile, the test dataset only contains action movies as dummy values prior inference.</p> In\u00a0[\u00a0]: Copied! <pre># Convert Datasets into Dataframes\nraw_datasets.set_format('pandas')\n</pre> # Convert Datasets into Dataframes raw_datasets.set_format('pandas') In\u00a0[\u00a0]: Copied! <pre># Convert Datasets into Dataframes\ntrain_dataset = raw_datasets['train'][:]\ntrain_dataset.info(memory_usage='deep')\n</pre> # Convert Datasets into Dataframes train_dataset = raw_datasets['train'][:] train_dataset.info(memory_usage='deep') <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 54000 entries, 0 to 53999\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          54000 non-null  int64 \n 1   movie_name  54000 non-null  object\n 2   synopsis    54000 non-null  object\n 3   labels      54000 non-null  int64 \ndtypes: int64(2), object(2)\nmemory usage: 15.4 MB\n</pre> In\u00a0[\u00a0]: Copied! <pre>train_dataset.head(3)\n</pre> train_dataset.head(3) Out[\u00a0]: id movie_name synopsis labels 0 44978 Super Me A young scriptwriter starts bringing valuable ... 4 1 50185 Entity Project A director and her friends renting a haunted h... 5 2 34131 Behavioral Family Therapy for Serious Psychiat... This is an educational video for families and ... 3 In\u00a0[\u00a0]: Copied! <pre># Drop duplicates from the train dataframe\ntrain_dataset = train_dataset.drop_duplicates(['movie_name', 'synopsis'])\ntrain_dataset.info(memory_usage = 'deep')\n</pre> # Drop duplicates from the train dataframe train_dataset = train_dataset.drop_duplicates(['movie_name', 'synopsis']) train_dataset.info(memory_usage = 'deep') <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 46344 entries, 0 to 53998\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          46344 non-null  int64 \n 1   movie_name  46344 non-null  object\n 2   synopsis    46344 non-null  object\n 3   labels      46344 non-null  int64 \ndtypes: int64(2), object(2)\nmemory usage: 13.8 MB\n</pre> <p>Answer: The train dataset cointained 7,656 duplicates.</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset.head(3)\n</pre> train_dataset.head(3) Out[\u00a0]: id movie_name synopsis labels 0 44978 Super Me A young scriptwriter starts bringing valuable ... 4 1 50185 Entity Project A director and her friends renting a haunted h... 5 2 34131 Behavioral Family Therapy for Serious Psychiat... This is an educational video for families and ... 3 In\u00a0[\u00a0]: Copied! <pre># Create a new column \"synopsis_len\" that contains the synopsis length\ntrain_dataset['synopsis_len'] = train_dataset['synopsis'].apply(lambda x: len(x))\ntrain_dataset.head(3)\n</pre> # Create a new column \"synopsis_len\" that contains the synopsis length train_dataset['synopsis_len'] = train_dataset['synopsis'].apply(lambda x: len(x)) train_dataset.head(3) Out[\u00a0]: id movie_name synopsis labels synopsis_len 0 44978 Super Me A young scriptwriter starts bringing valuable ... 4 141 1 50185 Entity Project A director and her friends renting a haunted h... 5 120 2 34131 Behavioral Family Therapy for Serious Psychiat... This is an educational video for families and ... 3 164 In\u00a0[\u00a0]: Copied! <pre># Create a new column \"movie_name_len\" that contains the length of movie_name\ntrain_dataset['movie_name_len'] = train_dataset['movie_name'].apply(lambda x: len(x))\ntrain_dataset.head(3)\n</pre> # Create a new column \"movie_name_len\" that contains the length of movie_name train_dataset['movie_name_len'] = train_dataset['movie_name'].apply(lambda x: len(x)) train_dataset.head(3) Out[\u00a0]: id movie_name synopsis labels synopsis_len movie_name_len 0 44978 Super Me A young scriptwriter starts bringing valuable ... 4 141 8 1 50185 Entity Project A director and her friends renting a haunted h... 5 120 14 2 34131 Behavioral Family Therapy for Serious Psychiat... This is an educational video for families and ... 3 164 59 In\u00a0[\u00a0]: Copied! <pre># Order train dataframe by synopsis_len\ntrain_dataset.sort_values(by='synopsis_len', ascending=False)\n</pre> # Order train dataframe by synopsis_len train_dataset.sort_values(by='synopsis_len', ascending=False) Out[\u00a0]: id movie_name synopsis labels synopsis_len movie_name_len 52518 46444 Final Destination Alex Browning is among a group of high school ... 5 400 17 49498 1468 Bhargava Ramudu Bhargava, an efficient, yet jobless young man ... 0 395 15 29141 71309 Krishnatulasi Krishna is a blind young man who works as a gu... 7 381 13 50834 44856 The Sex Cycle The Cocoa Poodle bar is the central meeting pl... 4 377 13 53370 4779 Uro Turning his back on a delinquent past and join... 0 370 3 ... ... ... ... ... ... ... 6891 71298 Qismat 2 Fortune 2. 7 10 8 38284 5454 Rader Invasion. 0 9 5 3301 15654 Adventure Night TBD 1 3 15 34698 42213 Dark Army NA. 4 3 9 26774 34314 Prima Ballerina TBA 3 3 15 <p>46344 rows \u00d7 6 columns</p> In\u00a0[\u00a0]: Copied! <pre>import plotly.figure_factory as ff\nfig = ff.create_distplot([train_dataset['synopsis_len']], ['length'], colors=['#2ca02c'])\nfig.update_layout(title_text='Word Count Distribution of Movie Synopsis')\nfig.show()\n</pre> import plotly.figure_factory as ff fig = ff.create_distplot([train_dataset['synopsis_len']], ['length'], colors=['#2ca02c']) fig.update_layout(title_text='Word Count Distribution of Movie Synopsis') fig.show() In\u00a0[\u00a0]: Copied! <pre>fig2 = ff.create_distplot([train_dataset['movie_name_len']], ['length'], colors=['#ffa408'])\nfig2.update_layout(title_text='Word Count Distribution of Movie Titles')\nfig2.show()\n</pre> fig2 = ff.create_distplot([train_dataset['movie_name_len']], ['length'], colors=['#ffa408']) fig2.update_layout(title_text='Word Count Distribution of Movie Titles') fig2.show() In\u00a0[\u00a0]: Copied! <pre>train_dataset['movie_name_len'].max(), train_dataset['synopsis_len'].max()\n</pre> train_dataset['movie_name_len'].max(), train_dataset['synopsis_len'].max() Out[\u00a0]: <pre>(180, 400)</pre> <p>The average length of characters in the movie name is 12. For the synopsis, we see two peaks around 145 and 230 characters. The maximum character size is 180 for the movie name and 400 for the synopsis. So, there won't be any issues during tokenization and training because the bert-base-uncased model supports a maximum character length of 512.</p> In\u00a0[\u00a0]: Copied! <pre># Convert the train_dataset Dataframe to DataSet format again\ntrain_ds = Dataset.from_pandas(train_dataset)\ntrain_ds.features\n</pre> # Convert the train_dataset Dataframe to DataSet format again train_ds = Dataset.from_pandas(train_dataset) train_ds.features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': Value(dtype='int64', id=None),\n 'synopsis_len': Value(dtype='int64', id=None),\n 'movie_name_len': Value(dtype='int64', id=None),\n '__index_level_0__': Value(dtype='int64', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Turn \"labels\" column into ClassLabel type\ntrain_ds = train_ds.class_encode_column('labels')\ntrain_ds.features\n</pre> # Turn \"labels\" column into ClassLabel type train_ds = train_ds.class_encode_column('labels') train_ds.features <pre>Stringifying the column:   0%|          | 0/46344 [00:00&lt;?, ? examples/s]</pre> <pre>Casting to class labels:   0%|          | 0/46344 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], id=None),\n 'synopsis_len': Value(dtype='int64', id=None),\n 'movie_name_len': Value(dtype='int64', id=None),\n '__index_level_0__': Value(dtype='int64', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Create tokenizer\n# i.e. bert-base-uncased, bert-large-uncased, bert-large-uncased-whole-word-masking\ncheckpoint = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenizer\n</pre> # Create tokenizer # i.e. bert-base-uncased, bert-large-uncased, bert-large-uncased-whole-word-masking checkpoint = 'bert-base-uncased' tokenizer = AutoTokenizer.from_pretrained(checkpoint) tokenizer <pre>tokenizer_config.json:   0%|          | 0.00/28.0 [00:00&lt;?, ?B/s]</pre> <pre>config.json:   0%|          | 0.00/570 [00:00&lt;?, ?B/s]</pre> <pre>vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]</pre> <pre>tokenizer.json:   0%|          | 0.00/466k [00:00&lt;?, ?B/s]</pre> Out[\u00a0]: <pre>BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}</pre> In\u00a0[\u00a0]: Copied! <pre># Do a sample tokenization\nsample_tokenized = tokenizer(train_ds['movie_name'][0], train_ds['synopsis'][0])\ntokenizer.decode(sample_tokenized['input_ids'])\n</pre> # Do a sample tokenization sample_tokenized = tokenizer(train_ds['movie_name'][0], train_ds['synopsis'][0]) tokenizer.decode(sample_tokenized['input_ids']) Out[\u00a0]: <pre>'[CLS] super me [SEP] a young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. selling them makes him rich. [SEP]'</pre> In\u00a0[\u00a0]: Copied! <pre>sample_tokenized\n</pre> sample_tokenized Out[\u00a0]: <pre>{'input_ids': [101, 3565, 2033, 102, 1037, 2402, 5896, 15994, 4627, 5026, 7070, 5200, 2067, 2013, 2010, 2460, 15446, 1997, 2108, 13303, 2011, 1037, 5698, 1012, 4855, 2068, 3084, 2032, 4138, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</pre> In\u00a0[\u00a0]: Copied! <pre># Split Train Dataset (train_ds) into training and test datasets\ntrain_ds = train_ds.train_test_split(test_size=0.2, stratify_by_column=\"labels\")\ntrain_ds\n</pre> # Split Train Dataset (train_ds) into training and test datasets train_ds = train_ds.train_test_split(test_size=0.2, stratify_by_column=\"labels\") train_ds Out[\u00a0]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', 'synopsis_len', 'movie_name_len', '__index_level_0__'],\n        num_rows: 37075\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', 'synopsis_len', 'movie_name_len', '__index_level_0__'],\n        num_rows: 9269\n    })\n})</pre> In\u00a0[\u00a0]: Copied! <pre># Define a tokenize function\ndef tokenize(ds):\n  return tokenizer(ds['movie_name'], ds['synopsis'], truncation=True)\n</pre> # Define a tokenize function def tokenize(ds):   return tokenizer(ds['movie_name'], ds['synopsis'], truncation=True) In\u00a0[\u00a0]: Copied! <pre># Tokenize train_ds\ntokenized_datasets = train_ds.map(tokenize, batched=True)\ntokenized_datasets\n</pre> # Tokenize train_ds tokenized_datasets = train_ds.map(tokenize, batched=True) tokenized_datasets <pre>Map:   0%|          | 0/37075 [00:00&lt;?, ? examples/s]</pre> <pre>Map:   0%|          | 0/9269 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', 'synopsis_len', 'movie_name_len', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 37075\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', 'synopsis_len', 'movie_name_len', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9269\n    })\n})</pre> In\u00a0[\u00a0]: Copied! <pre># Select a random sample to verify tokenization\ntokenizer.decode(tokenized_datasets['train']['input_ids'][37074])\n</pre> # Select a random sample to verify tokenization tokenizer.decode(tokenized_datasets['train']['input_ids'][37074]) Out[\u00a0]: <pre>'[CLS] begum [SEP] a sheltered beauty, begum, is introduced to the enchanting world of bollywood by the enigmatic madan where she discovers true freedom and love come at the price of her passion and life. [SEP]'</pre> In\u00a0[\u00a0]: Copied! <pre># Removing columns the model doesn't expect\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'movie_name', 'synopsis', 'synopsis_len', 'movie_name_len','__index_level_0__'])\ntokenized_datasets['train'].column_names\n</pre> # Removing columns the model doesn't expect tokenized_datasets = tokenized_datasets.remove_columns(['id', 'movie_name', 'synopsis', 'synopsis_len', 'movie_name_len','__index_level_0__']) tokenized_datasets['train'].column_names Out[\u00a0]: <pre>['labels', 'input_ids', 'token_type_ids', 'attention_mask']</pre> In\u00a0[\u00a0]: Copied! <pre># Setting the datasets format so that they can return Pytorch tensors\ntokenized_datasets.set_format(\"torch\")\n</pre> # Setting the datasets format so that they can return Pytorch tensors tokenized_datasets.set_format(\"torch\") In\u00a0[\u00a0]: Copied! <pre># Define a data_collator function for dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n</pre> # Define a data_collator function for dynamic padding data_collator = DataCollatorWithPadding(tokenizer=tokenizer) In\u00a0[\u00a0]: Copied! <pre># Defining DataLoaders\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(\n    tokenized_datasets['train'], shuffle=True, batch_size=32, collate_fn=data_collator\n)\n\neval_dataloader = DataLoader(\n    tokenized_datasets['test'], batch_size=64, collate_fn=data_collator\n)\n</pre> # Defining DataLoaders from torch.utils.data import DataLoader  train_dataloader = DataLoader(     tokenized_datasets['train'], shuffle=True, batch_size=32, collate_fn=data_collator )  eval_dataloader = DataLoader(     tokenized_datasets['test'], batch_size=64, collate_fn=data_collator ) In\u00a0[\u00a0]: Copied! <pre># Inspecting a batch from train_dataloader\nfor batch in train_dataloader:\n  break\n{k:v.shape for k,v in batch.items()}\n</pre> # Inspecting a batch from train_dataloader for batch in train_dataloader:   break {k:v.shape for k,v in batch.items()} <pre>You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n</pre> Out[\u00a0]: <pre>{'labels': torch.Size([32]),\n 'input_ids': torch.Size([32, 65]),\n 'token_type_ids': torch.Size([32, 65]),\n 'attention_mask': torch.Size([32, 65])}</pre> In\u00a0[\u00a0]: Copied! <pre># Inspecting a batch from train_dataloader\nbatch.input_ids\n</pre> # Inspecting a batch from train_dataloader batch.input_ids Out[\u00a0]: <pre>tensor([[  101,  3019,  5320,  ...,     0,     0,     0],\n        [  101,  1051, 10381,  ...,     0,     0,     0],\n        [  101, 13970, 13278,  ...,     0,     0,     0],\n        ...,\n        [  101, 14477,  9587,  ...,     0,     0,     0],\n        [  101,  1037,  3543,  ...,     0,     0,     0],\n        [  101, 15274,  1004,  ...,     0,     0,     0]])</pre> In\u00a0[\u00a0]: Copied! <pre># Instantiate a new model\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n</pre> # Instantiate a new model model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels) <pre>model.safetensors:   0%|          | 0.00/440M [00:00&lt;?, ?B/s]</pre> <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Passing a single batch to our model to check that everything is OK\noutputs = model(**batch)\nprint(outputs.loss, outputs.logits.shape)\n</pre> # Passing a single batch to our model to check that everything is OK outputs = model(**batch) print(outputs.loss, outputs.logits.shape) <pre>tensor(2.3496, grad_fn=&lt;NllLossBackward0&gt;)\ntorch.Size([32, 10])\n</pre> <p>Note: When labels are provided, HF transformers will return the loss and the logits (two for each input)</p> In\u00a0[\u00a0]: Copied! <pre>outputs.logits\n</pre> outputs.logits Out[\u00a0]: <pre>tensor([[-4.0785e-01, -5.9843e-01,  2.3831e-01, -3.6281e-01,  7.7003e-02,\n         -7.9042e-01,  3.7949e-01,  6.3509e-02,  2.5117e-01, -4.0870e-01],\n        [-4.7463e-01,  8.1971e-01, -5.7489e-01,  2.9883e-01, -5.2216e-03,\n          2.9287e-01,  1.3395e-01, -1.2713e-01, -6.2679e-03,  2.6193e-01],\n        [-3.2331e-01, -7.0246e-01,  3.2733e-01, -4.3661e-01,  1.2958e-02,\n         -9.1787e-01,  4.7303e-01,  3.7585e-02,  4.2619e-01, -4.0903e-01],\n        [-3.8950e-01, -5.9636e-01,  1.9592e-01, -3.3946e-01,  6.1083e-02,\n         -7.2596e-01,  4.0944e-01,  6.8973e-02,  2.9999e-01, -4.2091e-01],\n        [-3.8568e-01, -5.6021e-01,  1.7959e-01, -3.0257e-01,  6.5529e-02,\n         -7.6407e-01,  3.8535e-01,  2.6806e-02,  2.6749e-01, -4.2585e-01],\n        [-3.6171e-01, -6.5866e-01,  3.1120e-01, -3.8170e-01,  5.1930e-02,\n         -8.8321e-01,  4.5273e-01,  5.1025e-02,  3.6372e-01, -4.0050e-01],\n        [-4.2266e-01, -4.7145e-01,  9.5230e-02, -2.3389e-01,  1.3982e-01,\n         -6.2665e-01,  3.7735e-01,  9.9706e-02,  1.8753e-01, -4.0756e-01],\n        [-3.1637e-01,  8.8094e-01, -5.1321e-01,  5.4990e-01,  1.2587e-01,\n          3.7881e-01,  6.1411e-02, -1.5915e-01, -1.2994e-01,  4.6542e-01],\n        [-3.8507e-01,  8.0821e-01, -4.5865e-01,  4.5561e-01,  7.2987e-02,\n          2.9142e-01,  1.0862e-01, -1.3985e-01, -5.3073e-02,  3.0885e-01],\n        [-3.1688e-01,  8.4953e-01, -6.1110e-01,  4.8324e-01,  3.0182e-02,\n          5.0664e-01,  1.1268e-01, -1.3150e-01, -1.6073e-01,  2.8703e-01],\n        [-5.1548e-01, -1.6234e-01, -1.9065e-02, -1.6256e-01,  7.9030e-02,\n         -2.1718e-01,  2.8946e-01,  1.8431e-01,  5.1388e-02, -9.4204e-02],\n        [-4.1334e-01, -3.1754e-01,  3.5393e-02, -2.6649e-02,  4.9808e-02,\n         -1.2166e-01,  4.8317e-02,  1.0462e-01,  1.7923e-01, -1.8917e-01],\n        [-4.2132e-01, -4.9542e-01,  2.3680e-01, -2.0575e-01,  1.0921e-01,\n         -5.6644e-01,  2.0891e-01,  1.9182e-01,  2.1404e-01, -4.3724e-01],\n        [-6.1001e-01,  2.9906e-01, -2.8272e-01, -5.8040e-03,  1.1232e-01,\n         -5.5117e-03,  2.3484e-01, -6.8720e-02,  1.3603e-01,  2.2902e-01],\n        [-4.2867e-01, -3.6328e-01,  1.3073e-01, -2.2871e-01,  7.4158e-02,\n         -5.7429e-01,  3.9471e-01,  1.5585e-01,  1.6983e-01, -3.9815e-01],\n        [-3.4650e-01, -6.8317e-01,  3.0791e-01, -4.1893e-01,  3.2079e-02,\n         -8.8611e-01,  4.6996e-01,  6.2504e-02,  3.8550e-01, -3.9998e-01],\n        [-5.0109e-01,  8.8373e-01, -4.0189e-01,  2.5233e-01,  1.4930e-01,\n         -1.9989e-03,  2.4316e-01, -1.1267e-01, -4.7194e-02,  2.2868e-01],\n        [-4.4270e-01, -2.5146e-01,  1.8103e-02, -1.1784e-01,  9.0070e-02,\n         -2.7046e-01,  1.5864e-01,  1.0738e-01,  5.2518e-03, -1.6656e-01],\n        [-3.4070e-01, -7.1199e-01,  3.2499e-01, -4.5099e-01,  1.8579e-02,\n         -9.1884e-01,  4.7906e-01,  2.8550e-02,  4.2099e-01, -3.9343e-01],\n        [-1.6497e-01,  9.5924e-01, -6.0456e-01,  5.6138e-01,  9.0732e-02,\n          6.0200e-01, -9.3879e-03, -1.4700e-01, -2.7646e-01,  5.6783e-01],\n        [-4.4853e-01, -4.2034e-01,  2.2960e-01, -2.9798e-01,  1.3834e-01,\n         -6.6684e-01,  4.3417e-01,  6.2197e-02,  1.2980e-01, -4.2036e-01],\n        [-3.6195e-01, -5.8910e-01,  2.5943e-01, -3.1415e-01,  9.1517e-02,\n         -7.5003e-01,  3.9000e-01,  7.4762e-02,  2.3163e-01, -3.9896e-01],\n        [-3.6552e-01, -2.1078e-01,  5.7047e-02, -2.2510e-01,  6.4404e-02,\n         -4.7013e-01,  2.9130e-01,  8.8827e-02,  3.8984e-02, -2.7132e-01],\n        [-6.8175e-01,  6.3034e-01, -5.3148e-01,  3.4757e-01,  2.1629e-01,\n          1.2112e-01,  3.0105e-01, -9.3755e-02,  4.3006e-02,  2.2544e-01],\n        [-6.1631e-02,  8.8187e-01, -6.0550e-01,  5.7708e-01,  8.8134e-02,\n          6.4667e-01, -9.7189e-02, -3.8907e-02, -2.0468e-01,  4.9839e-01],\n        [-3.9742e-01, -4.9334e-01,  2.7592e-01, -3.0464e-01,  1.3201e-01,\n         -7.5158e-01,  3.9522e-01,  1.1564e-01,  1.8720e-01, -4.6010e-01],\n        [-3.4826e-01, -5.4302e-01,  2.3106e-01, -3.1282e-01,  9.6388e-02,\n         -7.6846e-01,  3.8242e-01,  1.1184e-01,  3.0391e-01, -3.6247e-01],\n        [-5.7508e-01, -2.1254e-01,  9.6993e-02, -1.8350e-01,  1.7445e-01,\n         -2.8679e-01,  1.8445e-01,  1.5498e-01,  1.0922e-01, -2.2348e-01],\n        [-4.7849e-01, -2.5155e-01, -8.0201e-02,  5.7167e-02, -4.9126e-04,\n         -1.2727e-01,  1.0912e-01,  7.7717e-02,  1.4069e-01, -1.5423e-01],\n        [ 1.3228e-01,  8.4822e-01, -5.7432e-01,  5.7294e-01,  3.9404e-03,\n          7.7920e-01, -2.2635e-01, -1.5605e-02, -2.6260e-01,  4.9051e-01],\n        [-3.2101e-01, -6.8110e-01,  3.2382e-01, -4.2349e-01,  6.5490e-02,\n         -9.3271e-01,  4.8653e-01,  8.8333e-02,  4.0458e-01, -3.8155e-01],\n        [-6.3350e-01, -1.1693e-01, -2.6130e-01,  1.4510e-01,  6.1222e-02,\n          2.0786e-01, -1.0654e-02,  8.4277e-02, -2.7730e-02,  1.3080e-01]],\n       grad_fn=&lt;AddmmBackward0&gt;)</pre> In\u00a0[\u00a0]: Copied! <pre># Instantiate a new model\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n</pre> # Instantiate a new model model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels) <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Setting an accelerator and optimizer\nfrom transformers import AdamW\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\noptimizer = AdamW(model.parameters(), lr=1e-5)\n</pre> # Setting an accelerator and optimizer from transformers import AdamW from accelerate import Accelerator  accelerator = Accelerator() optimizer = AdamW(model.parameters(), lr=1e-5) <pre>/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning:\n\nThis implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Prepare data for accelerator\ntrain_dl, eval_dl, model, optimizer = accelerator.prepare(\n    train_dataloader, eval_dataloader, model, optimizer\n)\n</pre> # Prepare data for accelerator train_dl, eval_dl, model, optimizer = accelerator.prepare(     train_dataloader, eval_dataloader, model, optimizer ) In\u00a0[\u00a0]: Copied! <pre># Setting a learning rate scheduler\nfrom transformers import get_scheduler\n\nnum_epochs = 2\nnum_training_steps = num_epochs * len(train_dl)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\nprint(num_training_steps)\n</pre> # Setting a learning rate scheduler from transformers import get_scheduler  num_epochs = 2 num_training_steps = num_epochs * len(train_dl) lr_scheduler = get_scheduler(     \"linear\",     optimizer=optimizer,     num_warmup_steps=0,     num_training_steps=num_training_steps ) print(num_training_steps) <pre>2318\n</pre> In\u00a0[\u00a0]: Copied! <pre>import torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice\n</pre> import torch device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") model.to(device) device Out[\u00a0]: <pre>device(type='cpu')</pre> In\u00a0[\u00a0]: Copied! <pre># Add a progress bar\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n  for batch in train_dataloader:\n    batch = {k:v.to(device) for k,v in batch.items()}\n    outputs = model(**batch)\n    loss = outputs.loss\n    loss.backward()\n\n    optimizer.step()\n    lr_scheduler.step()\n    optimizer.zero_grad()\n    progress_bar.update(1)\n</pre> # Add a progress bar from tqdm.auto import tqdm  progress_bar = tqdm(range(num_training_steps))  model.train() for epoch in range(num_epochs):   for batch in train_dataloader:     batch = {k:v.to(device) for k,v in batch.items()}     outputs = model(**batch)     loss = outputs.loss     loss.backward()      optimizer.step()     lr_scheduler.step()     optimizer.zero_grad()     progress_bar.update(1) <pre>  0%|          | 0/3477 [00:00&lt;?, ?it/s]</pre> In\u00a0[\u00a0]: Copied! <pre>metric = evaluate.load(\"accuracy\")\nmodel.eval()\n\nfor batch in eval_dataloader:\n  batch = {k: v.to(device) for k,v in batch.items()}\n  with torch.no_grad():\n    outputs = model(**batch)\n\n  logits = outputs.logits\n  predictions = torch.argmax(logits, dim=-1)\n  metric.add_batch(predictions=predictions, references=batch['labels'])\n\nmetric.compute()\n</pre> metric = evaluate.load(\"accuracy\") model.eval()  for batch in eval_dataloader:   batch = {k: v.to(device) for k,v in batch.items()}   with torch.no_grad():     outputs = model(**batch)    logits = outputs.logits   predictions = torch.argmax(logits, dim=-1)   metric.add_batch(predictions=predictions, references=batch['labels'])  metric.compute() <pre>Downloading builder script:   0%|          | 0.00/4.20k [00:00&lt;?, ?B/s]</pre> Out[\u00a0]: <pre>{'accuracy': 0.4532312007767828}</pre> In\u00a0[\u00a0]: Copied! <pre>from accelerate import Accelerator\nfrom transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n</pre> from accelerate import Accelerator from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler In\u00a0[\u00a0]: Copied! <pre># Model instantiation\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n</pre> # Model instantiation model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels) <pre>model.safetensors:   0%|          | 0.00/440M [00:00&lt;?, ?B/s]</pre> <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Setting an Optimizer, Accelerator, and Scheduler\noptimizer = AdamW(model.parameters(), lr=1e-5)\naccelerator = Accelerator()\n\ntrain_dl, eval_dl, model, optimizer = accelerator.prepare(\n    train_dataloader, eval_dataloader, model, optimizer\n)\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dl)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n</pre> # Setting an Optimizer, Accelerator, and Scheduler optimizer = AdamW(model.parameters(), lr=1e-5) accelerator = Accelerator()  train_dl, eval_dl, model, optimizer = accelerator.prepare(     train_dataloader, eval_dataloader, model, optimizer )  num_epochs = 3 num_training_steps = num_epochs * len(train_dl) lr_scheduler = get_scheduler(     \"linear\",     optimizer=optimizer,     num_warmup_steps=0,     num_training_steps=num_training_steps ) <pre>/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning:\n\nThis implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Verifying infrastructure settings\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice\n</pre> # Verifying infrastructure settings import torch device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") model.to(device) device Out[\u00a0]: <pre>device(type='cuda')</pre> In\u00a0[\u00a0]: Copied! <pre># Model training and evaluation\nfrom tqdm.auto import tqdm\n\nmetric = evaluate.load(\"accuracy\")\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_epochs):\n  # Training\n  model.train()\n  for batch in train_dl:\n    outputs = model(**batch)\n    loss = outputs.loss\n    accelerator.backward(loss)\n\n    optimizer.step()\n    lr_scheduler.step()\n    optimizer.zero_grad()\n    progress_bar.update(1)\n\n  # Evaluation\n  model.eval()\n  for batch in eval_dl:\n    with torch.no_grad():\n      outputs = model(**batch)\n\n    predictions = outputs.logits.argmax(dim=-1)\n    labels = batch[\"labels\"]\n\n    predictions_gathered = accelerator.gather(predictions)\n    labels_gathered = accelerator.gather(labels)\n    metric.add_batch(predictions=predictions_gathered, references=labels_gathered)\n\n  results = metric.compute()\n  print(f\"epoch {epoch}: {results['accuracy']}\")\n</pre> # Model training and evaluation from tqdm.auto import tqdm  metric = evaluate.load(\"accuracy\") progress_bar = tqdm(range(num_training_steps))  for epoch in range(num_epochs):   # Training   model.train()   for batch in train_dl:     outputs = model(**batch)     loss = outputs.loss     accelerator.backward(loss)      optimizer.step()     lr_scheduler.step()     optimizer.zero_grad()     progress_bar.update(1)    # Evaluation   model.eval()   for batch in eval_dl:     with torch.no_grad():       outputs = model(**batch)      predictions = outputs.logits.argmax(dim=-1)     labels = batch[\"labels\"]      predictions_gathered = accelerator.gather(predictions)     labels_gathered = accelerator.gather(labels)     metric.add_batch(predictions=predictions_gathered, references=labels_gathered)    results = metric.compute()   print(f\"epoch {epoch}: {results['accuracy']}\") <pre>Downloading builder script:   0%|          | 0.00/4.20k [00:00&lt;?, ?B/s]</pre> <pre>  0%|          | 0/3477 [00:00&lt;?, ?it/s]</pre> <pre>epoch 0: 0.4238860718524113\n</pre> <pre>epoch 1: 0.4341352896752616\n</pre> <pre>epoch 2: 0.43381163016506635\n</pre> <p>Now that we have fine-tuned our classification model, it's time to check how it performs on the test dataset. Since we aren't using the Trainer API, it's necessary to pre-process data of the test dataset.</p> In\u00a0[\u00a0]: Copied! <pre># Inspect the test dataset\nraw_datasets['test'].features\n</pre> # Inspect the test dataset raw_datasets['test'].features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['action'], id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Convert the test dataset to a dataframe\ntest_raw_dataset = raw_datasets['test'][:]\ntest_raw_dataset.info(memory_usage='deep')\n</pre> # Convert the test dataset to a dataframe test_raw_dataset = raw_datasets['test'][:] test_raw_dataset.info(memory_usage='deep') <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 36000 entries, 0 to 35999\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          36000 non-null  int64 \n 1   movie_name  36000 non-null  object\n 2   synopsis    36000 non-null  object\n 3   labels      36000 non-null  int64 \ndtypes: int64(2), object(2)\nmemory usage: 10.3 MB\n</pre> In\u00a0[\u00a0]: Copied! <pre># Turn the test dataframe into a Dataset format again\ntest_ds = Dataset.from_pandas(test_raw_dataset)\ntest_ds.features\n</pre> # Turn the test dataframe into a Dataset format again test_ds = Dataset.from_pandas(test_raw_dataset) test_ds.features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': Value(dtype='int64', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Turn \"labels\" column into a ClassType format\ntest_ds = test_ds.class_encode_column('labels')\ntest_ds.features\n</pre> # Turn \"labels\" column into a ClassType format test_ds = test_ds.class_encode_column('labels') test_ds.features <pre>Stringifying the column:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> <pre>Casting to class labels:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['0'], id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Tokenize the test dataset\ntokenized_test_ds = test_ds.map(tokenize, batched=True)\n</pre> # Tokenize the test dataset tokenized_test_ds = test_ds.map(tokenize, batched=True) <pre>Map:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> In\u00a0[\u00a0]: Copied! <pre># Inspect the content of the test dataset\ntokenized_test_ds.column_names\n</pre> # Inspect the content of the test dataset tokenized_test_ds.column_names Out[\u00a0]: <pre>['id',\n 'movie_name',\n 'synopsis',\n 'labels',\n 'input_ids',\n 'token_type_ids',\n 'attention_mask']</pre> In\u00a0[\u00a0]: Copied! <pre># Create a copy of the original test dataset\nfrom copy import deepcopy\ntokenized_test_ds_copy = deepcopy(tokenized_test_ds)\n</pre> # Create a copy of the original test dataset from copy import deepcopy tokenized_test_ds_copy = deepcopy(tokenized_test_ds) In\u00a0[\u00a0]: Copied! <pre># Remove columns the model doesn't expect\ntokenized_test_ds_copy = tokenized_test_ds_copy.remove_columns(['id', 'movie_name', 'synopsis'])\ntokenized_test_ds_copy.column_names\n</pre> # Remove columns the model doesn't expect tokenized_test_ds_copy = tokenized_test_ds_copy.remove_columns(['id', 'movie_name', 'synopsis']) tokenized_test_ds_copy.column_names Out[\u00a0]: <pre>['labels', 'input_ids', 'token_type_ids', 'attention_mask']</pre> In\u00a0[\u00a0]: Copied! <pre># Define a DataLoader for the test dataset\nfrom torch.utils.data import DataLoader\ntest_dataloader = DataLoader(\n    tokenized_test_ds_copy, batch_size=64, collate_fn=data_collator\n)\n</pre> # Define a DataLoader for the test dataset from torch.utils.data import DataLoader test_dataloader = DataLoader(     tokenized_test_ds_copy, batch_size=64, collate_fn=data_collator ) In\u00a0[\u00a0]: Copied! <pre># Inspect a batch from train_dataloader\nfor batch in test_dataloader:\n  break\n{k:v.shape for k,v in batch.items()}\n</pre> # Inspect a batch from train_dataloader for batch in test_dataloader:   break {k:v.shape for k,v in batch.items()}  Out[\u00a0]: <pre>{'labels': torch.Size([64]),\n 'input_ids': torch.Size([64, 73]),\n 'token_type_ids': torch.Size([64, 73]),\n 'attention_mask': torch.Size([64, 73])}</pre> In\u00a0[\u00a0]: Copied! <pre># Specify a device type since we aren't using accelerate for the prediction stage\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice\n</pre> # Specify a device type since we aren't using accelerate for the prediction stage import torch device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") model.to(device) device Out[\u00a0]: <pre>device(type='cuda')</pre> In\u00a0[\u00a0]: Copied! <pre># Run the model to get predictions\nnum_eval_steps = len(test_dataloader)\nprogress_bar = tqdm(range(num_eval_steps))\n\npredictions = []\nmodel.eval()\nfor batch in test_dataloader:\n  batch = {k:v.to(device) for k,v in batch.items()}\n  with torch.no_grad():\n    outputs = model(**batch)\n\n  batch_predictions = outputs.logits.argmax(dim=-1).tolist()\n  predictions.extend(batch_predictions)\n  progress_bar.update(1)\n</pre> # Run the model to get predictions num_eval_steps = len(test_dataloader) progress_bar = tqdm(range(num_eval_steps))  predictions = [] model.eval() for batch in test_dataloader:   batch = {k:v.to(device) for k,v in batch.items()}   with torch.no_grad():     outputs = model(**batch)    batch_predictions = outputs.logits.argmax(dim=-1).tolist()   predictions.extend(batch_predictions)   progress_bar.update(1) <pre>  0%|          | 0/563 [00:00&lt;?, ?it/s]</pre> In\u00a0[\u00a0]: Copied! <pre># Display some predictions\nprint(predictions[:20])\n</pre> # Display some predictions print(predictions[:20]) <pre>[3, 5, 4, 6, 8, 1, 9, 2, 5, 4, 0, 7, 2, 4, 5, 0, 3, 3, 9, 8]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Convert predictions to their string representations based on the mapping defined in the 'labels' feature.\npredicted_genre = raw_datasets['train'].features['labels'].int2str(predictions)\n</pre> # Convert predictions to their string representations based on the mapping defined in the 'labels' feature. predicted_genre = raw_datasets['train'].features['labels'].int2str(predictions) In\u00a0[\u00a0]: Copied! <pre># Create a dataframe specifying movie id and genre\ndf = pd.DataFrame({'id':tokenized_test_ds['id'], 'genre':predicted_genre})\ndf.head(3)\n</pre> # Create a dataframe specifying movie id and genre df = pd.DataFrame({'id':tokenized_test_ds['id'], 'genre':predicted_genre}) df.head(3) Out[\u00a0]: id genre 0 16863 family 1 48456 horror 2 41383 fantasy In\u00a0[\u00a0]: Copied! <pre># Save results to a csv file\ndf.to_csv('submission.csv')\n</pre> # Save results to a csv file df.to_csv('submission.csv')"},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#analyzing-movie-genre-predictions-through-the-lens-of-hugging-face-transformers-and-a-training-loop-approach","title":"Analyzing Movie Genre Predictions through the Lens of Hugging Face Transformers and a Training Loop Approach\u00b6","text":"<p>by Salomon Marquez</p> <p>01/07/2024</p>"},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#loading-movie-datasets","title":"Loading Movie Datasets\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#what-are-the-existing-movie-genres","title":"What are the existing movie genres?\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#removing-duplicated-items","title":"Removing Duplicated Items\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#analyzing-text-movie-titles-and-their-synopses","title":"Analyzing text movie titles and their synopses\u00b6","text":"<p>This section analysis the length of movie titles and their synopses.</p>"},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#tokenization","title":"Tokenization\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#preparing-data-for-the-training-stage","title":"Preparing data for the training stage\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#step-by-step-setting-of-the-training-stage","title":"Step-by-step setting of the training stage\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#model-instantiation","title":"Model Instantiation\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#setting-an-optimizer-and-accelerator","title":"Setting an optimizer and accelerator\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#setting-a-scheduler","title":"Setting a scheduler\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#verifying-infrastructure-settings","title":"Verifying infrastructure settings\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#setting-a-progress-bar-to-track-the-training-stage","title":"Setting a progress bar to track the training stage\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#setting-the-evaluation-stage","title":"Setting the evaluation stage\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#full-training-loop-with-accelerate","title":"Full training loop with accelerate\u00b6","text":""},{"location":"natural-language-processing/analyzing-movie-genre-predictions-using-transformers/#preparing-submission","title":"Preparing submission\u00b6","text":""},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/","title":"Movie Genre Prediction using Robust Model Architectures","text":"<p>This notebook presents a solution to the Movie Genre Prediction competition by Hugging Face. The goal is to classify movie genres based on their titles and synopses using BERT-based pre-trained models with over 340M parameters.</p> <p>The following models are considered:</p> <ul> <li>bert-large-uncased</li> <li>bert-large-uncased-whole-word-masking</li> </ul> <p>The notebook implements a complete training loop using the <code>accelerate</code> library. After training, the fine-tuned model is uploaded to the Hugging Face Hub. You can access and download the fine-tuned model from the Hugging Face Hub.</p> In\u00a0[\u00a0]: Copied! <pre># View the infrastructure provided by Colab\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') &gt;= 0:\n  print('Not connected to a GPU')\nelse:\n  print(gpu_info)\n</pre> # View the infrastructure provided by Colab gpu_info = !nvidia-smi gpu_info = '\\n'.join(gpu_info) if gpu_info.find('failed') &gt;= 0:   print('Not connected to a GPU') else:   print(gpu_info) <pre>Thu Jan 18 09:11:27 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   46C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> In\u00a0[\u00a0]: Copied! <pre># View the assigned RAM memory\nfrom psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\nif ram_gb &lt; 20:\n  print('Not using a high-RAM runtime')\nelse:\n  print('You are using a high-RAM runtime!')\n</pre> # View the assigned RAM memory from psutil import virtual_memory ram_gb = virtual_memory().total / 1e9 print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))  if ram_gb &lt; 20:   print('Not using a high-RAM runtime') else:   print('You are using a high-RAM runtime!') <pre>Your runtime has 13.6 gigabytes of available RAM\n\nNot using a high-RAM runtime\n</pre> In\u00a0[\u00a0]: Copied! <pre># Install libraries and modules\n!pip install evaluate datasets transformers[sentencepiece]\n!pip install accelerate -U\n!pip install huggingface_hub\n</pre> # Install libraries and modules !pip install evaluate datasets transformers[sentencepiece] !pip install accelerate -U !pip install huggingface_hub <pre>Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\nRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.35.2)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]&gt;=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\nRequirement already satisfied: responses&lt;0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\nRequirement already satisfied: pyarrow&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\nRequirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.0)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.1)\nRequirement already satisfied: sentencepiece!=0.1.92,&gt;=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (23.2.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (6.0.4)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.9.4)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.4.1)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.3.1)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (4.0.3)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.7.0-&gt;evaluate) (4.5.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;evaluate) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;evaluate) (2023.3.post1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;evaluate) (1.16.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.1)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch&gt;=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.2)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (4.5.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (2023.6.0)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (2.1.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-&gt;accelerate) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub-&gt;accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.10.0-&gt;accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2023.11.17)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.10.0-&gt;accelerate) (1.3.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\nRequirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (2023.11.17)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Check transformers and accelerate modules version\nimport transformers\nimport accelerate\n\ntransformers.__version__, accelerate.__version__\n</pre> # Check transformers and accelerate modules version import transformers import accelerate  transformers.__version__, accelerate.__version__ Out[\u00a0]: <pre>('4.35.2', '0.26.1')</pre> In\u00a0[\u00a0]: Copied! <pre># Login to Huggingface_hub to access movie dataset\n# You require a read access token. For more information, check https://huggingface.co/docs/hub/security-tokens\nfrom huggingface_hub import notebook_login\nnotebook_login()\n</pre> # Login to Huggingface_hub to access movie dataset # You require a read access token. For more information, check https://huggingface.co/docs/hub/security-tokens from huggingface_hub import notebook_login notebook_login() <pre>VBox(children=(HTML(value='&lt;center&gt; &lt;img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026</pre> In\u00a0[\u00a0]: Copied! <pre># Import libraries and packages\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\nfrom datasets import load_dataset, Dataset\nfrom collections import Counter\nimport evaluate\n\nimport numpy as np\nimport pandas as pd\nfrom rich import print\n</pre> # Import libraries and packages from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding from datasets import load_dataset, Dataset from collections import Counter import evaluate  import numpy as np import pandas as pd from rich import print In\u00a0[\u00a0]: Copied! <pre># Load competition datasets\nraw_datasets = load_dataset(\"datadrivenscience/movie-genre-prediction\")\nraw_datasets\n</pre> # Load competition datasets raw_datasets = load_dataset(\"datadrivenscience/movie-genre-prediction\") raw_datasets <pre>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n</pre> <pre>Downloading readme:   0%|          | 0.00/1.19k [00:00&lt;?, ?B/s]</pre> <pre>Downloading data:   0%|          | 0.00/7.16M [00:00&lt;?, ?B/s]</pre> <pre>Downloading data:   0%|          | 0.00/4.74M [00:00&lt;?, ?B/s]</pre> <pre>Generating train split:   0%|          | 0/54000 [00:00&lt;?, ? examples/s]</pre> <pre>Generating test split:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'genre'],\n        num_rows: 54000\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'genre'],\n        num_rows: 36000\n    })\n})</pre> In\u00a0[\u00a0]: Copied! <pre># Explore train dataset\nraw_datasets[\"train\"].features\n</pre> # Explore train dataset raw_datasets[\"train\"].features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'genre': Value(dtype='string', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Explore train dataset\nraw_datasets[\"train\"][:5]\n</pre> # Explore train dataset raw_datasets[\"train\"][:5] Out[\u00a0]: <pre>{'id': [44978, 50185, 34131, 78522, 2206],\n 'movie_name': ['Super Me',\n  'Entity Project',\n  'Behavioral Family Therapy for Serious Psychiatric Disorders',\n  'Blood Glacier',\n  'Apat na anino'],\n 'synopsis': ['A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. Selling them makes him rich.',\n  'A director and her friends renting a haunted house to capture paranormal events in order to prove it and become popular.',\n  'This is an educational video for families and family therapists that describes the Behavioral Family Therapy approach to dealing with serious psychiatric illnesses.',\n  'Scientists working in the Austrian Alps discover that a glacier is leaking a liquid that appears to be affecting local wildlife.',\n  'Buy Day - Four Men Widely - Apart in Life - By Night Shadows United in One Fight Venting the Fire of their Fury Against the Hated Oppressors.'],\n 'genre': ['fantasy', 'horror', 'family', 'scifi', 'action']}</pre> In\u00a0[\u00a0]: Copied! <pre># Explore test dataset\nraw_datasets[\"test\"].features\n</pre> # Explore test dataset raw_datasets[\"test\"].features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'genre': Value(dtype='string', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Explore test dataset\nraw_datasets[\"test\"][:5]\n</pre> # Explore test dataset raw_datasets[\"test\"][:5] Out[\u00a0]: <pre>{'id': [16863, 48456, 41383, 84007, 40269],\n 'movie_name': ['A Death Sentence',\n  'Intermedio',\n  '30 Chua Phai Tet',\n  'Paranoiac',\n  'Ordinary Happiness'],\n 'synopsis': [\"12 y.o. Ida's dad'll die without a DKK1,500,000 operation. Ida plans to steal the money from the bank, her mom installed alarm systems in. She'll need her climbing skills, her 2 friends and 3 go-karts.\",\n  'A group of four teenage friends become trapped in a Mexican border tunnel where they fall prey, one-by one, to tortured ghosts who haunt it.',\n  \"A guy left his home for 12 years till he came back to claim what's his from his father, the vast Land, just to uncover that he had to live that day, year-end Lunar day, for another 12 years.\",\n  'A man long believed dead returns to the family estate to claim his inheritance.',\n  'After a deadly accident, Paolo comes back on Earth just 92 minutes more, thanks to a calculation error made in a paradise office.'],\n 'genre': ['action', 'action', 'action', 'action', 'action']}</pre> In\u00a0[\u00a0]: Copied! <pre># Identifying the existing genres in the train dataset\nlabels = set(raw_datasets[\"train\"][\"genre\"])\nnum_labels = len(labels)\nnum_labels, labels\n</pre> # Identifying the existing genres in the train dataset labels = set(raw_datasets[\"train\"][\"genre\"]) num_labels = len(labels) num_labels, labels Out[\u00a0]: <pre>(10,\n {'action',\n  'adventure',\n  'crime',\n  'family',\n  'fantasy',\n  'horror',\n  'mystery',\n  'romance',\n  'scifi',\n  'thriller'})</pre> In\u00a0[\u00a0]: Copied! <pre># Counting the number of movies per genre in the train dataset\nlabels_count = Counter(raw_datasets['train']['genre'])\nprint(labels_count)\n</pre> # Counting the number of movies per genre in the train dataset labels_count = Counter(raw_datasets['train']['genre']) print(labels_count) <pre>Counter({\n    'fantasy': 5400,\n    'horror': 5400,\n    'family': 5400,\n    'scifi': 5400,\n    'action': 5400,\n    'crime': 5400,\n    'adventure': 5400,\n    'mystery': 5400,\n    'romance': 5400,\n    'thriller': 5400\n})\n</pre> In\u00a0[\u00a0]: Copied! <pre># Counting the number of movies per genre in the test dataset\nlabels_count_test = Counter(raw_datasets['test']['genre'])\nprint(labels_count_test)\n</pre> # Counting the number of movies per genre in the test dataset labels_count_test = Counter(raw_datasets['test']['genre']) print(labels_count_test) <pre>Counter({'action': 36000})\n</pre> In\u00a0[\u00a0]: Copied! <pre># Rename \"genre\" column as \"labels\" in the train dataset and turn into a ClassLabel type\nraw_datasets = raw_datasets.rename_column('genre','labels')\nraw_datasets = raw_datasets.class_encode_column('labels')\nraw_datasets['train'].features\n</pre> # Rename \"genre\" column as \"labels\" in the train dataset and turn into a ClassLabel type raw_datasets = raw_datasets.rename_column('genre','labels') raw_datasets = raw_datasets.class_encode_column('labels') raw_datasets['train'].features <pre>Casting to class labels:   0%|          | 0/54000 [00:00&lt;?, ? examples/s]</pre> <pre>Casting to class labels:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['action', 'adventure', 'crime', 'family', 'fantasy', 'horror', 'mystery', 'romance', 'scifi', 'thriller'], id=None)}</pre> <p>Answer: The train dataset contains 10 genres that seemed to be evenly distributed accross the dataset. Meanwhile, the test dataset only contains action movies as dummy values prior inference.</p> In\u00a0[\u00a0]: Copied! <pre># Convert Datasets into Dataframes\nraw_datasets.set_format('pandas')\n</pre> # Convert Datasets into Dataframes raw_datasets.set_format('pandas') In\u00a0[\u00a0]: Copied! <pre># Convert Datasets into Dataframes\ntrain_dataset = raw_datasets['train'][:]\ntrain_dataset.info(memory_usage='deep')\n</pre> # Convert Datasets into Dataframes train_dataset = raw_datasets['train'][:] train_dataset.info(memory_usage='deep') <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 54000 entries, 0 to 53999\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          54000 non-null  int64 \n 1   movie_name  54000 non-null  object\n 2   synopsis    54000 non-null  object\n 3   labels      54000 non-null  int64 \ndtypes: int64(2), object(2)\nmemory usage: 15.4 MB\n</pre> In\u00a0[\u00a0]: Copied! <pre>train_dataset.head(3)\n</pre> train_dataset.head(3) Out[\u00a0]: id movie_name synopsis labels 0 44978 Super Me A young scriptwriter starts bringing valuable ... 4 1 50185 Entity Project A director and her friends renting a haunted h... 5 2 34131 Behavioral Family Therapy for Serious Psychiat... This is an educational video for families and ... 3 In\u00a0[\u00a0]: Copied! <pre># Drop duplicates from the train dataframe\ntrain_dataset = train_dataset.drop_duplicates(['movie_name', 'synopsis'])\ntrain_dataset.info(memory_usage = 'deep')\n</pre> # Drop duplicates from the train dataframe train_dataset = train_dataset.drop_duplicates(['movie_name', 'synopsis']) train_dataset.info(memory_usage = 'deep') <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 46344 entries, 0 to 53998\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          46344 non-null  int64 \n 1   movie_name  46344 non-null  object\n 2   synopsis    46344 non-null  object\n 3   labels      46344 non-null  int64 \ndtypes: int64(2), object(2)\nmemory usage: 13.8 MB\n</pre> <p>Answer: The train dataset cointained 7,656 duplicates.</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset.head(3)\n</pre> train_dataset.head(3) Out[\u00a0]: id movie_name synopsis labels 0 44978 Super Me A young scriptwriter starts bringing valuable ... 4 1 50185 Entity Project A director and her friends renting a haunted h... 5 2 34131 Behavioral Family Therapy for Serious Psychiat... This is an educational video for families and ... 3 In\u00a0[\u00a0]: Copied! <pre># Create a new column \"synopsis_len\" that contains the synopsis length\ntrain_dataset['synopsis_len'] = train_dataset['synopsis'].apply(lambda x: len(x))\ntrain_dataset.head(3)\n</pre> # Create a new column \"synopsis_len\" that contains the synopsis length train_dataset['synopsis_len'] = train_dataset['synopsis'].apply(lambda x: len(x)) train_dataset.head(3) Out[\u00a0]: id movie_name synopsis labels synopsis_len 0 44978 Super Me A young scriptwriter starts bringing valuable ... 4 141 1 50185 Entity Project A director and her friends renting a haunted h... 5 120 2 34131 Behavioral Family Therapy for Serious Psychiat... This is an educational video for families and ... 3 164 In\u00a0[\u00a0]: Copied! <pre># Create a new column \"movie_name_len\" that contains the length of movie_name\ntrain_dataset['movie_name_len'] = train_dataset['movie_name'].apply(lambda x: len(x))\ntrain_dataset.head(3)\n</pre> # Create a new column \"movie_name_len\" that contains the length of movie_name train_dataset['movie_name_len'] = train_dataset['movie_name'].apply(lambda x: len(x)) train_dataset.head(3) Out[\u00a0]: id movie_name synopsis labels synopsis_len movie_name_len 0 44978 Super Me A young scriptwriter starts bringing valuable ... 4 141 8 1 50185 Entity Project A director and her friends renting a haunted h... 5 120 14 2 34131 Behavioral Family Therapy for Serious Psychiat... This is an educational video for families and ... 3 164 59 In\u00a0[\u00a0]: Copied! <pre># Order train dataframe by synopsis_len\ntrain_dataset.sort_values(by='synopsis_len', ascending=False)\n</pre> # Order train dataframe by synopsis_len train_dataset.sort_values(by='synopsis_len', ascending=False) Out[\u00a0]: id movie_name synopsis labels synopsis_len movie_name_len 52518 46444 Final Destination Alex Browning is among a group of high school ... 5 400 17 49498 1468 Bhargava Ramudu Bhargava, an efficient, yet jobless young man ... 0 395 15 29141 71309 Krishnatulasi Krishna is a blind young man who works as a gu... 7 381 13 50834 44856 The Sex Cycle The Cocoa Poodle bar is the central meeting pl... 4 377 13 53370 4779 Uro Turning his back on a delinquent past and join... 0 370 3 ... ... ... ... ... ... ... 6891 71298 Qismat 2 Fortune 2. 7 10 8 38284 5454 Rader Invasion. 0 9 5 3301 15654 Adventure Night TBD 1 3 15 34698 42213 Dark Army NA. 4 3 9 26774 34314 Prima Ballerina TBA 3 3 15 <p>46344 rows \u00d7 6 columns</p> In\u00a0[\u00a0]: Copied! <pre>import plotly.figure_factory as ff\nfig = ff.create_distplot([train_dataset['synopsis_len']], ['length'], colors=['#2ca02c'])\nfig.update_layout(title_text='Word Count Distribution of Movie Synopsis')\nfig.show()\n</pre> import plotly.figure_factory as ff fig = ff.create_distplot([train_dataset['synopsis_len']], ['length'], colors=['#2ca02c']) fig.update_layout(title_text='Word Count Distribution of Movie Synopsis') fig.show() In\u00a0[\u00a0]: Copied! <pre>fig2 = ff.create_distplot([train_dataset['movie_name_len']], ['length'], colors=['#ffa408'])\nfig2.update_layout(title_text='Word Count Distribution of Movie Titles')\nfig2.show()\n</pre> fig2 = ff.create_distplot([train_dataset['movie_name_len']], ['length'], colors=['#ffa408']) fig2.update_layout(title_text='Word Count Distribution of Movie Titles') fig2.show() In\u00a0[\u00a0]: Copied! <pre>train_dataset['movie_name_len'].max(), train_dataset['synopsis_len'].max()\n</pre> train_dataset['movie_name_len'].max(), train_dataset['synopsis_len'].max() Out[\u00a0]: <pre>(180, 400)</pre> <p>The average length of characters in the movie name is 12. For the synopsis, we see two peaks around 145 and 230 characters. The maximum character size is 180 for the movie name and 400 for the synopsis. So, there won't be any issues during tokenization and training because the bert-base-uncased model supports a maximum character length of 512.</p> In\u00a0[\u00a0]: Copied! <pre># Convert the train_dataset Dataframe to DataSet format again\ntrain_ds = Dataset.from_pandas(train_dataset)\ntrain_ds.features\n</pre> # Convert the train_dataset Dataframe to DataSet format again train_ds = Dataset.from_pandas(train_dataset) train_ds.features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': Value(dtype='int64', id=None),\n 'synopsis_len': Value(dtype='int64', id=None),\n 'movie_name_len': Value(dtype='int64', id=None),\n '__index_level_0__': Value(dtype='int64', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Turn \"labels\" column into ClassLabel type\ntrain_ds = train_ds.class_encode_column('labels')\ntrain_ds.features\n</pre> # Turn \"labels\" column into ClassLabel type train_ds = train_ds.class_encode_column('labels') train_ds.features <pre>Stringifying the column:   0%|          | 0/46344 [00:00&lt;?, ? examples/s]</pre> <pre>Casting to class labels:   0%|          | 0/46344 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], id=None),\n 'synopsis_len': Value(dtype='int64', id=None),\n 'movie_name_len': Value(dtype='int64', id=None),\n '__index_level_0__': Value(dtype='int64', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Create tokenizer\n# i.e. bert-base-uncased, bert-large-uncased, bert-large-uncased-whole-word-masking\ncheckpoint = 'bert-large-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenizer\n</pre> # Create tokenizer # i.e. bert-base-uncased, bert-large-uncased, bert-large-uncased-whole-word-masking checkpoint = 'bert-large-uncased' tokenizer = AutoTokenizer.from_pretrained(checkpoint) tokenizer <pre>tokenizer_config.json:   0%|          | 0.00/28.0 [00:00&lt;?, ?B/s]</pre> <pre>config.json:   0%|          | 0.00/571 [00:00&lt;?, ?B/s]</pre> <pre>vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]</pre> <pre>tokenizer.json:   0%|          | 0.00/466k [00:00&lt;?, ?B/s]</pre> Out[\u00a0]: <pre>BertTokenizerFast(name_or_path='bert-large-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}</pre> In\u00a0[\u00a0]: Copied! <pre># Do a sample tokenization\nsample_tokenized = tokenizer(train_ds['movie_name'][0], train_ds['synopsis'][0])\ntokenizer.decode(sample_tokenized['input_ids'])\n</pre> # Do a sample tokenization sample_tokenized = tokenizer(train_ds['movie_name'][0], train_ds['synopsis'][0]) tokenizer.decode(sample_tokenized['input_ids']) Out[\u00a0]: <pre>'[CLS] super me [SEP] a young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. selling them makes him rich. [SEP]'</pre> In\u00a0[\u00a0]: Copied! <pre>sample_tokenized\n</pre> sample_tokenized Out[\u00a0]: <pre>{'input_ids': [101, 3565, 2033, 102, 1037, 2402, 5896, 15994, 4627, 5026, 7070, 5200, 2067, 2013, 2010, 2460, 15446, 1997, 2108, 13303, 2011, 1037, 5698, 1012, 4855, 2068, 3084, 2032, 4138, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</pre> In\u00a0[\u00a0]: Copied! <pre># Split Train Dataset (train_ds) into training and test datasets\ntrain_ds = train_ds.train_test_split(test_size=0.1, stratify_by_column=\"labels\")\ntrain_ds\n</pre> # Split Train Dataset (train_ds) into training and test datasets train_ds = train_ds.train_test_split(test_size=0.1, stratify_by_column=\"labels\") train_ds Out[\u00a0]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', 'synopsis_len', 'movie_name_len', '__index_level_0__'],\n        num_rows: 41709\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', 'synopsis_len', 'movie_name_len', '__index_level_0__'],\n        num_rows: 4635\n    })\n})</pre> In\u00a0[\u00a0]: Copied! <pre># Define a tokenize function\ndef tokenize(ds):\n  return tokenizer(ds['movie_name'], ds['synopsis'], truncation=True)\n</pre> # Define a tokenize function def tokenize(ds):   return tokenizer(ds['movie_name'], ds['synopsis'], truncation=True) In\u00a0[\u00a0]: Copied! <pre># Tokenize train_ds\ntokenized_datasets = train_ds.map(tokenize, batched=True)\ntokenized_datasets\n</pre> # Tokenize train_ds tokenized_datasets = train_ds.map(tokenize, batched=True) tokenized_datasets <pre>Map:   0%|          | 0/41709 [00:00&lt;?, ? examples/s]</pre> <pre>Map:   0%|          | 0/4635 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', 'synopsis_len', 'movie_name_len', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 41709\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', 'synopsis_len', 'movie_name_len', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 4635\n    })\n})</pre> In\u00a0[\u00a0]: Copied! <pre># Select a random sample to verify tokenization\ntokenizer.decode(tokenized_datasets['train']['input_ids'][37074])\n</pre> # Select a random sample to verify tokenization tokenizer.decode(tokenized_datasets['train']['input_ids'][37074]) Out[\u00a0]: <pre>\"[CLS] deadliest cause [SEP] an aspiring environmentalist who is reluctantly working on a bering sea crabber soon suspects the cranky skipper is the biological father he's never known. genre : adventure / drama / environmental / climate change. [SEP]\"</pre> In\u00a0[\u00a0]: Copied! <pre># Removing columns the model doesn't expect\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'movie_name', 'synopsis', 'synopsis_len', 'movie_name_len','__index_level_0__'])\ntokenized_datasets['train'].column_names\n</pre> # Removing columns the model doesn't expect tokenized_datasets = tokenized_datasets.remove_columns(['id', 'movie_name', 'synopsis', 'synopsis_len', 'movie_name_len','__index_level_0__']) tokenized_datasets['train'].column_names Out[\u00a0]: <pre>['labels', 'input_ids', 'token_type_ids', 'attention_mask']</pre> In\u00a0[\u00a0]: Copied! <pre># Setting the datasets format so that they can return Pytorch tensors\ntokenized_datasets.set_format(\"torch\")\n</pre> # Setting the datasets format so that they can return Pytorch tensors tokenized_datasets.set_format(\"torch\") In\u00a0[\u00a0]: Copied! <pre># Define a data_collator function for dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n</pre> # Define a data_collator function for dynamic padding data_collator = DataCollatorWithPadding(tokenizer=tokenizer) In\u00a0[\u00a0]: Copied! <pre># Define DataLoaders\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(\n    tokenized_datasets['train'], shuffle=True, batch_size=32, collate_fn=data_collator\n)\n\neval_dataloader = DataLoader(\n    tokenized_datasets['test'], batch_size=64, collate_fn=data_collator\n)\n</pre> # Define DataLoaders from torch.utils.data import DataLoader  train_dataloader = DataLoader(     tokenized_datasets['train'], shuffle=True, batch_size=32, collate_fn=data_collator )  eval_dataloader = DataLoader(     tokenized_datasets['test'], batch_size=64, collate_fn=data_collator ) In\u00a0[\u00a0]: Copied! <pre># Inspect a batch from train_dataloader\nfor batch in train_dataloader:\n  break\n{k:v.shape for k,v in batch.items()}\n</pre> # Inspect a batch from train_dataloader for batch in train_dataloader:   break {k:v.shape for k,v in batch.items()} <pre>You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n</pre> Out[\u00a0]: <pre>{'labels': torch.Size([32]),\n 'input_ids': torch.Size([32, 69]),\n 'token_type_ids': torch.Size([32, 69]),\n 'attention_mask': torch.Size([32, 69])}</pre> In\u00a0[\u00a0]: Copied! <pre># Inspect a batch from train_dataloader\nbatch.input_ids\n</pre> # Inspect a batch from train_dataloader batch.input_ids Out[\u00a0]: <pre>tensor([[  101,  1996, 22260,  ...,     0,     0,     0],\n        [  101, 13970,  2386,  ...,     0,     0,     0],\n        [  101, 17359,  8524,  ...,     0,     0,     0],\n        ...,\n        [  101,  2191,  2039,  ...,     0,     0,     0],\n        [  101,  2720,  1012,  ...,     0,     0,     0],\n        [  101,  2619,  2569,  ...,     0,     0,     0]])</pre> In\u00a0[\u00a0]: Copied! <pre>from accelerate import Accelerator\nfrom transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n</pre> from accelerate import Accelerator from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler In\u00a0[\u00a0]: Copied! <pre>model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n</pre> model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels) <pre>model.safetensors:   0%|          | 0.00/1.34G [00:00&lt;?, ?B/s]</pre> <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Passing a single batch to our model to check that everything is OK\noutputs = model(**batch)\nprint(outputs.loss, outputs.logits.shape)\n</pre> # Passing a single batch to our model to check that everything is OK outputs = model(**batch) print(outputs.loss, outputs.logits.shape) <pre>tensor(2.3844, grad_fn=&lt;NllLossBackward0&gt;)\ntorch.Size([32, 10])\n</pre> In\u00a0[\u00a0]: Copied! <pre># Instantiate a new model\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n</pre> # Instantiate a new model model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels) <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Setting an Optimizer and Accelerator\noptimizer = AdamW(model.parameters(), lr=1e-5)\naccelerator = Accelerator()\n\n# Prepare data for accelerator\ntrain_dl, eval_dl, model, optimizer = accelerator.prepare(\n    train_dataloader, eval_dataloader, model, optimizer\n)\n</pre> # Setting an Optimizer and Accelerator optimizer = AdamW(model.parameters(), lr=1e-5) accelerator = Accelerator()  # Prepare data for accelerator train_dl, eval_dl, model, optimizer = accelerator.prepare(     train_dataloader, eval_dataloader, model, optimizer ) <pre>/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning:\n\nThis implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Setting a get_scheduler function\n# Choose one of the following schedulers type to configure the get_scheduler function\n\n# Linear scheduler\nnum_epochs = 2\nnum_training_steps = num_epochs * len(train_dl)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\nprint(num_training_steps)\n</pre> # Setting a get_scheduler function # Choose one of the following schedulers type to configure the get_scheduler function  # Linear scheduler num_epochs = 2 num_training_steps = num_epochs * len(train_dl) lr_scheduler = get_scheduler(     \"linear\",     optimizer=optimizer,     num_warmup_steps=0,     num_training_steps=num_training_steps ) print(num_training_steps) <pre>2608\n</pre> In\u00a0[\u00a0]: Copied! <pre># Linear scheduler with warmup (get_linear_schedule_with_warmup)\n#num_epochs = 2\n#num_training_steps = num_epochs * len(train_dl)\n#lr_scheduler = get_linear_schedule_with_warmup (\n#    optimizer=optimizer,\n#    num_warmup_steps=num_training_steps*0.1,\n#    num_training_steps=num_training_steps\n#)\n#print(num_training_steps)\n</pre> # Linear scheduler with warmup (get_linear_schedule_with_warmup) #num_epochs = 2 #num_training_steps = num_epochs * len(train_dl) #lr_scheduler = get_linear_schedule_with_warmup ( #    optimizer=optimizer, #    num_warmup_steps=num_training_steps*0.1, #    num_training_steps=num_training_steps #) #print(num_training_steps) In\u00a0[\u00a0]: Copied! <pre># Cosine scheduler (get_cosine_schedule_with_warmup)\n#num_epochs = 2\n#num_training_steps = num_epochs * len(train_dl)\n#lr_scheduler = get_cosine_schedule_with_warmup(\n#    optimizer=optimizer,\n#    num_warmup_steps=num_training_steps*0.1,\n#    num_training_steps=num_training_steps\n#)\n#print(num_training_steps)\n</pre> # Cosine scheduler (get_cosine_schedule_with_warmup) #num_epochs = 2 #num_training_steps = num_epochs * len(train_dl) #lr_scheduler = get_cosine_schedule_with_warmup( #    optimizer=optimizer, #    num_warmup_steps=num_training_steps*0.1, #    num_training_steps=num_training_steps #) #print(num_training_steps) In\u00a0[\u00a0]: Copied! <pre># Cosine scheduler (get_cosine_with_hard_restarts_schedule_with_warmup)\n#num_epochs = 2\n#num_training_steps = num_epochs * len(train_dl)\n#lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n#    optimizer=optimizer,\n#    num_warmup_steps=num_training_steps*0.1,\n#    num_training_steps=num_training_steps,\n#    #num_cycles=1\n#)\n#print(num_training_steps)\n</pre> # Cosine scheduler (get_cosine_with_hard_restarts_schedule_with_warmup) #num_epochs = 2 #num_training_steps = num_epochs * len(train_dl) #lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup( #    optimizer=optimizer, #    num_warmup_steps=num_training_steps*0.1, #    num_training_steps=num_training_steps, #    #num_cycles=1 #) #print(num_training_steps) In\u00a0[\u00a0]: Copied! <pre># Verifying infrastructure settings\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice\n</pre> # Verifying infrastructure settings import torch device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") model.to(device) device Out[\u00a0]: <pre>device(type='cuda')</pre> In\u00a0[\u00a0]: Copied! <pre># Model training and evaluation\nfrom tqdm.auto import tqdm\n\nmetric = evaluate.load(\"accuracy\")\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_epochs):\n  # Training\n  model.train()\n  for batch in train_dl:\n    outputs = model(**batch)\n    loss = outputs.loss\n    accelerator.backward(loss)\n\n    optimizer.step()\n    lr_scheduler.step()\n    optimizer.zero_grad()\n    progress_bar.update(1)\n\n  # Evaluation\n  model.eval()\n  for batch in eval_dl:\n    with torch.no_grad():\n      outputs = model(**batch)\n\n    predictions = outputs.logits.argmax(dim=-1)\n    labels = batch[\"labels\"]\n\n    predictions_gathered = accelerator.gather(predictions)\n    labels_gathered = accelerator.gather(labels)\n    metric.add_batch(predictions=predictions_gathered, references=labels_gathered)\n\n  results = metric.compute()\n  print(f\"epoch {epoch}: {results['accuracy']}\")\n</pre> # Model training and evaluation from tqdm.auto import tqdm  metric = evaluate.load(\"accuracy\") progress_bar = tqdm(range(num_training_steps))  for epoch in range(num_epochs):   # Training   model.train()   for batch in train_dl:     outputs = model(**batch)     loss = outputs.loss     accelerator.backward(loss)      optimizer.step()     lr_scheduler.step()     optimizer.zero_grad()     progress_bar.update(1)    # Evaluation   model.eval()   for batch in eval_dl:     with torch.no_grad():       outputs = model(**batch)      predictions = outputs.logits.argmax(dim=-1)     labels = batch[\"labels\"]      predictions_gathered = accelerator.gather(predictions)     labels_gathered = accelerator.gather(labels)     metric.add_batch(predictions=predictions_gathered, references=labels_gathered)    results = metric.compute()   print(f\"epoch {epoch}: {results['accuracy']}\") <pre>Downloading builder script:   0%|          | 0.00/4.20k [00:00&lt;?, ?B/s]</pre> <pre>  0%|          | 0/2608 [00:00&lt;?, ?it/s]</pre> <pre>epoch 0: 0.45674217907227616\n</pre> <pre>epoch 1: 0.46364617044228695\n</pre> In\u00a0[\u00a0]: Copied! <pre># Inspect test dataset\nraw_datasets['test'].features\n</pre> # Inspect test dataset raw_datasets['test'].features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['action'], id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Convert test dataset into a dataframe\ntest_raw_dataset = raw_datasets['test'][:]\ntest_raw_dataset.info(memory_usage='deep')\n</pre> # Convert test dataset into a dataframe test_raw_dataset = raw_datasets['test'][:] test_raw_dataset.info(memory_usage='deep') <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 36000 entries, 0 to 35999\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          36000 non-null  int64 \n 1   movie_name  36000 non-null  object\n 2   synopsis    36000 non-null  object\n 3   labels      36000 non-null  int64 \ndtypes: int64(2), object(2)\nmemory usage: 10.3 MB\n</pre> In\u00a0[\u00a0]: Copied! <pre># Turn test dataframe into a Dataset format again\ntest_ds = Dataset.from_pandas(test_raw_dataset)\ntest_ds.features\n</pre> # Turn test dataframe into a Dataset format again test_ds = Dataset.from_pandas(test_raw_dataset) test_ds.features Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': Value(dtype='int64', id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Turn \"labels\" column into a ClassType format\ntest_ds = test_ds.class_encode_column('labels')\ntest_ds.features\n</pre> # Turn \"labels\" column into a ClassType format test_ds = test_ds.class_encode_column('labels') test_ds.features <pre>Stringifying the column:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> <pre>Casting to class labels:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> Out[\u00a0]: <pre>{'id': Value(dtype='int64', id=None),\n 'movie_name': Value(dtype='string', id=None),\n 'synopsis': Value(dtype='string', id=None),\n 'labels': ClassLabel(names=['0'], id=None)}</pre> In\u00a0[\u00a0]: Copied! <pre># Tokenize test dataset\ntokenized_test_ds = test_ds.map(tokenize, batched=True)\n</pre> # Tokenize test dataset tokenized_test_ds = test_ds.map(tokenize, batched=True) <pre>Map:   0%|          | 0/36000 [00:00&lt;?, ? examples/s]</pre> In\u00a0[\u00a0]: Copied! <pre># Inspect the content of the test dataset\ntokenized_test_ds.column_names\n</pre> # Inspect the content of the test dataset tokenized_test_ds.column_names Out[\u00a0]: <pre>['id',\n 'movie_name',\n 'synopsis',\n 'labels',\n 'input_ids',\n 'token_type_ids',\n 'attention_mask']</pre> In\u00a0[\u00a0]: Copied! <pre># Create a copy of the original test dataset\nfrom copy import deepcopy\ntokenized_test_ds_copy = deepcopy(tokenized_test_ds)\n</pre> # Create a copy of the original test dataset from copy import deepcopy tokenized_test_ds_copy = deepcopy(tokenized_test_ds) In\u00a0[\u00a0]: Copied! <pre># Remove columns the model doesn't expect\ntokenized_test_ds_copy = tokenized_test_ds_copy.remove_columns(['id', 'movie_name', 'synopsis'])\ntokenized_test_ds_copy.column_names\n</pre> # Remove columns the model doesn't expect tokenized_test_ds_copy = tokenized_test_ds_copy.remove_columns(['id', 'movie_name', 'synopsis']) tokenized_test_ds_copy.column_names Out[\u00a0]: <pre>['labels', 'input_ids', 'token_type_ids', 'attention_mask']</pre> In\u00a0[\u00a0]: Copied! <pre># Define a DataLoader for the test dataset\ntest_dataloader = DataLoader(\n    tokenized_test_ds_copy, batch_size=64, collate_fn=data_collator\n)\n</pre> # Define a DataLoader for the test dataset test_dataloader = DataLoader(     tokenized_test_ds_copy, batch_size=64, collate_fn=data_collator ) In\u00a0[\u00a0]: Copied! <pre># Inspect a batch from test_dataloader\nfor batch in test_dataloader:\n  break\n{k:v.shape for k,v in batch.items()}\n</pre> # Inspect a batch from test_dataloader for batch in test_dataloader:   break {k:v.shape for k,v in batch.items()}  Out[\u00a0]: <pre>{'labels': torch.Size([64]),\n 'input_ids': torch.Size([64, 73]),\n 'token_type_ids': torch.Size([64, 73]),\n 'attention_mask': torch.Size([64, 73])}</pre> In\u00a0[\u00a0]: Copied! <pre># Specify a device type since we aren't using accelerate for the prediction stage\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice\n</pre> # Specify a device type since we aren't using accelerate for the prediction stage device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") model.to(device) device Out[\u00a0]: <pre>device(type='cuda')</pre> In\u00a0[\u00a0]: Copied! <pre># Run the model to get predictions\nnum_eval_steps = len(test_dataloader)\nprogress_bar = tqdm(range(num_eval_steps))\n\npredictions = []\nmodel.eval()\nfor batch in test_dataloader:\n  batch = {k:v.to(device) for k,v in batch.items()}\n  with torch.no_grad():\n    outputs = model(**batch)\n\n  batch_predictions = outputs.logits.argmax(dim=-1).tolist()\n  predictions.extend(batch_predictions)\n  progress_bar.update(1)\n</pre> # Run the model to get predictions num_eval_steps = len(test_dataloader) progress_bar = tqdm(range(num_eval_steps))  predictions = [] model.eval() for batch in test_dataloader:   batch = {k:v.to(device) for k,v in batch.items()}   with torch.no_grad():     outputs = model(**batch)    batch_predictions = outputs.logits.argmax(dim=-1).tolist()   predictions.extend(batch_predictions)   progress_bar.update(1) <pre>  0%|          | 0/563 [00:00&lt;?, ?it/s]</pre> In\u00a0[\u00a0]: Copied! <pre># Display some predictions\nprint(predictions[:20])\n</pre> # Display some predictions print(predictions[:20]) <pre>[3, 5, 7, 6, 8, 1, 9, 2, 5, 4, 0, 7, 2, 3, 5, 0, 3, 3, 9, 8]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Convert predictions to their string representations based on the mapping defined in the 'labels' feature.\npredicted_genre = raw_datasets['train'].features['labels'].int2str(predictions)\n</pre> # Convert predictions to their string representations based on the mapping defined in the 'labels' feature. predicted_genre = raw_datasets['train'].features['labels'].int2str(predictions) In\u00a0[\u00a0]: Copied! <pre># Create a dataframe specifying movie id and genre\ndf = pd.DataFrame({'id':tokenized_test_ds['id'], 'genre':predicted_genre})\ndf.head(3)\n</pre> # Create a dataframe specifying movie id and genre df = pd.DataFrame({'id':tokenized_test_ds['id'], 'genre':predicted_genre}) df.head(3) Out[\u00a0]: id genre 0 16863 family 1 48456 horror 2 41383 romance In\u00a0[\u00a0]: Copied! <pre># Save results to a csv file\ndf.to_csv('submission.csv')\n</pre> # Save results to a csv file df.to_csv('submission.csv') In\u00a0[\u00a0]: Copied! <pre># Check whether you're up-to-date with git\n!git --version\n</pre> # Check whether you're up-to-date with git !git --version <pre>git version 2.34.1\n</pre> In\u00a0[\u00a0]: Copied! <pre># Use a token with writing permission on the Hugging Face Hub\n# For more information, check https://huggingface.co/docs/hub/security-tokens\nfrom huggingface_hub import notebook_login\nnotebook_login()\n</pre> # Use a token with writing permission on the Hugging Face Hub # For more information, check https://huggingface.co/docs/hub/security-tokens from huggingface_hub import notebook_login notebook_login() <pre>VBox(children=(HTML(value='&lt;center&gt; &lt;img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026</pre> In\u00a0[\u00a0]: Copied! <pre># To succesfully complete this taks, you need to create a new repository in your Hugging Face account.\n#\u00a0Then you will pass on the name of the newly created repository to the variable \"repo_name\" as indicated below\n\nfrom huggingface_hub import Repository\nrepo_name = \"sblaizer/saving-a-model-to-the-hf-hub\"\noutput_dir = \"bert-large-uncased-movie-prediction\"\nrepo = Repository(output_dir, clone_from=repo_name)\n</pre> # To succesfully complete this taks, you need to create a new repository in your Hugging Face account. #\u00a0Then you will pass on the name of the newly created repository to the variable \"repo_name\" as indicated below  from huggingface_hub import Repository repo_name = \"sblaizer/saving-a-model-to-the-hf-hub\" output_dir = \"bert-large-uncased-movie-prediction\" repo = Repository(output_dir, clone_from=repo_name) <pre>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning:\n\n'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\nFor more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n\nCloning https://huggingface.co/sblaizer/saving-a-model-to-the-hf-hub into local empty directory.\nWARNING:huggingface_hub.repository:Cloning https://huggingface.co/sblaizer/saving-a-model-to-the-hf-hub into local empty directory.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Save the model and its tokenizer\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n</pre> # Save the model and its tokenizer model.save_pretrained(output_dir) tokenizer.save_pretrained(output_dir) Out[\u00a0]: <pre>('bert-large-uncased-movie-prediction/tokenizer_config.json',\n 'bert-large-uncased-movie-prediction/special_tokens_map.json',\n 'bert-large-uncased-movie-prediction/vocab.txt',\n 'bert-large-uncased-movie-prediction/added_tokens.json',\n 'bert-large-uncased-movie-prediction/tokenizer.json')</pre> In\u00a0[\u00a0]: Copied! <pre># Push changes to the Hugging Face Hub\nrepo.push_to_hub(commit_message=\"Add model and tokenizer files\")\n</pre> # Push changes to the Hugging Face Hub repo.push_to_hub(commit_message=\"Add model and tokenizer files\") <pre>Upload file model.safetensors:   0%|          | 1.00/1.25G [00:00&lt;?, ?B/s]</pre> <pre>To https://huggingface.co/sblaizer/saving-a-model-to-the-hf-hub\n   ce70aee..5b65067  main -&gt; main\n\nWARNING:huggingface_hub.repository:To https://huggingface.co/sblaizer/saving-a-model-to-the-hf-hub\n   ce70aee..5b65067  main -&gt; main\n\n</pre> Out[\u00a0]: <pre>'https://huggingface.co/sblaizer/saving-a-model-to-the-hf-hub/commit/5b650670d5e6d4aad73e39d33b0810aebcd589a0'</pre>"},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#movie-genre-prediction-using-robust-model-architectures","title":"Movie Genre Prediction using Robust Model Architectures\u00b6","text":"<p>by Salomon Marquez</p> <p>01/10/2024</p>"},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#loading-movie-datasets","title":"Loading Movie Datasets\u00b6","text":""},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#what-are-the-existing-movie-genres","title":"What are the existing movie genres?\u00b6","text":""},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#removing-duplicated-items","title":"Removing duplicated items\u00b6","text":""},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#analyzing-text-movie-titles-and-their-synopses","title":"Analyzing text movie titles and their synopses\u00b6","text":"<p>This section analysis the length of movie titles and their synopses.</p>"},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#tokenization","title":"Tokenization\u00b6","text":""},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#preparing-data-for-the-training-stage","title":"Preparing data for the training stage\u00b6","text":""},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#full-training-loop-with-accelerate","title":"Full training loop with accelerate\u00b6","text":""},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#predicting-results-from-the-test-dataset","title":"Predicting results from the test dataset\u00b6","text":"<p>Now that we have fine-tuned our classification model, it's time to check how it performs on the test dataset. Since we aren't using the Trainer API, it's necessary to pre-process data of the test dataset.</p>"},{"location":"natural-language-processing/higher_order_huggingface_models_for_movie_predictions/#saving-the-model-to-the-hugging-face-hub","title":"Saving the model to the Hugging Face Hub\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/","title":"In Depth Analysis of Kaggle and Arxiv Datasets","text":"<p>This notebook is a follow-up to the EDA Kaggle and Arxiv datasets. The aim is to dive deeper into the following tasks:</p> <ul> <li>Integrate all text data-related competitions (9) from the past two years into the metadata analysis of the Kaggle write-ups. In the first EDA, we only referred to five competitions.</li> <li>Analyze the Arxiv dataset in greater detail to compare the insights gained in academia with those learned from text data write-ups reported in A Journey Through Text Data Competitions.</li> <li>Take a step further by using the PKE model to extract keywords from both Kaggle write-ups and Arxiv datasets, considering n-gram candidates, stopwords, and integrating a function to compute idf weights.</li> <li>Present results using resources other than horizontal bar plots, such as stylecloud and n-gram plots.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n</pre> # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory  import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename))  # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\"  # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session In\u00a0[\u00a0]: Copied! <pre># Installing Modules\n!pip install git+https://github.com/boudinfl/pke.git\n!pip install stylecloud wordcloud\n</pre> # Installing Modules !pip install git+https://github.com/boudinfl/pke.git !pip install stylecloud wordcloud In\u00a0[\u00a0]: Copied! <pre># Library Definition\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nimport re\n\nimport string\nfrom string import punctuation\nimport pke\nfrom pke import compute_document_frequency\n\nimport stylecloud\nfrom PIL import Image\nfrom IPython.display import Image\n\nimport gc\n</pre> # Library Definition import numpy as np  import pandas as pd import seaborn as sns import matplotlib.pyplot as plt   import re  import string from string import punctuation import pke from pke import compute_document_frequency  import stylecloud from PIL import Image from IPython.display import Image  import gc In\u00a0[\u00a0]: Copied! <pre># Reading the Kaggle writeups dataset\nwriteup_df = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/kaggle_writeups_20230510.csv\", parse_dates=[0,3]) # Consider the first four columns as date-format ones\nwriteup_df.head(3)\n</pre> # Reading the Kaggle writeups dataset writeup_df = pd.read_csv(\"/kaggle/input/2023-kaggle-ai-report/kaggle_writeups_20230510.csv\", parse_dates=[0,3]) # Consider the first four columns as date-format ones writeup_df.head(3) In\u00a0[\u00a0]: Copied! <pre>writeup_df.info(memory_usage='deep')\n</pre> writeup_df.info(memory_usage='deep') In\u00a0[\u00a0]: Copied! <pre>print(\"Number of writeups in the dataset: \" + str(len(writeup_df)))\n</pre> print(\"Number of writeups in the dataset: \" + str(len(writeup_df))) <p>The Kaggle writeups dataset contains a total of 3,127 writeups and uses 22 MB of memory</p> In\u00a0[\u00a0]: Copied! <pre>num_competitions = writeup_df[\"Title of Competition\"].nunique()\nprint(f\"The dataset has {num_competitions} competitions.\")\n</pre> num_competitions = writeup_df[\"Title of Competition\"].nunique() print(f\"The dataset has {num_competitions} competitions.\") <p>Answer: The Kaggle writeups dataset includes 310 competitions with a total of 3,127 writeups</p> In\u00a0[\u00a0]: Copied! <pre>early_comp = writeup_df[\"Competition Launch Date\"].min().strftime('%Y-%m-%d')\nlate_comp = writeup_df[\"Competition Launch Date\"].max().strftime('%Y-%m-%d')\n\nprint(f\"The earliest competition is {early_comp} \\nThe latest competition is {late_comp}\")\n</pre> early_comp = writeup_df[\"Competition Launch Date\"].min().strftime('%Y-%m-%d') late_comp = writeup_df[\"Competition Launch Date\"].max().strftime('%Y-%m-%d')  print(f\"The earliest competition is {early_comp} \\nThe latest competition is {late_comp}\") <p>Answer: The dates of the Kaggle competitions range from 2010-08-03 to 2023-02-23</p> In\u00a0[\u00a0]: Copied! <pre>writeup_past2years_df = writeup_df[(writeup_df[\"Competition Launch Date\"].dt.year &gt;= 2021) &amp; (writeup_df[\"Competition Launch Date\"].dt.month &gt;= 1)]\nnumcomp_past2years = writeup_past2years_df['Title of Competition'].nunique()\n\nprint(f\"Number of competitions from the past two years is {numcomp_past2years}\")\n</pre> writeup_past2years_df = writeup_df[(writeup_df[\"Competition Launch Date\"].dt.year &gt;= 2021) &amp; (writeup_df[\"Competition Launch Date\"].dt.month &gt;= 1)] numcomp_past2years = writeup_past2years_df['Title of Competition'].nunique()  print(f\"Number of competitions from the past two years is {numcomp_past2years}\") In\u00a0[\u00a0]: Copied! <pre>len(writeup_past2years_df)\n</pre> len(writeup_past2years_df) In\u00a0[\u00a0]: Copied! <pre>writeup_past2years_df[\"Title of Competition\"].unique()\n</pre> writeup_past2years_df[\"Title of Competition\"].unique() <p>Answer: There are 71 competitions held within the past two years (from January 2021 to February 2023) having 1,073 writeups.</p> In\u00a0[\u00a0]: Copied! <pre>writeup_past2years_df[\"Title of Competition\"].value_counts().head()\n</pre> writeup_past2years_df[\"Title of Competition\"].value_counts().head() In\u00a0[\u00a0]: Copied! <pre>writeup_past2years_df[\"Title of Competition\"].value_counts().tail()\n</pre> writeup_past2years_df[\"Title of Competition\"].value_counts().tail() <p>Answers: The Feedback Prize - English Language Learning and Jigsaw Rate Severity of Toxic Comments competitions take the lead (both are text-related competitions) whereas the Herbarium 2021 and Herbarium 2022 competitions have only one writeup.</p> In\u00a0[\u00a0]: Copied! <pre>textdata_df = pd.read_csv(\"/kaggle/input/top-3-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport - Text Data Write-ups 27.csv\")\ntextdata_df.head(3)\n</pre> textdata_df = pd.read_csv(\"/kaggle/input/top-3-kaggle-text-data-competitions-2021-2023/Summary_27write-ups_AIreport - Text Data Write-ups 27.csv\") textdata_df.head(3) In\u00a0[\u00a0]: Copied! <pre>textdata_df[\"Competition\"].unique()\n</pre> textdata_df[\"Competition\"].unique() In\u00a0[\u00a0]: Copied! <pre># Turning unique competitions into a list \nlist_textdata_comp = list(textdata_df[\"Competition\"].unique())\n</pre> # Turning unique competitions into a list  list_textdata_comp = list(textdata_df[\"Competition\"].unique()) In\u00a0[\u00a0]: Copied! <pre># Correcting middle dash typos of the list\nlist_textdata_comp[0] = 'Feedback Prize - Predicting Effective Arguments'\nlist_textdata_comp[3] = 'Feedback Prize - Evaluating Student Writing'\nlist_textdata_comp[5] = 'chaii - Hindi and Tamil Question Answering'\nlist_textdata_comp[7] = 'Coleridge Initiative - Show US the Data'\nlist_textdata_comp[8] = 'NBME - Score Clinical Patient Notes'\n\nlist_textdata_comp\n</pre> # Correcting middle dash typos of the list list_textdata_comp[0] = 'Feedback Prize - Predicting Effective Arguments' list_textdata_comp[3] = 'Feedback Prize - Evaluating Student Writing' list_textdata_comp[5] = 'chaii - Hindi and Tamil Question Answering' list_textdata_comp[7] = 'Coleridge Initiative - Show US the Data' list_textdata_comp[8] = 'NBME - Score Clinical Patient Notes'  list_textdata_comp In\u00a0[\u00a0]: Copied! <pre># Filtering out text data related competitions from the writeups of the past two years\ntext_past2years_df = writeup_past2years_df[writeup_past2years_df[\"Title of Competition\"].isin(list_textdata_comp)].copy()\ntext_past2years_df[\"Title of Competition\"].unique()\n</pre> # Filtering out text data related competitions from the writeups of the past two years text_past2years_df = writeup_past2years_df[writeup_past2years_df[\"Title of Competition\"].isin(list_textdata_comp)].copy() text_past2years_df[\"Title of Competition\"].unique() In\u00a0[\u00a0]: Copied! <pre>print(f\"Total writeups from the past two years: {len(writeup_past2years_df)}\")\nprint(f\"Total writeups related to text data competitions from the past two years: {len(text_past2years_df)}\")\n</pre> print(f\"Total writeups from the past two years: {len(writeup_past2years_df)}\") print(f\"Total writeups related to text data competitions from the past two years: {len(text_past2years_df)}\") In\u00a0[\u00a0]: Copied! <pre>text_past2years_df = text_past2years_df.reset_index(drop=True)\ntext_past2years_df.sort_values(by='Competition Launch Date', ascending=True)\n</pre> text_past2years_df = text_past2years_df.reset_index(drop=True) text_past2years_df.sort_values(by='Competition Launch Date', ascending=True) <p>Response: There are 9 competitions related to text data spanning from March 2021 to May 2022 having a total of 208 writeups.</p> In\u00a0[\u00a0]: Copied! <pre>text_past2years_df[\"Title of Competition\"].value_counts()\n</pre> text_past2years_df[\"Title of Competition\"].value_counts() <p>Response: Jigsaw Rate Severity of Toxic Comments takes the lead with 33 writeups</p> <p>Let's have a look at the format of a single writeup:</p> In\u00a0[\u00a0]: Copied! <pre>text_past2years_df[\"Writeup\"][0]\n</pre> text_past2years_df[\"Writeup\"][0] In\u00a0[\u00a0]: Copied! <pre># Creating a function that performs several text data cleaning steps \ndef clean_text(df, col_to_clean):\n\n    # Remove HTML tags\n    df['cleaned_text'] = df[col_to_clean].apply(lambda x: re.sub('&lt;[^&lt;]+?&gt;', ' ', x))\n \n    # Remove brackets and apostrophes from Python lists\n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r\"[\\[\\]'\\\"]\",\" \", x))\n    \n    # Remove change of line characters \n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"\\n\", \" \", regex=True)\n   \n    # Remove special characters\n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"-\", \"\", regex=False)\n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"[^a-zA-Z0-9 ]\", \"\", regex=True)\n     \n    # Lowercase text\n    df['cleaned_text'] = df['cleaned_text'].str.lower()\n    \n    return df\n</pre> # Creating a function that performs several text data cleaning steps  def clean_text(df, col_to_clean):      # Remove HTML tags     df['cleaned_text'] = df[col_to_clean].apply(lambda x: re.sub('&lt;[^&lt;]+?&gt;', ' ', x))       # Remove brackets and apostrophes from Python lists     df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r\"[\\[\\]'\\\"]\",\" \", x))          # Remove change of line characters      df['cleaned_text'] = df['cleaned_text'].str.replace(\"\\n\", \" \", regex=True)         # Remove special characters     df['cleaned_text'] = df['cleaned_text'].str.replace(\"-\", \"\", regex=False)     df['cleaned_text'] = df['cleaned_text'].str.replace(\"[^a-zA-Z0-9 ]\", \"\", regex=True)           # Lowercase text     df['cleaned_text'] = df['cleaned_text'].str.lower()          return df <p>After applying the cleaning function, this is the outcome we obtained:</p> In\u00a0[\u00a0]: Copied! <pre># Applying `clean_text()` function on writeups\ntext_past2years_df = clean_text(text_past2years_df, 'Writeup')\ntext_past2years_df['cleaned_text'][0]\n</pre> # Applying `clean_text()` function on writeups text_past2years_df = clean_text(text_past2years_df, 'Writeup') text_past2years_df['cleaned_text'][0] In\u00a0[\u00a0]: Copied! <pre># Creating a list containing all writeups\nlst_writeups = text_past2years_df['cleaned_text'].to_list()\n</pre> # Creating a list containing all writeups lst_writeups = text_past2years_df['cleaned_text'].to_list() <p>This function calculates the frequency of keywords in the collection of writeups. If using a CPU setting, this task will take around 7 min to complete.</p> In\u00a0[\u00a0]: Copied! <pre>#Reference1: https://github.com/boudinfl/pke/blob/master/examples/compute-df-counts.ipynb\n#Reference2: https://boudinfl.github.io/pke/build/html/unsupervised.html\ncompute_document_frequency(\n    documents=lst_writeups,     # List of writeups\n    output_file='inspec.df.gz',\n    language='en',              # language of the input files\n    normalization='stemming',   # use porter stemmer\n    stoplist=list(punctuation), # stoplist (punctuation marks)\n    n=3                         # compute n-grams up to 3-grams\n)\n</pre> #Reference1: https://github.com/boudinfl/pke/blob/master/examples/compute-df-counts.ipynb #Reference2: https://boudinfl.github.io/pke/build/html/unsupervised.html compute_document_frequency(     documents=lst_writeups,     # List of writeups     output_file='inspec.df.gz',     language='en',              # language of the input files     normalization='stemming',   # use porter stemmer     stoplist=list(punctuation), # stoplist (punctuation marks)     n=3                         # compute n-grams up to 3-grams ) <p>Let's have a look at the frequency of 20 keywords from the writeups collection</p> In\u00a0[\u00a0]: Copied! <pre>from pke import load_document_frequency_file\ndict_freq = load_document_frequency_file(input_file='inspec.df.gz')\n\ncount = 0  # Initialize a counter\nfor key, value in dict_freq.items():\n    if count &lt; 20:  # Limit to the first 5 key-value pairs\n        print(f'{key}: {value}')\n        count += 1\n    else:\n        break\n</pre> from pke import load_document_frequency_file dict_freq = load_document_frequency_file(input_file='inspec.df.gz')  count = 0  # Initialize a counter for key, value in dict_freq.items():     if count &lt; 20:  # Limit to the first 5 key-value pairs         print(f'{key}: {value}')         count += 1     else:         break In\u00a0[\u00a0]: Copied! <pre># Erasing non-utilized variables to freeing up memory\nimport gc\n\ndel writeup_df\ndel writeup_past2years_df \n\n# Freeing up memory \ngc.collect()\n</pre> # Erasing non-utilized variables to freeing up memory import gc  del writeup_df del writeup_past2years_df   # Freeing up memory  gc.collect() <p>The keyword extraction stage is based on the TfIdf (Term Frequency-Inverse Document Frequency) method from PKE. TfIdf is a popular and effective technique for identifying keyphrases in a collection of text documents. We have created the <code>extract_keywords()</code> function to extract the top 5 keywords from each writeup. This function will process the 208 writeups and render a total of 1,040 keywords (208*5).</p> In\u00a0[\u00a0]: Copied! <pre>def extract_keywords(text):\n    stoplist = list(string.punctuation)\n    stoplist += pke.lang.stopwords.get('en')\n    extractor = pke.unsupervised.TfIdf()\n    extractor.load_document(input=text,\n                           language='en',\n                           stoplist=stoplist,\n                           normalization=None)\n \n    extractor.candidate_selection(n=3) #Select 1 to 3 grams\n    df = load_document_frequency_file(input_file='inspec.df.gz')\n    extractor.candidate_weighting(df=df) #Candidate weighting using document frequencies\n    keyphrases = extractor.get_n_best(n=10)\n    \n    # Extract top 5 keywords\n    keywords = [keyword[0] for keyword in keyphrases[:5]]\n    \n    return keywords\n</pre> def extract_keywords(text):     stoplist = list(string.punctuation)     stoplist += pke.lang.stopwords.get('en')     extractor = pke.unsupervised.TfIdf()     extractor.load_document(input=text,                            language='en',                            stoplist=stoplist,                            normalization=None)       extractor.candidate_selection(n=3) #Select 1 to 3 grams     df = load_document_frequency_file(input_file='inspec.df.gz')     extractor.candidate_weighting(df=df) #Candidate weighting using document frequencies     keyphrases = extractor.get_n_best(n=10)          # Extract top 5 keywords     keywords = [keyword[0] for keyword in keyphrases[:5]]          return keywords In\u00a0[\u00a0]: Copied! <pre># Calculating a memory usage estimation of the collection of writeups to be processed.\ntext_past2years_df['cleaned_text'].info(memory_usage='deep')\n</pre> # Calculating a memory usage estimation of the collection of writeups to be processed. text_past2years_df['cleaned_text'].info(memory_usage='deep') In\u00a0[\u00a0]: Copied! <pre># Creating a bar to track the keyword extraction progress\nfrom tqdm import tqdm\nwith tqdm(total=len(text_past2years_df), desc=\"Processing\") as pbar:\n    def apply_with_progress(text):\n        result = extract_keywords(text)\n        pbar.update(1)  # Update the progress bar\n        return result\n\n    # Apply the function to the Series with progress tracking\n    abstract_keywords = text_past2years_df['cleaned_text'].apply(apply_with_progress)\n</pre> # Creating a bar to track the keyword extraction progress from tqdm import tqdm with tqdm(total=len(text_past2years_df), desc=\"Processing\") as pbar:     def apply_with_progress(text):         result = extract_keywords(text)         pbar.update(1)  # Update the progress bar         return result      # Apply the function to the Series with progress tracking     abstract_keywords = text_past2years_df['cleaned_text'].apply(apply_with_progress) In\u00a0[\u00a0]: Copied! <pre># Displaying the top 5 keywords of the first 10 writeups\nabstract_keywords[:10]\n</pre> # Displaying the top 5 keywords of the first 10 writeups abstract_keywords[:10] <p>The result is a list of lists that contains the top 5 keywords of each writeup. Let's store these results in a new <code>keywords_lst</code> column.</p> In\u00a0[\u00a0]: Copied! <pre>text_past2years_df['keywords_lst'] = abstract_keywords\ntext_past2years_df.head(1)\n</pre> text_past2years_df['keywords_lst'] = abstract_keywords text_past2years_df.head(1) In\u00a0[\u00a0]: Copied! <pre># Freeing up memory \ngc.collect()\n</pre> # Freeing up memory  gc.collect() In\u00a0[\u00a0]: Copied! <pre>text_past2years_df_exploded = text_past2years_df.explode('keywords_lst')\ntext_past2years_df_exploded = text_past2years_df_exploded.reset_index(drop=True)\ntext_past2years_df_exploded['keywords_lst'].value_counts().head(30)\n</pre> text_past2years_df_exploded = text_past2years_df.explode('keywords_lst') text_past2years_df_exploded = text_past2years_df_exploded.reset_index(drop=True) text_past2years_df_exploded['keywords_lst'].value_counts().head(30) In\u00a0[\u00a0]: Copied! <pre># Converting the previous keyword list into a dataframe\nkeywords_count_serie = text_past2years_df_exploded['keywords_lst'].value_counts()\nkeywords_count_df = pd.DataFrame({'Keywords': keywords_count_serie.index,'Count': keywords_count_serie.values})\nkeywords_count_df.head(5)\n</pre> # Converting the previous keyword list into a dataframe keywords_count_serie = text_past2years_df_exploded['keywords_lst'].value_counts() keywords_count_df = pd.DataFrame({'Keywords': keywords_count_serie.index,'Count': keywords_count_serie.values}) keywords_count_df.head(5) In\u00a0[\u00a0]: Copied! <pre># Selecting the top 50 words\nkeywords_count_50_df = keywords_count_df[:50]\n</pre> # Selecting the top 50 words keywords_count_50_df = keywords_count_df[:50] In\u00a0[\u00a0]: Copied! <pre>import plotly.express as px\n\nfig = px.bar(keywords_count_50_df, x='Count', y='Keywords', title='Top 50 keywords found in all Writeups by the TfIdf method', orientation='h', width=750, height=900, color='Keywords')\nfig.show()\n</pre> import plotly.express as px  fig = px.bar(keywords_count_50_df, x='Count', y='Keywords', title='Top 50 keywords found in all Writeups by the TfIdf method', orientation='h', width=750, height=900, color='Keywords') fig.show() In\u00a0[\u00a0]: Copied! <pre># Filtering out duplicated keywords\nkeywords_tree = text_past2years_df_exploded['keywords_lst'].to_list()\nset_keywords_tree = set(keywords_tree)\nlst_keywords_tree = list(set_keywords_tree)\nprint(f\"Total keywords: {len(keywords_tree)} \\nUnique keywords: {len(lst_keywords_tree)}\")\n</pre> # Filtering out duplicated keywords keywords_tree = text_past2years_df_exploded['keywords_lst'].to_list() set_keywords_tree = set(keywords_tree) lst_keywords_tree = list(set_keywords_tree) print(f\"Total keywords: {len(keywords_tree)} \\nUnique keywords: {len(lst_keywords_tree)}\") In\u00a0[\u00a0]: Copied! <pre># Creating a word cloud image using stylecloud\nstylecloud.gen_stylecloud(\n    text=' '.join(lst_keywords_tree), \n    icon_name='fas fa-tree',                     # 'fas fa-cloud'; 'fas fa-eye'; ''\n    palette='cmocean.sequential.Matter_10',\n    background_color='black',\n    gradient='horizontal',\n    size=1024\n)\nImage(filename=\"./stylecloud.png\", width=1024, height=768)\n</pre> # Creating a word cloud image using stylecloud stylecloud.gen_stylecloud(     text=' '.join(lst_keywords_tree),      icon_name='fas fa-tree',                     # 'fas fa-cloud'; 'fas fa-eye'; ''     palette='cmocean.sequential.Matter_10',     background_color='black',     gradient='horizontal',     size=1024 ) Image(filename=\"./stylecloud.png\", width=1024, height=768) In\u00a0[\u00a0]: Copied! <pre>text_architectures_keywords = [\n    \"fasttext\", \"roberta\", \"bert\", \"gpt\", \"rnn\", \"cnn\", \"gru\", \"t5\", \"electra\", \"xlnet\",\n    \"encoder\", \"decoder\", \"lstm\", \"transformer\", \"deberta\", \"codebert\"\n]\n</pre> text_architectures_keywords = [     \"fasttext\", \"roberta\", \"bert\", \"gpt\", \"rnn\", \"cnn\", \"gru\", \"t5\", \"electra\", \"xlnet\",     \"encoder\", \"decoder\", \"lstm\", \"transformer\", \"deberta\", \"codebert\" ] In\u00a0[\u00a0]: Copied! <pre># Function that matchs a list of specific words with a given column of a dataframe\ndef count_ocurrences_in_dataframe(df, column_name, strings_list):\n    # Convert the string_list input to a set for faster membership checking\n    strings_set = set(strings_list)\n    \n    # Filter out the dataframe to only include rows where 'column_name' contains any of the strings in 'strings_list' \n    # This is used to create a regular expression pattern where the '|' pipe acts as an \"OR\" operator.\n    filtered_df = df[df[column_name].str.contains('|'.join(strings_set))]\n    \n    # Create a dictionary to store the counting results\n    results_dict = {'String': [], 'Occurrences':[]}\n    \n    # Iterate over the strings list\n    for string in strings_list:\n        # Add the string and its corresponding count to the dictionary\n        results_dict['String'].append(string)\n      \n        # Count the actual ocurrences in the filtered dataframe\n        actual_occurrences = filtered_df[column_name].str.count(string).sum()\n        results_dict['Occurrences'].append(actual_occurrences)\n    \n    # Convert the dictionary to a dataframe\n    counts_df = pd.DataFrame(results_dict)\n    \n    return counts_df\n    \n</pre> # Function that matchs a list of specific words with a given column of a dataframe def count_ocurrences_in_dataframe(df, column_name, strings_list):     # Convert the string_list input to a set for faster membership checking     strings_set = set(strings_list)          # Filter out the dataframe to only include rows where 'column_name' contains any of the strings in 'strings_list'      # This is used to create a regular expression pattern where the '|' pipe acts as an \"OR\" operator.     filtered_df = df[df[column_name].str.contains('|'.join(strings_set))]          # Create a dictionary to store the counting results     results_dict = {'String': [], 'Occurrences':[]}          # Iterate over the strings list     for string in strings_list:         # Add the string and its corresponding count to the dictionary         results_dict['String'].append(string)                # Count the actual ocurrences in the filtered dataframe         actual_occurrences = filtered_df[column_name].str.count(string).sum()         results_dict['Occurrences'].append(actual_occurrences)          # Convert the dictionary to a dataframe     counts_df = pd.DataFrame(results_dict)          return counts_df      In\u00a0[\u00a0]: Copied! <pre>result = count_ocurrences_in_dataframe(text_past2years_df, 'cleaned_text', text_architectures_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result\n</pre> result = count_ocurrences_in_dataframe(text_past2years_df, 'cleaned_text', text_architectures_keywords) sorted_result = result.sort_values('Occurrences', ascending=False) sorted_result In\u00a0[\u00a0]: Copied! <pre>fig, ax= plt.subplots(1, 1, figsize=(8,4))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Architectures')\nax.set_xlabel('Occurrences')\nax.set_title('Architectures used in Kaggle text data competitions', fontsize=12)\n#ax.set_limits([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()\n</pre> fig, ax= plt.subplots(1, 1, figsize=(8,4)) sns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')  ax.set_ylabel('Architectures') ax.set_xlabel('Occurrences') ax.set_title('Architectures used in Kaggle text data competitions', fontsize=12) #ax.set_limits([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False)  plt.xticks(rotation=90) plt.show() <p>Answer: The top 3 architectures used in Kaggle text data competitions are BERT, DEBERTA, and ROBERTA.</p> In\u00a0[\u00a0]: Copied! <pre>techniques_keywords = [\n    \"pseudo labeling\",\n    \"masked language modeling\",\n    \"adversarial weight perturbation\",\n    \"model ensembling\",\n    \"model efficiency\",\n    \"data augmentation\"\n]\n</pre> techniques_keywords = [     \"pseudo labeling\",     \"masked language modeling\",     \"adversarial weight perturbation\",     \"model ensembling\",     \"model efficiency\",     \"data augmentation\" ] In\u00a0[\u00a0]: Copied! <pre>result = count_ocurrences_in_dataframe(text_past2years_df, 'cleaned_text', techniques_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result\n</pre> result = count_ocurrences_in_dataframe(text_past2years_df, 'cleaned_text', techniques_keywords) sorted_result = result.sort_values('Occurrences', ascending=False) sorted_result In\u00a0[\u00a0]: Copied! <pre>fig, ax= plt.subplots(1, 1, figsize=(8,4))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Techniques')\nax.set_xlabel('Occurrences')\nax.set_title('Trending NLP techniques found in Kaggle writeups')\n#ax.set_limits([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()\n</pre> fig, ax= plt.subplots(1, 1, figsize=(8,4)) sns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')  ax.set_ylabel('Techniques') ax.set_xlabel('Occurrences') ax.set_title('Trending NLP techniques found in Kaggle writeups') #ax.set_limits([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False)  plt.xticks(rotation=90) plt.show() <p>Answer: Pseudo labeling is the most referred technique along with data augmentation. Interestingly, it seems that kagglers didn't worry at all about optimizing their model's efficiency.</p> In\u00a0[\u00a0]: Copied! <pre>domain_keywords = [\n     \"text mining\",\n     \"text analytics\",\n     \"text preprocessing\",\n     \"text classification\",\n     \"text clustering\",\n     \"named entity recognition\",\n     \"topic modeling\",\n     \"information retrieval\",\n     \"text summarization\",\n     \"text generation\",\n     \"text similarity\",\n     \"word embeddings\",\n     \"document classification\",\n     \"text feature extraction\",\n     \"text segmentation\",\n     \"text normalization\",\n     \"text corpora\",\n     \"textual data analysis\",\n     \"question answering\",\n     \"sentiment analysis\",\n     \"language modeling\"    \n]\n</pre> domain_keywords = [      \"text mining\",      \"text analytics\",      \"text preprocessing\",      \"text classification\",      \"text clustering\",      \"named entity recognition\",      \"topic modeling\",      \"information retrieval\",      \"text summarization\",      \"text generation\",      \"text similarity\",      \"word embeddings\",      \"document classification\",      \"text feature extraction\",      \"text segmentation\",      \"text normalization\",      \"text corpora\",      \"textual data analysis\",      \"question answering\",      \"sentiment analysis\",      \"language modeling\"     ] In\u00a0[\u00a0]: Copied! <pre>result = count_ocurrences_in_dataframe(text_past2years_df, 'cleaned_text', domain_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result\n</pre> result = count_ocurrences_in_dataframe(text_past2years_df, 'cleaned_text', domain_keywords) sorted_result = result.sort_values('Occurrences', ascending=False) sorted_result In\u00a0[\u00a0]: Copied! <pre>fig, ax= plt.subplots(1, 1, figsize=(8,6))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Domains')\nax.set_xlabel('Occurrences')\nax.set_title('NLP domains found in Kaggle writeups')\nax.set_xlim([0, 12])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()\n</pre> fig, ax= plt.subplots(1, 1, figsize=(8,6)) sns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')  ax.set_ylabel('Domains') ax.set_xlabel('Occurrences') ax.set_title('NLP domains found in Kaggle writeups') ax.set_xlim([0, 12]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False)  plt.xticks(rotation=90) plt.show() <p>Answer: Question and answering and text classification domains are the most referred in the collection of writeups.</p> In\u00a0[\u00a0]: Copied! <pre># Freeing up memory \ngc.collect()\n</pre> # Freeing up memory  gc.collect() In\u00a0[\u00a0]: Copied! <pre># Loading the Arxiv Dataset\ndf_arxiv = pd.read_json(\n    '/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json',\n    lines = True, \n    convert_dates = True, \n    chunksize = 100000\n)\n</pre> # Loading the Arxiv Dataset df_arxiv = pd.read_json(     '/kaggle/input/2023-kaggle-ai-report/arxiv_metadata_20230510.json',     lines = True,      convert_dates = True,      chunksize = 100000 ) In\u00a0[\u00a0]: Copied! <pre># Reading a single chunk from the Arxiv dataset \nfor chunk in df_arxiv:\n    break\nlen(chunk)\n</pre> # Reading a single chunk from the Arxiv dataset  for chunk in df_arxiv:     break len(chunk) In\u00a0[\u00a0]: Copied! <pre>chunk.head(3)\n</pre> chunk.head(3) In\u00a0[\u00a0]: Copied! <pre>chunk.info(memory_usage='deep')\n</pre> chunk.info(memory_usage='deep') In\u00a0[\u00a0]: Copied! <pre># Reading all chunks and concatenating them into a single dataframe\narxiv_df = pd.DataFrame()\nfor chunk in df_arxiv:\n    arxiv_df = pd.concat([arxiv_df, chunk], ignore_index=True)\narxiv_df.head(5) \n</pre> # Reading all chunks and concatenating them into a single dataframe arxiv_df = pd.DataFrame() for chunk in df_arxiv:     arxiv_df = pd.concat([arxiv_df, chunk], ignore_index=True) arxiv_df.head(5)  In\u00a0[\u00a0]: Copied! <pre>arxiv_df.info(memory_usage='deep')\n</pre> arxiv_df.info(memory_usage='deep') <p>We found around 2.15 M papers in the Arxiv dataset. This dataframe has a memory usage of 4.0 GB.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"The Arxiv Dataset has {arxiv_df['categories'].nunique()} unique categories\")\n</pre> print(f\"The Arxiv Dataset has {arxiv_df['categories'].nunique()} unique categories\") <p>Answer: The Arxiv dataset has around 76 k categories</p> In\u00a0[\u00a0]: Copied! <pre># Turning the 'update_date' column into a datetime format column \narxiv_df['update_date'] = pd.to_datetime(arxiv_df['update_date'])\narxiv_df.info()\n</pre> # Turning the 'update_date' column into a datetime format column  arxiv_df['update_date'] = pd.to_datetime(arxiv_df['update_date']) arxiv_df.info() In\u00a0[\u00a0]: Copied! <pre>arxiv_date_min = arxiv_df['update_date'].min().strftime('%Y-%m-%d')\narxiv_date_max = arxiv_df['update_date'].max().strftime('%Y-%m-%d')\nprint(f\"The Arxiv Dataset includes papers from {arxiv_date_min} to {arxiv_date_max}\")\n</pre> arxiv_date_min = arxiv_df['update_date'].min().strftime('%Y-%m-%d') arxiv_date_max = arxiv_df['update_date'].max().strftime('%Y-%m-%d') print(f\"The Arxiv Dataset includes papers from {arxiv_date_min} to {arxiv_date_max}\") <p>Answer:  The Arxiv Dataset contains papers from 2007-05-23 to 2023-05-05</p> In\u00a0[\u00a0]: Copied! <pre>arxiv_2years_df = arxiv_df[(arxiv_df['update_date'].dt.year &gt;= 2021) &amp; (arxiv_df['update_date'].dt.month &gt;= 1)].copy()\narxiv_2years_df = arxiv_2years_df.reset_index(drop=True)\narxiv_2years_df.head(3)\n</pre> arxiv_2years_df = arxiv_df[(arxiv_df['update_date'].dt.year &gt;= 2021) &amp; (arxiv_df['update_date'].dt.month &gt;= 1)].copy() arxiv_2years_df = arxiv_2years_df.reset_index(drop=True) arxiv_2years_df.head(3) In\u00a0[\u00a0]: Copied! <pre>print(f\"The Arxiv Dataset contains {len(arxiv_2years_df)} papers from January 2021 to 2023\")\n</pre> print(f\"The Arxiv Dataset contains {len(arxiv_2years_df)} papers from January 2021 to 2023\") <p>Answer: The Arxiv Dataset contains around 527 k papers from January 2021 to 2023</p> In\u00a0[\u00a0]: Copied! <pre>arxiv_2years_df['categories'].value_counts().head(20)\n</pre> arxiv_2years_df['categories'].value_counts().head(20) <p>Answer: Computer Vision takes the lead on number of papers followed by quantum physics.</p> In\u00a0[\u00a0]: Copied! <pre>arxiv_2years_df['categories'].value_counts().tail(20)\n</pre> arxiv_2years_df['categories'].value_counts().tail(20) <p>Answer: We noticed that categories that are concatenated in a single row are hard to identify as unique categories</p> In\u00a0[\u00a0]: Copied! <pre># Filtering out only text data related papers from the Arxiv dataset\nnlp_categories_arxiv = [\n    \"cs.SE\",      # Software Engineering\n    \"cs.CY\",      # Computers and Society\n    \"cs.IR\",      # Information Retrieval\n    \"cs.CL\",      # Computation and Language\n    \"cs.LG\",      # Machine Learning\n    \"cs.NE\",      # Neural and Evolutionary Computing\n    \"cs.AI\",      # Artificial Intelligence\n    \"cs.DL\",      # Digital Libraries\n    \"cs.HC\",      # Human Computer Interaction\n    \"cs.SI\",      # Social and Information Networks\n    \"stat.ML\",    # Machine Learning\n    \"cs.SD\",      # Sound\n    \"cs.CR\",      # Cryptography and Security\n    \"q-fin.ST\",   # Statistical Finance\n    \"quant-ph\",   # Quantum Physics\n    \"q-bio.OT\",   # Other Quantitative Biology\n    \"physics.comp-ph\", # Computational Physics\n    \"physics.data-an\", # Data Analysis, Statistics, and Probability\n    \"cs.AR\"            # Hardware Architecture\n]\n</pre> # Filtering out only text data related papers from the Arxiv dataset nlp_categories_arxiv = [     \"cs.SE\",      # Software Engineering     \"cs.CY\",      # Computers and Society     \"cs.IR\",      # Information Retrieval     \"cs.CL\",      # Computation and Language     \"cs.LG\",      # Machine Learning     \"cs.NE\",      # Neural and Evolutionary Computing     \"cs.AI\",      # Artificial Intelligence     \"cs.DL\",      # Digital Libraries     \"cs.HC\",      # Human Computer Interaction     \"cs.SI\",      # Social and Information Networks     \"stat.ML\",    # Machine Learning     \"cs.SD\",      # Sound     \"cs.CR\",      # Cryptography and Security     \"q-fin.ST\",   # Statistical Finance     \"quant-ph\",   # Quantum Physics     \"q-bio.OT\",   # Other Quantitative Biology     \"physics.comp-ph\", # Computational Physics     \"physics.data-an\", # Data Analysis, Statistics, and Probability     \"cs.AR\"            # Hardware Architecture ] In\u00a0[\u00a0]: Copied! <pre>nlp_arxiv_2years_df = arxiv_2years_df[arxiv_2years_df.categories.isin(nlp_categories_arxiv)]\nnlp_arxiv_2years_df = nlp_arxiv_2years_df.reset_index(drop=True)\nprint(f\"Number of papers from the past two years: {len(arxiv_2years_df)} \\nNumber of NLP-related papers from the past two years: {len(nlp_arxiv_2years_df)}\")\n</pre> nlp_arxiv_2years_df = arxiv_2years_df[arxiv_2years_df.categories.isin(nlp_categories_arxiv)] nlp_arxiv_2years_df = nlp_arxiv_2years_df.reset_index(drop=True) print(f\"Number of papers from the past two years: {len(arxiv_2years_df)} \\nNumber of NLP-related papers from the past two years: {len(nlp_arxiv_2years_df)}\") In\u00a0[\u00a0]: Copied! <pre>nlp_arxiv_2years_df.sort_values('update_date', ascending=True)\n</pre> nlp_arxiv_2years_df.sort_values('update_date', ascending=True) <p>Answer: There is a total of 527 k papers from the past two years (from January 2021 to May 2023) and around 46 k of them corresponding to the NLP domain.</p> In\u00a0[\u00a0]: Copied! <pre>nlp_keywords_serie = nlp_arxiv_2years_df['categories'].value_counts()\nnlp_keywords_serie\n</pre> nlp_keywords_serie = nlp_arxiv_2years_df['categories'].value_counts() nlp_keywords_serie In\u00a0[\u00a0]: Copied! <pre>import plotly.express as px\n\nkeywords_count_df = pd.DataFrame({'Keywords': nlp_keywords_serie.index,'Count': nlp_keywords_serie.values})\nfig = px.bar(keywords_count_df, x='Count', y='Keywords', title='NLP-related papers per category found in the Arxiv dataset', orientation='h', width=750, height=900, color='Keywords')\nfig.show()\n</pre> import plotly.express as px  keywords_count_df = pd.DataFrame({'Keywords': nlp_keywords_serie.index,'Count': nlp_keywords_serie.values}) fig = px.bar(keywords_count_df, x='Count', y='Keywords', title='NLP-related papers per category found in the Arxiv dataset', orientation='h', width=750, height=900, color='Keywords') fig.show() <p>Answer: Interestingly, Quantum Physics takes the lead in NLP-related papers followed by Computation and Language, and Machine Learning papers.</p> In\u00a0[\u00a0]: Copied! <pre># Eliminating duplicates \nnlp_arxiv_2years_cleaned_df = nlp_arxiv_2years_df.drop_duplicates(subset=['title'])\nlen(nlp_arxiv_2years_df), len(nlp_arxiv_2years_cleaned_df)\n</pre> # Eliminating duplicates  nlp_arxiv_2years_cleaned_df = nlp_arxiv_2years_df.drop_duplicates(subset=['title']) len(nlp_arxiv_2years_df), len(nlp_arxiv_2years_cleaned_df) In\u00a0[\u00a0]: Copied! <pre># Printing a sample abstract \nnlp_arxiv_2years_cleaned_df['abstract'][0]\n</pre> # Printing a sample abstract  nlp_arxiv_2years_cleaned_df['abstract'][0] In\u00a0[\u00a0]: Copied! <pre># Creating a function that performs several text data cleaning steps \ndef clean_text(df, col_to_clean):\n\n    # Remove HTML tags\n    df['cleaned_text'] = df[col_to_clean].apply(lambda x: re.sub('&lt;[^&lt;]+?&gt;', ' ', x))\n \n    # Remove brackets and apostrophes from Python lists\n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r\"[\\[\\]'\\\"]\",\" \", x))\n    \n    # Remove change of line characters \n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"\\n\", \" \", regex=True)\n   \n    # Remove special characters\n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"-\", \"\", regex=False)\n    df['cleaned_text'] = df['cleaned_text'].str.replace(\"[^a-zA-Z0-9 ]\", \"\", regex=True)\n     \n    # Lowercase text\n    df['cleaned_text'] = df['cleaned_text'].str.lower()\n    \n    return df\n</pre> # Creating a function that performs several text data cleaning steps  def clean_text(df, col_to_clean):      # Remove HTML tags     df['cleaned_text'] = df[col_to_clean].apply(lambda x: re.sub('&lt;[^&lt;]+?&gt;', ' ', x))       # Remove brackets and apostrophes from Python lists     df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r\"[\\[\\]'\\\"]\",\" \", x))          # Remove change of line characters      df['cleaned_text'] = df['cleaned_text'].str.replace(\"\\n\", \" \", regex=True)         # Remove special characters     df['cleaned_text'] = df['cleaned_text'].str.replace(\"-\", \"\", regex=False)     df['cleaned_text'] = df['cleaned_text'].str.replace(\"[^a-zA-Z0-9 ]\", \"\", regex=True)           # Lowercase text     df['cleaned_text'] = df['cleaned_text'].str.lower()          return df In\u00a0[\u00a0]: Copied! <pre># Applying the `clean_text()` function on abstracts\nnlp_arxiv_2years_copy_df = nlp_arxiv_2years_cleaned_df.copy()\nnlp_arxiv_2years_copy_df = clean_text(nlp_arxiv_2years_copy_df, 'abstract')\n\n# Printing a cleaned abstract\nnlp_arxiv_2years_copy_df['cleaned_text'][0]\n</pre> # Applying the `clean_text()` function on abstracts nlp_arxiv_2years_copy_df = nlp_arxiv_2years_cleaned_df.copy() nlp_arxiv_2years_copy_df = clean_text(nlp_arxiv_2years_copy_df, 'abstract')  # Printing a cleaned abstract nlp_arxiv_2years_copy_df['cleaned_text'][0] In\u00a0[\u00a0]: Copied! <pre>nlp_arxiv_2years_copy_df.head(1)\n</pre> nlp_arxiv_2years_copy_df.head(1) In\u00a0[\u00a0]: Copied! <pre># Cleaning up memory \nimport gc\n\ndel df_arxiv\ndel chunk\ndel arxiv_df\ndel arxiv_2years_df\ndel nlp_arxiv_2years_df\ndel nlp_arxiv_2years_cleaned_df\n\ngc.collect() \n</pre> # Cleaning up memory  import gc  del df_arxiv del chunk del arxiv_df del arxiv_2years_df del nlp_arxiv_2years_df del nlp_arxiv_2years_cleaned_df  gc.collect()  In\u00a0[\u00a0]: Copied! <pre>!pip install git+https://github.com/boudinfl/pke.git\n</pre> !pip install git+https://github.com/boudinfl/pke.git In\u00a0[\u00a0]: Copied! <pre>def extract_keywords(text):\n    stoplist = list(string.punctuation)\n    stoplist += pke.lang.stopwords.get('en')\n    extractor = pke.unsupervised.TfIdf()\n    extractor.load_document(input=text,\n                           language='en',\n                           stoplist=stoplist,\n                           normalization=None)\n \n    extractor.candidate_selection() #Select 1 to 3 grams\n    extractor.candidate_weighting() #Candidate weighting using document frequencies\n    keyphrases = extractor.get_n_best(n=10)\n    \n    # Extract top 5 keywords\n    keywords = [keyword[0] for keyword in keyphrases[:5]]\n   \n    return keywords\n</pre> def extract_keywords(text):     stoplist = list(string.punctuation)     stoplist += pke.lang.stopwords.get('en')     extractor = pke.unsupervised.TfIdf()     extractor.load_document(input=text,                            language='en',                            stoplist=stoplist,                            normalization=None)       extractor.candidate_selection() #Select 1 to 3 grams     extractor.candidate_weighting() #Candidate weighting using document frequencies     keyphrases = extractor.get_n_best(n=10)          # Extract top 5 keywords     keywords = [keyword[0] for keyword in keyphrases[:5]]         return keywords In\u00a0[\u00a0]: Copied! <pre>nlp_arxiv_2years_copy_df['cleaned_text'].info(memory_usage='deep')\n</pre> nlp_arxiv_2years_copy_df['cleaned_text'].info(memory_usage='deep') <p>Important: The size of text-related papers (52.5 MB) is 250 times larger than that of the processed Kaggle writeups collection (208 KB). This implies that the keyword extraction process for the ArXiv dataset will experience a significant RAM memory overload if using the standard CPU settings. For this reason, we have selected only 5,000 abstracts to prove that the keyword extraction process works by identifying 25,000 keywords (5*5,000).</p> In\u00a0[\u00a0]: Copied! <pre># Selecting only 5,000 abstracts to be processed\nclean_abstract = nlp_arxiv_2years_copy_df['cleaned_text'][:5000]\nclean_abstract.info(memory_usage='deep')\n</pre> # Selecting only 5,000 abstracts to be processed clean_abstract = nlp_arxiv_2years_copy_df['cleaned_text'][:5000] clean_abstract.info(memory_usage='deep') In\u00a0[\u00a0]: Copied! <pre>!pip install tqdm\n</pre> !pip install tqdm In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n</pre> from tqdm import tqdm In\u00a0[\u00a0]: Copied! <pre># Creating a tqdm progress bar to track the keyword extraction process. It takes about 6 h\ncleanup_interval = 20\n\nwith tqdm(total=len(clean_abstract), desc=\"Processing\") as pbar:\n    def apply_with_progress(text):\n        result = extract_keywords(text)\n        pbar.update(1)  # Update the progress bar\n        # Check if it's time to clean up memory\n        if pbar.n % cleanup_interval == 0:\n            gc.collect()\n        return result\n\n    # Apply the function to the Series with progress tracking\n    abstract_keywords = clean_abstract.apply(apply_with_progress)\n</pre> # Creating a tqdm progress bar to track the keyword extraction process. It takes about 6 h cleanup_interval = 20  with tqdm(total=len(clean_abstract), desc=\"Processing\") as pbar:     def apply_with_progress(text):         result = extract_keywords(text)         pbar.update(1)  # Update the progress bar         # Check if it's time to clean up memory         if pbar.n % cleanup_interval == 0:             gc.collect()         return result      # Apply the function to the Series with progress tracking     abstract_keywords = clean_abstract.apply(apply_with_progress) In\u00a0[\u00a0]: Copied! <pre>abstract_keywords[100:]\n</pre> abstract_keywords[100:] In\u00a0[\u00a0]: Copied! <pre># Finding the most popular keywords from 5,000 papers\nnlp_keywords_serie = abstract_keywords.explode()\nnlp_keywords_count = nlp_keywords_serie.value_counts()\nnlp_keywords_count_df = pd.DataFrame({'Keywords': nlp_keywords_count.index,'Count': nlp_keywords_count.values})\nnlp_keywords_count_df.head(5)\n</pre> # Finding the most popular keywords from 5,000 papers nlp_keywords_serie = abstract_keywords.explode() nlp_keywords_count = nlp_keywords_serie.value_counts() nlp_keywords_count_df = pd.DataFrame({'Keywords': nlp_keywords_count.index,'Count': nlp_keywords_count.values}) nlp_keywords_count_df.head(5) In\u00a0[\u00a0]: Copied! <pre># Selecting the top 50 words\nnlp_keywords_count50_df = nlp_keywords_count_df[:50]\n</pre> # Selecting the top 50 words nlp_keywords_count50_df = nlp_keywords_count_df[:50] In\u00a0[\u00a0]: Copied! <pre>import plotly.express as px\n\nfig = px.bar(nlp_keywords_count50_df, x='Count', y='Keywords', title='Top 50 keywords found in 5,000 abstracts by the TfIdf method', orientation='h', width=750, height=900, color='Keywords')\nfig.show()\n</pre> import plotly.express as px  fig = px.bar(nlp_keywords_count50_df, x='Count', y='Keywords', title='Top 50 keywords found in 5,000 abstracts by the TfIdf method', orientation='h', width=750, height=900, color='Keywords') fig.show() In\u00a0[\u00a0]: Copied! <pre># Filtering out duplicated keywords\nkeywords_tree = nlp_keywords_serie.to_list()\nset_keywords_tree = set(keywords_tree)\nlst_keywords_tree = list(set_keywords_tree)\nprint(f\"Total keywords: {len(keywords_tree)} \\nUnique keywords: {len(lst_keywords_tree)}\")\n</pre> # Filtering out duplicated keywords keywords_tree = nlp_keywords_serie.to_list() set_keywords_tree = set(keywords_tree) lst_keywords_tree = list(set_keywords_tree) print(f\"Total keywords: {len(keywords_tree)} \\nUnique keywords: {len(lst_keywords_tree)}\") In\u00a0[\u00a0]: Copied! <pre># Creating a word cloud image using stylecloud\nstylecloud.gen_stylecloud(\n    text=' '.join(lst_keywords_tree), \n    icon_name='fas fa-tree',                     # 'fas fa-cloud'; 'fas fa-eye'; ''\n    palette='cmocean.sequential.Matter_10',\n    background_color='black',\n    gradient='horizontal',\n    size=1024\n)\nImage(filename=\"./stylecloud.png\", width=1024, height=768)\n</pre> # Creating a word cloud image using stylecloud stylecloud.gen_stylecloud(     text=' '.join(lst_keywords_tree),      icon_name='fas fa-tree',                     # 'fas fa-cloud'; 'fas fa-eye'; ''     palette='cmocean.sequential.Matter_10',     background_color='black',     gradient='horizontal',     size=1024 ) Image(filename=\"./stylecloud.png\", width=1024, height=768) In\u00a0[\u00a0]: Copied! <pre>text_architectures_keywords = [\n    \"fasttext\", \"roberta\", \"bert\", \"gpt\", \"rnn\", \"cnn\", \"gru\", \"t5\", \"electra\", \"xlnet\",\n    \"encoder\", \"decoder\", \"lstm\", \"transformer\", \"deberta\", \"codebert\"\n]\n</pre> text_architectures_keywords = [     \"fasttext\", \"roberta\", \"bert\", \"gpt\", \"rnn\", \"cnn\", \"gru\", \"t5\", \"electra\", \"xlnet\",     \"encoder\", \"decoder\", \"lstm\", \"transformer\", \"deberta\", \"codebert\" ] In\u00a0[\u00a0]: Copied! <pre># Function that matchs a list of specific words with a column of a dataframe\ndef count_ocurrences_in_dataframe(df, column_name, strings_list):\n    # Convert the string_list input to a set format for faster membership checking\n    strings_set = set(strings_list)\n    \n    # Filter out the dataframe to only include rows where 'column_name' contains any of the strings in 'strings_list' \n    # This is used to create a regular expression pattern where the '|' pipe acts as an \"OR\" operator.\n    filtered_df = df[df[column_name].str.contains('|'.join(strings_set))]\n    \n    # Create a dictionary to store the counting results\n    results_dict = {'String': [], 'Occurrences':[]}\n    \n    # Iterate over the strings list\n    for string in strings_list:\n        # Add the string and its corresponding count to the dictionary\n        results_dict['String'].append(string)\n      \n        # Count the actual ocurrences in the filtered dataframe\n        actual_occurrences = filtered_df[column_name].str.count(string).sum()\n        results_dict['Occurrences'].append(actual_occurrences)\n    \n    # Convert the dictionary to a dataframe\n    counts_df = pd.DataFrame(results_dict)\n    \n    return counts_df\n    \n</pre> # Function that matchs a list of specific words with a column of a dataframe def count_ocurrences_in_dataframe(df, column_name, strings_list):     # Convert the string_list input to a set format for faster membership checking     strings_set = set(strings_list)          # Filter out the dataframe to only include rows where 'column_name' contains any of the strings in 'strings_list'      # This is used to create a regular expression pattern where the '|' pipe acts as an \"OR\" operator.     filtered_df = df[df[column_name].str.contains('|'.join(strings_set))]          # Create a dictionary to store the counting results     results_dict = {'String': [], 'Occurrences':[]}          # Iterate over the strings list     for string in strings_list:         # Add the string and its corresponding count to the dictionary         results_dict['String'].append(string)                # Count the actual ocurrences in the filtered dataframe         actual_occurrences = filtered_df[column_name].str.count(string).sum()         results_dict['Occurrences'].append(actual_occurrences)          # Convert the dictionary to a dataframe     counts_df = pd.DataFrame(results_dict)          return counts_df      In\u00a0[\u00a0]: Copied! <pre>result = count_ocurrences_in_dataframe(nlp_arxiv_2years_copy_df, 'cleaned_text', text_architectures_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result\n</pre> result = count_ocurrences_in_dataframe(nlp_arxiv_2years_copy_df, 'cleaned_text', text_architectures_keywords) sorted_result = result.sort_values('Occurrences', ascending=False) sorted_result In\u00a0[\u00a0]: Copied! <pre>fig, ax= plt.subplots(1, 1, figsize=(8,4))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Architectures')\nax.set_xlabel('Occurrences')\nax.set_title('Architectures used in the Arxiv Dataset', fontsize=12)\n#ax.set_limits([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()\n</pre> fig, ax= plt.subplots(1, 1, figsize=(8,4)) sns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')  ax.set_ylabel('Architectures') ax.set_xlabel('Occurrences') ax.set_title('Architectures used in the Arxiv Dataset', fontsize=12) #ax.set_limits([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False)  plt.xticks(rotation=90) plt.show() <p>Answer: It seems that BERT and encoder-based architectures take the lead in Academia along with trasformers.</p> In\u00a0[\u00a0]: Copied! <pre>techniques_keywords = [\n    \"pseudo labeling\",\n    \"masked language modeling\",\n    \"adversarial weight perturbation\",\n    \"model ensembling\",\n    \"model efficiency\",\n    \"data augmentation\"\n]\n</pre> techniques_keywords = [     \"pseudo labeling\",     \"masked language modeling\",     \"adversarial weight perturbation\",     \"model ensembling\",     \"model efficiency\",     \"data augmentation\" ] In\u00a0[\u00a0]: Copied! <pre>result = count_ocurrences_in_dataframe(nlp_arxiv_2years_copy_df, 'cleaned_text', techniques_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result\n</pre> result = count_ocurrences_in_dataframe(nlp_arxiv_2years_copy_df, 'cleaned_text', techniques_keywords) sorted_result = result.sort_values('Occurrences', ascending=False) sorted_result In\u00a0[\u00a0]: Copied! <pre>fig, ax= plt.subplots(1, 1, figsize=(8,4))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Techniques')\nax.set_xlabel('Occurrences')\nax.set_title('Trending NLP techniques found in Academia')\n#ax.set_limits([])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()\n</pre> fig, ax= plt.subplots(1, 1, figsize=(8,4)) sns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')  ax.set_ylabel('Techniques') ax.set_xlabel('Occurrences') ax.set_title('Trending NLP techniques found in Academia') #ax.set_limits([]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False)  plt.xticks(rotation=90) plt.show() <p>Answer: Unlike the text-related Kaggle writeups, researchers are more interested in data augmentation techniques rather than pseudo labeling.</p> In\u00a0[\u00a0]: Copied! <pre>text_data_keywords = [\n     \"text mining\",\n     \"text analytics\",\n     \"text preprocessing\",\n     \"text classification\",\n     \"text clustering\",\n     \"named entity recognition\",\n     \"topic modeling\",\n     \"information retrieval\",\n     \"text summarization\",\n     \"text generation\",\n     \"text similarity\",\n     \"word embeddings\",\n     \"document classification\",\n     \"text feature extraction\",\n     \"text segmentation\",\n     \"text normalization\",\n     \"text corpora\",\n     \"textual data analysis\",\n     \"question answering\",\n     \"sentiment analysis\",\n     \"language modeling\"    \n]\n</pre> text_data_keywords = [      \"text mining\",      \"text analytics\",      \"text preprocessing\",      \"text classification\",      \"text clustering\",      \"named entity recognition\",      \"topic modeling\",      \"information retrieval\",      \"text summarization\",      \"text generation\",      \"text similarity\",      \"word embeddings\",      \"document classification\",      \"text feature extraction\",      \"text segmentation\",      \"text normalization\",      \"text corpora\",      \"textual data analysis\",      \"question answering\",      \"sentiment analysis\",      \"language modeling\"     ] In\u00a0[\u00a0]: Copied! <pre>result = count_ocurrences_in_dataframe(nlp_arxiv_2years_copy_df, 'cleaned_text', text_data_keywords)\nsorted_result = result.sort_values('Occurrences', ascending=False)\nsorted_result\n</pre> result = count_ocurrences_in_dataframe(nlp_arxiv_2years_copy_df, 'cleaned_text', text_data_keywords) sorted_result = result.sort_values('Occurrences', ascending=False) sorted_result In\u00a0[\u00a0]: Copied! <pre>fig, ax= plt.subplots(1, 1, figsize=(8,6))\nsns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')\n\nax.set_ylabel('Domains')\nax.set_xlabel('Occurrences')\nax.set_title('NLP domains found in Academia')\nax.set_xlim([0, 800])\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.xticks(rotation=90)\nplt.show()\n</pre> fig, ax= plt.subplots(1, 1, figsize=(8,6)) sns.barplot(x = sorted_result['Occurrences'], y = sorted_result['String'], palette='flare')  ax.set_ylabel('Domains') ax.set_xlabel('Occurrences') ax.set_title('NLP domains found in Academia') ax.set_xlim([0, 800]) ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False)  plt.xticks(rotation=90) plt.show() <p>Answer: Regarding the NLP domains, we found agreement of interest in both the Kaggle community and Academia focusing their efforts on question and answering and text classification fields.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#in-depth-analysis-of-kaggle-and-arxiv-datasets","title":"In Depth Analysis of Kaggle and Arxiv Datasets\u00b6","text":"<p>by Salomon Marquez</p> <p>01/12/2023</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#1-analyzing-kaggle-writeups-dataset","title":"1. Analyzing Kaggle Writeups Dataset\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#how-many-unique-competitions-has-the-kaggle-writeups-dataset","title":"How many unique competitions has the Kaggle writeups dataset?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-are-the-earliest-and-latest-competitions","title":"What are the earliest and latest competitions?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#how-many-competitions-are-from-the-past-two-years","title":"How many competitions are from the past two years?\u00b6","text":"<p>Let's consider competitions from January 2021 onwards.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-are-the-competitions-from-the-past-two-years-with-most-writeups","title":"What are the competitions from the past two years with most writeups?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-are-the-competitions-from-the-past-two-years-with-less-writeups","title":"What are the competitions from the past two years with less writeups?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-is-the-number-of-writeups-corresponding-to-text-data-competitions-held-in-the-past-two-years","title":"What is the number of writeups corresponding to text data competitions held in the past two years?\u00b6","text":"<p>For this analysis, we're going to use the following external dataset Top 3 Kaggle Text Data Competitions (2021-2023) that has identified nine competitions related to text data.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-is-the-number-of-writeups-per-text-data-competition","title":"What is the number of writeups per text data competition?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#2-extracting-keywords-from-text-data-writeups","title":"2. Extracting keywords from text data writeups\u00b6","text":"<p>Now that we have identified 9 competitions related to text data and their 208 writeups, let's analyze the content of the writeups using the PKE (Python Keyword Extraction) module. Before stepping into this task, it's paramount to implement a cleaning text data stage.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#cleaning-text-data","title":"Cleaning text data\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#computing-the-frequency-of-keywords-in-writeups","title":"Computing the frequency of keywords in writeups\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#extracting-keywords-from-writeups","title":"Extracting keywords from writeups\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#plotting-extracted-keywords-from-writeups","title":"Plotting extracted keywords from writeups\u00b6","text":"<p>Let's count the most mentioned keywords in the collection of writeups</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#3-examining-popular-architectures-domains-and-techniques-used-in-kaggle-writeups-based-on-word-occurrences","title":"3. Examining popular architectures, domains, and techniques used in Kaggle writeups based on word occurrences\u00b6","text":"<p>In the previous section, we extracted the top 5 keywords of every writeup and computed an empirical analysis of their occurrences in all writeups. We identified common words, including 'models,' 'competition,' 'training,' 'ensemble,' and 'different.' However, these words do not appear to offer valuable insights about the write-ups. In this section, we will formulate specific questions and provide keywords that are more likely to yield better results in understanding the techniques, text data domains, and architectures used in Kaggle's text data competitions.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-are-the-main-architectures-used-in-the-solutions-of-text-data-competitions","title":"What are the main architectures used in the solutions of text data competitions?\u00b6","text":"<p>We considered the following 16 architectures as keywords for this question.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#which-of-the-following-techniques-is-mostly-used-in-the-solutions-of-text-data-competitions","title":"Which of the following techniques is mostly used in the solutions of text data competitions?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#which-of-the-following-domains-is-mostly-referred-in-the-solutions-of-text-data-competitions","title":"Which of the following domains is mostly referred in the solutions of text data competitions?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#4-analyzing-the-arxiv-dataset","title":"4. Analyzing the Arxiv Dataset\u00b6","text":"<p>Likewise the Kaggle writeup dataset, we're going to analyze the Arxiv dataset to gain more insights about the strategies followed in Academia related to text data.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#how-many-distinct-categories-of-papers-are-present-in-the-arxiv-dataset","title":"How many distinct categories of papers are present in the ArXiv Dataset?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-are-the-earliest-and-latest-papers-in-the-arxiv-dataset","title":"What are the earliest and latest papers in the Arxiv Dataset?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#how-many-papers-has-the-arxiv-dataset-from-the-past-two-years","title":"How many papers has the Arxiv Dataset from the past two years?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-are-the-papers-with-most-categories-from-the-past-two-years","title":"What are the papers with most categories from the past two years?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-are-the-papers-with-less-categories-from-the-past-two-years","title":"What are the papers with less categories from the past two years?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-is-the-number-of-nlp-related-papers-from-the-past-two-years","title":"What is the number of NLP-related papers from the past two years?\u00b6","text":"<p>To identify the most popular categories in the NLP domain in the Arxiv Dataset, we searched the term Natural Language Processing in the arxiv dataset from 2021-01 to 2023. Then we ordered the results by Annnoucement date (oldest first) and then by Annnoucement date (newest first) and identified 19 categories that were mostly referred by researchers.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-is-the-count-of-nlp-related-papers-per-category-from-the-past-two-years","title":"What is the count of NLP-related papers per category from the past two years?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#5-extracting-keywords-from-arxiv-papers","title":"5. Extracting keywords from Arxiv papers\u00b6","text":"<p>In this section, we analyze the content of abstracts of papers using the PKE (Python Keyword Extraction) module to identify main keywords.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#cleaning-the-arxiv-dataset","title":"Cleaning the Arxiv Dataset\u00b6","text":"<p>Before stepping into this task, it's paramount to implement a cleaning text data stage.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#computing-keyword-extraction-on-the-arxiv-dataset","title":"Computing keyword extraction on the Arxiv Dataset\u00b6","text":"<p>The keyword extraction stage is based on the TfIdf (Term Frequency-Inverse Document Frequency) method from PKE. TfIdf is a popular and effective technique for identifying keyphrases in a collection of text documents. We have created the <code>extract_keywords()</code> function to extract the top 5 keywords from each abstract.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#implementing-a-progress-bar-to-track-the-keyword-extraction-process","title":"Implementing a progress bar to track the keyword extraction process\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#plotting-identified-keywords-from-5000-papers","title":"Plotting identified keywords from 5,000 papers\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#6-examining-popular-architectures-domains-and-techniques-in-the-arxiv-dataset-based-on-word-occurrences","title":"6. Examining popular architectures, domains, and techniques in the Arxiv dataset based on word occurrences\u00b6","text":"<p>Likewise the analysis on the Kaggle writeup dataset, in this section we make specific questions and provide keywords to narrow down our analysis. We will focus specifically on the techniques, text data domains, and architectures mostly employed in the papers of the Arxiv dataset.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#what-are-the-main-architectures-used-in-academia","title":"What are the main architectures used in Academia?\u00b6","text":"<p>We considered the following 16 architectures as keywords for this question.</p>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#which-of-the-following-techniques-is-mostly-used-in-academia","title":"Which of the following techniques is mostly used in Academia?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#which-of-the-following-domains-is-mostly-referred-in-academia","title":"Which of the following domains is mostly referred in Academia?\u00b6","text":""},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#conclusion","title":"Conclusion\u00b6","text":"<p>By considering 9 text-data-related competitions instead of 5, we identified 208 writeups (70 times more data to be analyzed than in our previous EDA). This helped us gain a better understanding of Kaggle text data competitions. We've also expanded our consideration to 19 categories that could contain NLP papers, as opposed to the previous 12, for the ArXiv dataset. Here's a general summary of our findings over the last two years:</p> <ul> <li>BERT and encoder-based architectures are the most popular in both the Kaggle community and academic contexts.</li> <li>Pseudo labeling was the most frequently referenced technique in text data writeups, while data augmentation was more prevalent in text-related papers. It appears that researchers prioritize model efficiency, whereas Kagglers might overlook it in their solutions.</li> <li>Both the Kaggle community and academia are increasingly focusing their efforts on Question and Answer (Q&amp;A) and text classification domains.</li> </ul>"},{"location":"natural-language-processing/in-depth-analysis-of-kaggle-and-arxiv-datasets/#appendix","title":"Appendix\u00b6","text":"<p>Finally, here are some useful tips for processing large datasets:</p> <ol> <li><p>Keep an eye on the RAM memory usage at every stage of your dataset analysis. You can:</p> <ul> <li>Assess the memory size of dataframes using the <code>df.info(memory_usage='deep')</code> command</li> <li>Consider removing dataframes that you no longer need with <code>del df</code></li> <li>Free up memory whenever possible using the <code>gc.collect()</code> command.</li> <li>Use the following commands to assess the memory usage of your variables:</li> </ul> <pre><code>    from __future__ import print_function  # for Python2\n    import sys\n\n    local_vars = list(locals().items())\n    for var, obj in local_vars:\n    print(var, sys.getsizeof(obj))    \n</code></pre> </li> <li><p>Implement a progress bar when executing large processes</p> </li> </ol>"},{"location":"technical-writing/","title":"Welcome to My Technical Writer's Portfolio","text":"<p>I am a highly accomplished Senior Technical Writer with a Ph.D. in Electronics Engineering. I have more than 10 years of experience in academic writing and over 5 years in technical writing for the software industry. My technical background allows me to explain complex concepts in a more understandable way for different audiences.</p> <p>As a Senior Technical Writer, I have played multiple roles as a consultant, technical lead, mentor, hiring interviewer, and open-source contributor. I have collaborated on a wide range of projects related to dating, financial technologies, and video streaming platforms, producing various types of documentation such as API documentation, architecture guides, developer guides, onboarding guides, user manuals, concept guides, glossaries, and READMEs. I thrive on working on challenging projects that allow me to learn new technologies and establish collaborative work strategies among stakeholders, developers, and other technical writers.</p> <p>I am passionate about cloud computing, AI-assisted tools, and data engineering, and always stay up-to-date with the latest trends and advancements in these fields.</p> <p>Visit my documentation's portfolio \ud83d\udcda</p>"},{"location":"technical-writing/#skills","title":"Skills","text":"Core Skills Content Tools Information architecture API documentation JIRA AWS and GCP practitioner Software architecture guides Confluence Content design and development READMEs Git Data Engineer practitioner Sequence diagrams Swagger Open source documentation Developer guides Markdown User guides Postman Technical specifications Jupyter notebooks Static Sites MySQL Getting started and onboarding guides Text editors Glossaries MS Office"},{"location":"technical-writing/#projects","title":"Projects","text":"<ul> <li> <p>Video streaming services     Integration of a streaming platform into STB devices. I created how-to guides that helped newcomers configure their devices and routers. I also supported the quality testing team and carried out project management tasks. The development team worked with JavaScript, RESTful APIs, and AWS Athena.</p> </li> <li> <p>Financial services      Migration of a legacy-based financial system into a microservice-based architecture. I created how-to guides that helped developers to connect to databases, monitor resources, manage parameters, deploy applications using Octopus, and query data using AWS Athena. I produced glossaries, policies, and user guides and implemented a Docs-like-Code approach to deliver technical documentation in a static site powered by Hugo and hosted in GitLab Pages.</p> </li> <li> <p>Dating services      Integration of BlueSnap, Stripe, and Apple Pay payment processors with customer services. I created sequence diagrams and API documentation that helped developers to understand the flow of payment transactions for payment processors, including Stripe, BlueSnap, and Apple Pay.</p> </li> </ul>"},{"location":"technical-writing/#open-source","title":"Open Source","text":"<p>I contributed to the Open Source Initiative (OSI) at Wizeline by conducting the followings tasks:</p> <ul> <li>Analyzed existing OSIs from third-party companies to define the OSI at Wizeline</li> <li>Helped on the design of the OSI guidelines at Wizeline by defining and documenting the main contribution pathways: getting to know OS, using OS, contributing to OS, and creating a new OS project. </li> <li>Supported the launching of the first open source project at Wizeline on the documentation side by creating the README files of the project\u2019s repositories</li> <li>Created a repository containing documentation and licenses that served as a template to generate new open source projects at Wizeline</li> <li>Created a Kanban board to organize, prioritize and keep tracking of tasks on the documentation side. This strategy fostered the collaboration and contribution of a team of three TWs to the Open Source initiative</li> </ul>"},{"location":"technical-writing/#presentations","title":"Presentations","text":"<ul> <li> <p>TW Academy at Wizeline (Spring 2022) </p> <ul> <li>Lecturer of the session How to Write Software Architectures. I created and delivered the material for this session in a video format for on-demand studying. </li> <li>Mentored two students </li> </ul> </li> <li> <p>TW Academy at Wizeline (May 2021) \\     Mentored one student. Some responsibilities included: review assignments, have weekly one-on-one sessions to provide training and best practices, assess the student\u2019s cultural fit, and complete the student's rubric.</p> </li> <li> <p>TW Academy at Wizeline (March 2020) \\     Lecturer of the session How to Write Software Architectures </p> </li> </ul>"},{"location":"technical-writing/#leadership-initiatives-at-wizeline","title":"Leadership Initiatives at Wizeline","text":"<ul> <li> <p>I led a study group of 16 technical writers for three months to pursue the AWS Cloud Practitioner Certification. Some activities included: define and organize study materials, coordinate study sessions, create questions that fostered discussions, and give demos.</p> </li> <li> <p>I mentored three technical writers joining the TW team for six months. Some activities included: conduct peer reviews, support their career growth, identify opportunity areas and how to cope with them, and help them unblock any stoppers.</p> </li> <li> <p>I led the TW onboarding squad for one year. Primary responsibilities included: send a welcome email to new technical writers along with a handful of useful resources to get onboarded, revamp the TW style guide assessment, and provide guidance to TW team leads on how to support their mentees.</p> </li> </ul>"}]}